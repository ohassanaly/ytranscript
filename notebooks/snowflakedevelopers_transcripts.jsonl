{"url": "https://www.youtube.com/watch?v=HhzpDM36Pzg", "title": "Snowflake Postgres In 2 Minutes: Fully Managed Postgres On Snowflake", "transcript": "We're excited to bring Postgress, [music] the world's most popular open-source database to the Snowflake platform. This fully managed solution eliminates the [music] architectural divide between transactional apps and analytical insights [music] by consolidating your data footprint onto a single powerful platform. You can streamline development and drive innovation. >> [music] >> To get started with Snowflake Postgress, begin in your Snow Site window and create a new [music] instance. Give it a name. Choose the instance type. Put the amount of memory and CPUs you'd like and how much storage [music] you need. These can be changed later without having to migrate data. Select if you need a high availability failover and create the instance. You can use the connection string [music] on screen to connect Postgress from a command line or a graphical interface. All standard Postgress [music] features like extensions and permission are available. Here we connect Snowflake Postgress Psql and create [music] a basic table and a select. And here we're connecting an application configuration to Snowflake Postgress. Snowflake Postgress also comes with smart management tools to run Postgress like a pro. See your cache [music] and index usages to stay on top of performance. You can also check on table size, vacuum or long running queries. We're bringing Postgres [music] inside the security perimeter of Snowflake. Snowflake Postgress is designed for environments requiring robust security and compliance standards, giving you secure data within a unified framework. Gain granular control with customer [music] managed keys and network access control via private link in VPC peering. Keep your data protected, audit ready and compliant with the regulatory requirements. Snowflake Postgres [music] is 100% compatible with community Postgress. You can lift [music] and shift existing onprem and cloud Postgress workloads with zero code changes and little to no downtime. Try Snowflake Postgress [music] today. Fully managed Postgress built with easy connected and trusted enterprisegrade features for the world of Snowflake. "}
{"url": "https://www.youtube.com/watch?v=glDWNpHX8g4", "title": "Behind The Cape - Snowflake Apache Iceberg  and Polaris", "transcript": "Hey everybody, welcome to another episode of Behind the Cape. I'm data superhero Keith Beler and today I'm joined by fellow data superhero Sorrokumar. Um, today we're going to be talking about Snowflake Polaris open catalog and Apache iceberg tables. I know I'm very excited. Welcome to the show Sorro. Great to have you. >> Thanks Keith for having me on the show. >> Yeah, why don't why don't you tell tell everybody a little bit about yourself? >> Sure. Sure. Um before I wanted to before I introduce myself, I want to say something. Uh ever since I have been part of the data superhero program, I have been following behind the cape series and it was there in my wish list to be on your show. So I'm very glad to be part of this. >> Well, thank you. I'm very excited to have you and I think everybody's going to really be excited for for whatever you have in store for us today. I know personally I'm haven't gotten to play with iceberg tables or even polar open catalog. So I'm very intrigued and anxious to learn as well. So >> all right let's go for it. >> Great go for it. >> All right for the audience I am Sav Kumar and I work as a senior manager in the data and AI team at Accenture Australia where I help organizations design and build modern data platform at scale. I'm also honored to be a Snowflake data superhero and an AWS golden jacket which has given me an opportunity to work closely with cloud and data communities across the globe. Outside of work, I co-host the Sor and Saga show, a podcast on YouTube and Spotify where we explore stories in tech career and human side behind the professionals we meet. I'm excited to bring those experience in today's session as we dive into how Snowflake, Iceberg, and Polaris are reshaping the modern lakehouse. Great. >> So I hope you're ready because today we are diving in one of the most exciting shift happening in the data world right now. We have all heard the promise of the modern lakehouse. But the reality multiple engines, multiple formats, duplicated data, broken pipelines and the way too many late night slack messages saying why my table different in Spark and in Snowflake. But that era is ending because today I'm talking about building an interoperable data platform and how Snowflake, Apache, Iceberg and Polaris are redefining the model lakehouse. This isn't just an architecture pattern. This is the moment where open formats, open catalog and governed access come together finally giving us one data layer that any engine can work with reliably, consistently, and at scale. So let's jump in. So before we dived in, here's a quick look at what we are covering today. We'll start with a short introduction setting the stage for why interoperability is becoming one of the biggest priorities in modern data platform. Then we'll explore interoperability challenges in data lake. What really happens when multiple engine like spark, fling, trino, and snowflake all try to access the same data. From there, we'll move into how Snowflake open catalog powers multi-engine interoperability. Next, I'll walk you through how this integration help unify data links, connecting the dots between open formats, catalog, and snowflake governance layer. Then, we'll move into the demo architecture where I'll show how all these pieces come together in real world scenario followed by a live demo. After that, I'll share a few best practices and recommendations, lessons learned from early adopters, and some design patterns to keep in mind. And finally, we'll wrap up with some Q&A. Imagine a world where your data is entrapped by the tools you use. Where Spark for big data processing, Flink for streaming, Trino for federated queries, and Snowflake for analytics and governance all work together seamlessly on the same data sets. No copies, no conflicts, no governance gaps. That's the world Snowflake is building with Apache Iceberg and Polaris. And it's closer than you might think. Now before we dive into how Snowflake, Iceberg, and Polaris make this possible, it is important to understand why interoperability has been a challenge. Each engine as you see in this diagram, Spark, Fling, Trino, Snowflake brings its own catalog, its own metadata layer, its own way of handling schema evolution and governance. The result, data silos, duplication, and a lot of late night troubleshooting. So, let's start there by by unpacking what makes interoperability so difficult in today's data ecosystem and why a shared catalog might be the missing piece. For years, organization have been building powerful data platforms, each with their own preferred tools, engine and storage choices. But underneath all that innovation, there is a persistent problem of catalog fragmentation. Every engine, whether it's Spark, Trino or Snowflake often comes with its own catalog implementation. Some uses an old high meta store, some rely on custom or vendor specific catalog. And the moment you do that, you create silos, pockets of data that can't be easily shared or discovered outside their native engine. So when teams try to combine workloads across engines, they end up duplicating data, reingesting it or rebuilding metadata just to make systems talk to each other. Then there's a problem of atomicity and concurrency. Catalogs are responsible for coordinating metadata updates, things like schema changes, partition management, and version commits. But without a shared coordination layer, if two engines write to the same data set at the same time, you risk metadata conflicts or worse corruption. Next comes governance and security. When you have multiple cataloges, each with its own way of defining metadata, you lose consistency. Policies like role level access, data masking, lineage tracking or tagging might exist in one catalog, but they don't automatically carry over to another. That makes it nearly impossible to enforce a unified governance model across all your data engines. And finally, data movement overhead. Without a unified catalog shared namespace, organization resort to moving or copying data between system just to make it accessible. Each copy add cost, latency and operational complexity. And ironically, the very act of moving data breaks the single source of promise of single source of truth. So let's look at how Snowflake open catalog powers multi-engine interoperability. But before I talk about the architecture of Snowflake open catalog, let's go back to the core challenge. Every data every data engine speaks a different language. Spark, Fling, Trino, Snowflake, they all understand data, but they don't always agree on how to describe it, evolve it, or govern it. And that's exactly where things starts to break. Apache ice was the first big step towards fixing that. It gave us a universal table format, one that defines how data is stored, versioned, and updated in object stores like S3, Azure, Blob, or Google Cloud Storage. But here is the catch. Even if everyone speaks the same language, they still need a common dictionary. That's where Apache Polaris and specifically Snowflake open catalog steps in. Polaris provides a vendor neutral rest pate catalog service that act as a single source of truth for table metadata, schema definitions and transactions. As you see in this diagram, one rest API provides a unified interface between engines and the catalog which point your data in the data lake. So when Sparks writes something, Trino and Snowflake immediately knows it's there. No duplication, no conflicts, no stale views. And then Snowflake takes this one step further. With Snowflake's open catalog and Snowflake horizon, we get enterprisegrade governance on top of the open data ecosystem. Policies like data masking, lineage, and tagging extend seamlessly across all engine. In summary, iceberg standardizes the table. Polaris unifies the catalog and snowflake ensures that it's all governed. So, how does this combination of snowflake, iceberg, and polaris actually unify data links? At its core, it's about creating a single source of truth with Polaris as the shared catalog. Multiple engine, snowflake, spark, trino, fling, all can coordinate over the same iceberg tables. No more duplicate data sets or stale copies. Everyone sees the same version of truth. It also gives organization engine agility. Teams can choose the best engine for each workload. Streaming, ML training or analytics without worrying about silos or data movement. Then there is governance and security. Through snowflake horizon enterprise policies like role level security masking and lineage can apply uniformly across data accessed by any engine. That means openness without sacrificing control. Because Polaris is open source, it also breaks down vendor lockin. Your catalog layer stays portable and independent even as your infrastructure evolves. And finally, it simplifies metadata management. Instead of building custom catalog or glue layers, Polaris provides a single production grade metadata service for all engines. So together, Snowflake, Iceberg, and Polaris deliver something we have been chasing for years. Open, governed, and interoperable data all built around one shared source of truth. Now we are about to see in action a demo of how Snowflake, Polaris and iceberg come together to form a truly interoperable data platform. In this demo architecture, we see data lake in Amazon S3 serve as a physical storage layer holding the iceberg tables data file and metadata. Snowflake and Apache Spark are the compute engine that both read from and write to iceberg tables. Apache polaris catalog maintains a namespace that organizes iceberg tables and ensures consistent table version schema and transaction across multiple engines. So the demo scenario looks like this. Snowflake creates iceberg table through polaris catalog under a given name. Snowflake then writes data into the iceberg table. Spark then reads data from the same iceberg table maintaining asset consistency. Now this is one of the scenario. In this demo today, I'll go through 10 different scenarios. Now, let's jump into the demo. [snorts] Throughout the demo, I'll be transitioning across four screens. The first screen represents Spark running within a Jupyter notebook. The second screen represents a Snowflake account which acts as another engine. The third screen represents a Snowflake open catalog account. and the fourth screen represent a data lake in Amazon S3. So the demo has too many moving part. My intent of showing you all this screen is so that while I go through the demo and transition across the screen, it makes sense for you to when I'm talking about which platform. So if you look at in this demo, I have a data lake on Amazon S3 and I'm navigating through the bucket structure here and I have a customer iceberg table T1. Now I am in the snowflake open catalog screen where I have a catalog called external catalog snowflake. This catalog is currently pointing to the data lake which I've shown in S3 here. And we have some configuration around how the base location of S3 which is which the date uh catalog is pointing to. It's an external catalog and we have some admin privileges that is granting access to the catalog. As I navigate through the connections, you have snowflake and spark as two engines which are talking to the catalog and that's the principal role which is assigned to these two engine. Now as I explore the catalog we'll see that we have polaris demo icework database and then we have a schema and a customer ice table which which we have seen in the now this is my snowflake uh screen where I'm showing the table and we could see that snowflake can query the iceberg catalog. Now this is my spark screen where I'm able to see the iceberg table while querying. So you see now we have this data lake on Amazon S3 and that can be referenced via the polaris catalog. Then I have snowflake and then spark running on my local machine which can query the iceberg table. Now I'll go through running my first scenario where I'll go ahead and create a new ice table called customer ice table T2. Now snowflake has successfully created the iceborg table. Here I'll go back to the uh data link and refresh. And now see the iceberg table T2 has been successfully created. I refresh the catalog and see the polaris catalog and list the iceberg table. I'll query snowflake. Now I'll refresh and query the my spark engine and see the spark can read the same table which was created by snowflake. Now I go through the scenario two where I'll be inserting some records into the table. So this query has recorded 150,000 record into the iceberg table. Let's go and query the table to CP content. You can see the iceberg table which was created. Snowflake is able to query the table and display the content. We'll go to the uh spark console and run the query and see if spark can read the content of the iceberg table. So see our spark is able to read the content of the same iceberg table which was created on snowflake. Now we'll go through another scenario where we will be updating the iceberg table with some record. So if you see this uh query has updated five records into the into the database. Now let's go ahead and query Spark. So you could see we have 150,5 records. So Spark is able to detect the changes to the five record which was added to the table. Now we'll run another scenario where I'm inserting a custom record into the table. So that record is created. I'll go ahead and check the count. So we have one record inserted in the customer iceberg table. When we query, we could see the customer table there in the now this is the spark console. We'll run the query and see if the new changes are there. So we could query the same customer key from spark as well. So you're seeing across this scenario the same table was created and updated across the two engine and it has giving a consistent visibility. Yeah, that's the end of my demo. I will move to the next screen. So let's talk about some best practices and recommendations. So when you begin your journey with Polaris and iceberg, the key is to start small and scale with intent. Begin with a pilot workload, a single domain or a few tables to validate interoperability, governance and performance before touching mission critical data. Keep an eye on catalog health, monitor transaction latency, commit conflicts and compaction metrics to ensure Polaris runs smoothly at scale. Then adopt schema discipline. Use iceberg schema evolution and partition carefully to avoid unnecessary metadata complexity. Lead with governance alignment. Test your access control filters and lineage consistently across Snowflake, Spark and other engine. And finally, stay close to the community. Polars in iceberg are evolving quickly. So track new religious connectors and best practices to stay ahead. Start small, observe carefully, and evolve deliberately. As we wrap up, I want to leave you with few reflections from past project and what they mean for you as you think about your own data platform. Across industries and use cases, I've seen the same pattern repeat. No matter how modern the stack looks on your diagram, platform starts to struggle where interoperability is an afterthought. Different team picks different engine data gets copied just for now. Catalog drifts and suddenly you are maintaining a platform instead of innovating on it. One key takeway for me has been that platforms rarely fail because of storage or compute. They fail because of fragmentation. fragmented engines, fragmented metadata, fragmented governance. Another lesson is that open formats alone aren't enough. Adopting something like iceberg is a huge step forward, but without a shared catalog and a unified governance layer, you are only solving part of the puzzle. And finally, the most important takeaway. Interoperability isn't a nice to have feature. It's a design principle. The most successful organization I have been working with are the ones that intentionally designing for multiple engine access, consistent governance and low friction collaboration from the beginning. With Snowflake, Apache, Iceberg and Polaris, we are now have the building blocks to do this much more deliberate robust way. My hope is that you leave today not just with the architecture pattern but with a mindset designed for interability from day one. [snorts] Thank you. Now we can go through some >> I have a lot that was fantastic. Um and I have I have some questions and >> maybe they're questions that people would want also. So you know one thing you were showing at the beginning was all of you know you got Flink, you got Spark, um Snowflake and then you have like the the storage. All of those could possibly be running on different um public clouds. Do they all have to be running on the same public cloud for that level of an architecture to work? Or if say your storage is in Azure but you're doing Snowflake on AWS, can that still be um that architecture still be used? >> Yes. So is the whole point is uh when we talk about interoperability, the whole point is you have your data lake in one cloud platform. we could you could still connect the iceberg uh tables with engines running on any platform. See it doesn't have to be but one point to notice when when we talk about snowflake open catalog or polar is there's a concept of external catalog and internal catalog. So when you decide that I am uh that is a key decision an organization needs to take whether they are using a because with iceberg there's an iceberg native catalog which each inent has spark has its native catalog for iceberg snowflake has it and if we decide to go ahead with a snowflake native catalog then what happens is there are some restrictions around read and write. So native catalog would allow you to write but then other engines to read. In this scenario, we are using an Apache or Polaris, which is an open catalog. In this scenario, you could see Snowflake can write to the table, but Spark can only read. >> Right? >> Now, if I if I turn the tables around and create an internal catalog, which is like uh an internal catalog in Polus means all engines like Spark, and Trino and Fling, they can write, but Snowflake can only read, >> right? So because this as I said this area is evolving. This is not like we not haven't seen the Nostar but soon we'll have a an ecosystem where all engines can read and write without this limitations. So you really need to have a good like you said when you were kind of like things to think about really going in with a with a with a strategy in terms of like is Snowflake doing all a lot of the heavy lifting but you just want to use Spark whatever to read the data right or the inverse or or whatever you know it's really you know one of my big things over the years was always about strategy stop think about what you're going to do before you go do it so that you don't put your put yourself in a corner Um because that was one of my questions was what if I've been using iceberg prior to snowflake supporting it for some time and then now snowflake comes we want to put snowflake um you know is that still possible to do to sit there and say now I'm going to put snowflake on top of what we've already been using from iceberg and and allow snowflake to um to interact with that data. >> Yeah. So which means uh if you're already using iceberg and which means you are not using a snowflake managed iceberg table you are doing an externally managed iceberg data so you can still uh via this pattern you can still have snowflake talk to the iceberg data lake. Yeah, >> but not manage it like which means as an organization you are already managing the iceberg catalog and >> which could some organizations tend to be flexible they want to manage inhouse rather than going propriety snowflake but as I said this is evolving uh and and and soon uh because interoperability isn't like it's it shouldn't be an afterthought it should be like thought upfront when you are designing your data platform Yeah, exactly. I mean, this is amazing because I can remember I'm only going to go back five years where I had architectural um not really battles, but me and and other architect in terms of, you know, Snowflake putting data in S3, they're writing Spark. Um, and what was tough was that they had their needs, I had my needs, and we couldn't do anything except try to make two copies of the data, right? And and it was, you know, are you primarily maintaining it, using it, and we just need to level your So, it was always this tugofwar like where are you going to put it? And this is great to sit here and say, well, we could put it in a location we can both work with it and both benefit from. So, u, that's pretty amazing. Yeah. Yeah. And uh if you for the audience if we know that snowflake has recently announced uh collaboration with other SAS vendors like we have in workday and Salesforce they are also trying to uh adopt interoperability and make zero copy data sharing across the platforms via this work which is great innovation. Imagine you are they don't have to copy data across platforms and share. Uh so that's uh that's amazing to see how this this um um ecosystem is evolving. >> Yeah. >> I would also like to say that uh there's a generative AI wave like it's a buzz word around and every organization is talking about agentic AI but I feel uh iceberg and data interability is the foundations of our data platform. >> Yeah. Have you seen um you know because I've heard organizations hey why not put everything in iceberg and not in snowflake. Have you seen a like an impact in terms of performance latency capabilities that you can do in snowflake that you don't have when inside Iceberg? like can you still do dynamic masking or or other things like that that maybe you wouldn't want to put in an iceberg if you really want to use these features inside snowflake. >> So if if if you look at uh it's a very good question Keith. So if you look at uh the data in terms of there was a benchmarking done by snowflake across query performances of native snowflake native tables versus iceberg table which are managed by snowflake and an external table and uh you would be surprised to see there was negligible performance difference between the native table and iceberg and especially uh a lot of people were worried about what happens the data is stored externally on on object store in any of the cloud platform and you have the snowflake power of Snowflake virtual warehouses compute engine querying and you would be amazed to see that the performance difference was nearly negligible. >> That's good. >> And and there's a nice white paper on this like comparing the uh query performance across and benchmarking and with the innovation around compute and now you have adaptive virtual warehouses and you're seeing this uh uh >> this would be a deal breaker in terms of performance. >> Yeah. So that's that's good to know. Oh, that's great. >> And your second question around data masking. So if you uh data masking is more native to Snowflake. So as we already see when the data is stored and as uh as as a user of snowflake when I log in and masking gets activated based on my role >> my current role right so if the spark is quering uh my iceberg table spark doesn't have that context of that role >> right so that restriction is still there so only a snowflake user within the snowflake ecosystem >> right >> can can do masking in my demo I did have the masking scenario where uh you could mask some com columns and apply a masking policy on an iceberg table which is stored in the object store and you still have your masking activated >> through horizon catalog. uh but while the reverse is not true uh spark has to then do an internal then >> I feel there's one abstraction above these native tool where we call the universal catalog at polaris catalog where you shift the role and access to your catalog level rather than native engine which means all engines go through the same roles and permission and apply masking at catalog like level not at native level and that would solve the problem. Yep. How about things like inside Snowflake zero copy cloning? Because I know when you do a zero copy clone, it doesn't physically copy the data. So does a zerocopy clone still work on a an iceberg table? >> So zero copy cloning was as you see the concept was your your data is stored in micro partitions and on snowflake and when you do a clone operation it's a metadata operation, right? Right? So you have a copy of metadata which is pointing to your old micro partitions. Now in for iceberg we don't have micro partition because data is stored on an external object store not within snowflake. >> Yeah. >> So uh I'm not sure if that would work like if you copy and referencing because you already have your iceborg table lying outside of the snowflake. >> Exactly. Great. This this was great. Um I it was very informative um educational and uh you know I'm kind of a bit jealous that you get to play with play with it. My my job on a daily basis I don't get to. So I'm living vicariously through you um and what you've put together. But I think this was this was great cuz maybe this will kind of soften some of the battles of of organizations battling across like multiple platforms and trying to move all their data to to one to one specific platform. So >> true. True. And then and then the biggest fear of organization around vendor lock in >> they at least don't have to worry about vendor lockin because they are investing in an open format and open catalog. So >> yeah and and data migration I mean it we're not moving megabytes or gigabytes of data anymore right it's terabytes upon terabytes you know pabytes upon pabytes and that's expensive to move around if you don't have to. Um that's that's even better. So um is there any way people can know more about you more information on what you shared with us today? >> Yeah, I have I have a medium page and a LinkedIn page. So if if anyone wants to uh connect with me, please connect with me on LinkedIn or follow my medium page. I uh I have published the content of the talk today into a medium blog. So which is good if you want to do an offline trade. Nice. And for everybody, we will um also put the links to everything that um Soros shared today right inside the comments below. Um but again, I want to thank you for joining me today. Um this was a very exciting topic. I hope everybody enjoyed it and again, thanks for coming. >> Thanks Keith for having me on this show. Uh it was my pleasure and I really enjoyed uh this conversation. >> Great. Thanks everybody. We'll see you all next episode. Bye. >> [music] "}
{"url": "https://www.youtube.com/watch?v=vnxHrD7VNp0", "title": "[LIVE] Vibe Code Data Apps - Part 2", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=iWAA7KprbpI", "title": "AWS S3 Tables To Snowflake Using Zero-ETL Iceberg Integration", "transcript": "Stop moving your data, start using it. Today, I'm going to show you how to achieve a true zero architecture. We're going to connect new AWS S3 tables directly to Snowflake using Apache iceberg risk catalog. No complex pipelines, no data copying, just pure open standard interoperability. Let's dive in. Hey builders, another day, another snowflake tip. Today I'm going to show you how you can access AWS S3 tables within Snowflake. I say we at Snowflake had made that thing super [snorts] super easy. Let me show you how. I assume that you already have an S3 tables created and all the necessary permissions given and IM rules created. So if you don't know how to do it, I have bunch of AWS references on the video description which you can follow to get created. For this demo, I'm going to use one S3 table bucket which I already created which is called as Kamesh S3 tables demo pin twins. It's a table that's going to hold set of pinwin data. Thanks to my friend Shannon who has given this data set and it's available publicly. I also put that uh data set on the description of this video. So first let's get go to uh Athena and then query this table make sure that we have data in it. Uh basically as you see here uh it uses S3 tables catalog. Under that I have my S3 table bucket name. Uh let's go query this one. See how many records we have. We are roughly around 333 records. And also let's query a few more thing one more like just five records to make sure that we can refer it back once we integrate with Snowflake. There we go. So we have bunch of records here as well. Before we actually create a catalog integration um in snowflake, the very first thing that I have to do is like go to lake formation within AWS and as a lake formation administrator. Go to administration and application integration settings and enable this checkbox saying allow external engine access. So this will allow us to access this S3 tables and underlying packets within snowflake and kind of using iceberg. The ne the very first step at snowflake side what you have to do is that I have to basically create this catalog integration catalog integration allows her to integrate uh the snowflake tables or snowflake catalog with external catalogs in this case it's going to be glue catalog so using catalog API type as glue giving catalog UR is going to use iceberg rest URI for glue which is this glue region a amazon.com/isberg I'm going to use a catalog name space which I have already created under my S3 tables as an iceberg table name space is a table format is iceberg and then the catalog name which basically uh your account ID followed by S3 tables catalog and um your cable bucket name if you remember that's what we used when we queried within Athena the very very important and crucial stuff here is that like how snowflake makes things super easy is using access delegation mode to vended credentials what does it mean is that by default when I two ended credentials AWS is going to give me a temporary credentials when kind of authenticating against this integration and then it's going to disappear after some time because it's temporary credentials. So I don't need to remember or I don't need to add credentials every time I create this integration. The next piece is basically is for rest authentication. These are all b uh things that belongs to iceberg specification. So if you have more details go refer iceberg documentation. I have also put that documentation in the description of the video. uh I needed uh basically IM role to access this uh S3 tables. That's what I've given here as part of the gist for this demo uh which I have again added on the description of the video. I have the uh the policy JSON that is required to have define this role and an external ID. Uh I'm using an hardcoded external ID here but uh if you leave it to blanch snowflake is going to create your uh an external ID which you can use as part of your trust relationship. Let's get started with creating this catalog integration. There you go. I got my catalog integration created. The next step I need to do is like uh I need to describe this particular catalog integration because I need two information from this. So I'm just going to go here describe catalog integration. Uh when you see this catalog integration, you need two information. API AWS um AM user on API AWS external ID. These two values we need to go and update your role back uh as part of the principles and the uh uh relationship external ID as part of your trust relationship. So just copy these values and go back to your Amazon AWS IM role and go and update them. So what I was meaning is that if you go to AM and if you go to the trust relationship you'll find this principle where we need to update API AWS AM user on and for example the condition for the external ID. This is where you need to update the a API AWS external ID which otherwise your integration will technically not be able to talk to each other. Okay, fine. So what's next? The next step is all creating nice book table. So let's let's go down. Uh I'm just going to go back here. Let me shift to this tab and then I just copy pasted it. Um just create an iceberg table. I'm calling this as glue penguins. Uh just to fluffix glue so that like it's easy for us to identify this. I'm pointing it back to this AWS uh S3 tables demo penguins catalog integration. Catalog table name. Uh if you remember that's what we queried in Athena. Uh that's the penguins table that I'm using. I'm doing auto refresh so that it the table underlying table is periodically auto refresh every time there's an update happening into the uh S3 tables. This go gets created. There we go. The tables penguins uh penguins got created. So let's query uh to verify that we can uh query all the data from snowflake. So I'm just going to uh take this little app uh so that like uh the queries are clear. I'm just going to go back here uh copy these queries. I put it here uh and then the first one I'm going to call select star obviously should get uh sorry it should be blue penguins. We got 333 records. And then I'm also going to do the same thing quing by record here. There we go. So we able now able to access all the records from the penguins's s3 tables within snowflake using iceberg catalog. And that's a demo. I'm Kame sat, lead developer advocate at Snowflake. If you want to try this yourself, check out the description below for the docs demo sources and other useful links. >> [music] "}
{"url": "https://www.youtube.com/watch?v=sY7_whjeyf0", "title": "Snowflake Intelligence in Action: AI Agent Implementation with adesso", "transcript": "[music] Hello everyone and welcome to the Snowflake intelligence demo. My name is Sebastian. I'm a cloud data engineer at Adesso which is a Snowflake elite partner and intelligence launch partner. We bring many years of project experience in implementing modern data and AI platforms. With our expertise in Snowflake intelligence, we enable AI powered data experience that are simple, scalable, and ready for the future. Maybe you know me from my last video on OpenFlow. Today we are looking at something completely new. In this demo, I will show you how Snowflake intelligence allows us to combine structured data, unstructured documents, and the built-in Cortex AI capabilities to deliver intelligent context area answers, including charts, summaries, and deep insights. But first, let's look at this higher level overview. It shows the basic components of intelligence. The basis is formed by agents which plan and execute complex complex task using various tools. For our demo, we built a product and process knowledge agent, the PMP agent. It uses the Cortex analyst as a tool, structured product and process data from tables as well as the Cortex search service and PDS containing compliance and sustainability guidelines. We have also added a custom tool that executes a Snowflake function to send emails. Really cool stuff. And I'm excited to show you how this agent works in action. So, let's jump straight to Snow's site. I'm now in the AI and ML section under agents. Here you have an overview of all intelligence agents including our PNP agent. You also have have the option of custom settings. Let's look into it. For example, you can define your own display name for your brand or write a welcome message to be displayed to the user. Under custom appearance, you can make graphical changes. You can select a new color theme or even upload your own logo. This is a great way to design your own chatbot UI. So, I will not show you step by step how we created each individual service and agent as there are wonderful Snowflake hands-on guides and a detailed step-by-step guide in the Snowflake documentation. But I would like to show you what we configured and why to give you a kind of template for your own use. When I click on the agent, we get to this overview of our agent. You can see the name, the display name, and I defined a short description which is which is only for the users and provides general context. So let's not waste too much time here and move on to the next step. The first important point which can be very helpful is to set rules for how the agent should sound and responds to the users. For example, I wrote, \"Please respond in a clear, concise, and professional tone, perfect for our customers whenever it's helpful and supported, generate a chart, and so on. I highly recommend this. It clearly improves the chat experience.\" Then we have the three tools I mentioned in the highle overview at the beginning. First, I added the Cortex analyst. And as you can see, a really detailed description of the underlying data. But don't worry, Snowflake offers a truly innovative feature here. If I go to edit and then to tools, I can show you that I did not write this description by myself. You can get to write a complete very detailed description to guide the agent via Cortex AI. It automatically analyzes the tables, the columns, and the data to create a good list. The only thing I added myself is this final sentence with a summary of when the agent should use this tool. Then you can select the warehouse to be used and set a query time out to protect um against long query execution and save costs. So the same um applies to the Cortex search service. So it's not too difficult. The settings for the custom tool are slightly different. So let's take a quick look at those as well. The type is either a procedure or a function. Up to this point, it's almost identical to the other settings. And again I generate again this um description using Cortex AI. What is added here are parameters that can be passed to the function. My send mail um procedure has three input parameters. The actual message of the email as the body, the recipient email addresses which could also be an array and the subject of the email. The last two parameters are optional and the description states that if they are empty, a default value will be set. In this case, the user's default email addresses. The same applies to the subject. The only required one is the body to ensure that no um empty emails are sent. And I defined that I want to use HTML syntax for this. So that's all to using these tools efficiently. The last important point about agent is a topic of orchestration. Here we define how the agent reasons, choose task, chooses the right tools and sequences actions. You can choose a specific model in the settings if you want, but I don't know if there are any significant differences. So I will let Snowflake decide which model is the best choice. In this description, it is again important to tell the agent exactly when to execute which tool and to access access either structured or unstructured data. If you want, you can pause this video at this point to read these instructions carefully. So after several deployments and tests, I can tell you that these configurations work really well and deliver great results. Feel free to use them as a template for your own agent if you like and try them out of yourself. But enough theory. Let's move on to the Snowflake intelligence and see this agent in action. By clicking here, you will be taken to the intelligence UI where you can start chatting right away. You have the option to upload files, select the current agent, and select the source, for example, the two we created, but I would recommend leaving it on auto select as a rule. So the first thing I want to know is how many products were produced in each year and which year shows the highest production volume. The agent starts directly analyzing my request, planning the next steps including intermediate steps and finally providing a result as a text. Before we get to the result, you can view the entire tour process. Here we see how the agent responds to the request, which tool it chooses to obtain the data and answer the question. And finally, it then generates in secret query and executes on the data stored in snowflake. Then it concides further steps. Should I visualize it? Yes, because we said in the response instructions, please visualize every answer if it's helpful and possible. [snorts] Now, we see the total products produced by year and 2023 was the year with the highest production. He also created a graph which I can copy directly or download as an image and integrate it directly into dashboards. Um here we have also a structure table view uh which can also be downloaded as a CSV file. Below the chart our response instruction now provide us with um very helpful summary with an interpretation from the agent itself. On the one hand, it says that the production volume is declining from year to year and notes that the data for 2025 is not yet complete as it's only November. Really cool. Okay, let's ask one last question. This time I'm going back to the examples example questions and want to know um did we meet the 2024 CO2 reduction target compared to the 2023 one baseline and by how much did we over or undershoot this target? To answer this question, the agent must use both tools because we need the structured and the unstructured data from the PDS. Let's see if he can do it. So if he's done, let's take a first look at the details again. And as you can see, he analyzes the request and decides to first search for the reduction goals in the documents. Now that he has found them, he realizes that he needs the current emission data for comparison and using the analyst tool. Then he generates the SQL for the structured data again gets the data and decides to visualize the result. We have also hyperlinks for the documents. Now the quick answer that we have achieved um is the reduction target for 2024 and a detailed list of the values. The performance summary gives us another perfect overview of the result. And finally, the graphic and the final interpretation. Wow, I'm so happy with the results that I would like to have them sent directly to me by email. So for this reason I write, hey please send me a complete overview of these results with the performance summary to my email address. To conclude this demo, I would like to show you how you can send emails directly via chat, which I think is a cool way to send the resource to the right people. So now he's a bit thinking and yeah, finally he is done and sent an answer that he um sent this email successfully and uh what the email includes. So now I re received the email and um yeah as you can see it includes um the CO CO2 reduction target performance summary for the year 2024. We have some informations um such as the requirements of the policy target. Um we have also a structured answer. So a structured table of each emission data and uh what we received. We have also the key performance indicator and a final conclusion by the agent. So this email could really be a base uh for a report and I think it's really cool. Thanks for watching this demo. I hope you enjoyed it and that the video helps you to set up snowflake intelligence yourself. Intelligence is now general available. So get in touch with it on snow site or visit the snowflake documentation. And of course if you have any question let me know in the comments and don't forget to like this video. [music] "}
{"url": "https://www.youtube.com/watch?v=cmT9_Tc6AF4", "title": "[LIVE] Build Autonomous Lakehouse Pipelines with Dynamic Tables & Apache Iceberg", "transcript": "Heat. Hey, Heat. All right, welcome everyone. We are live. Thank you so much for joining our LinkedIn live stream today. My name is Gilberto Hernandez. I'm a lead developer advocate here at Snowflake. I'm really excited because today we're going to be talking about a couple of features that I love to use within Snowflake. So, first is dynamic tables. You may be familiar uh with them. You may have heard of them, but they have kind of grown in scope to add new features and capabilities recently. So, we're going to cover a bit of that today. The second thing we're going to cover is Iceberg, uh everyone's favorite open table format. And we're also going to talk about how these two come together in Snowflake to help uh power your lakehouse pipelines. Um, so before we get we have quite a bit to cover, but before we get into it, just want to give folks a heads up. This is a live event. So if you have questions, if you have comments, uh, feel free to drop them if whether you're watching via LinkedIn or YouTube, we'll be able to see your comments, we'll be able to see your questions, and then towards the end, we're going to have a Q&A session where, uh, we're going to try to answer as many questions as possible. And if we're not able to get to your question today, we will come back to your question and answer it, um, on either LinkedIn or YouTube. So again, this is live event. Please go ahead and drop in your questions and comments um throughout the show. Um all right, so let's go ahead and kick things off. Uh today I am joined by my colleague and expert uh here at Snowflake on these uh features. His name is Parog Jane. Parog, welcome. Thank you so much for being here. Um before we kind of get into the agenda, I wanted to give you uh you know just a few seconds to introduce yourself. >> Sure. Thank you Gulberto for inviting me for this live demo. Um I'm Prague Jane. I work as a field CTO in Snowflake and uh primarily for data engineering. So I take care of all the um transformation and compute and data engineering features in Snowflake. >> Awesome. Yeah. All right. We're we're excited to have you here. Um as I mentioned, we're going to be covering dynamic tables. We're going to be covering iceberg. So, I figured that before we get into um our live demo for uh later today, maybe we can set the stage for folks and just give them um a quick primer on the couple things that we're going to be covering today. And maybe we could start with iceberg uh perog and maybe give us an overview for folks who aren't familiar, maybe they've heard of it, maybe they're curious about it, what iceberg is um and I'll go ahead and hand it over to you so you can walk us through that. >> That's fantastic. Yeah, sure. So I do have a couple of slides for that and it will interesting to just see um how these two features combine and then get to get along together. Uh so before we move on this is just a safe harbor disclaimer. There are some uh slides that might be futuristic maybe talking about some future um features um but that should be okay. Um before we jump onto dynamic table, I just want to make sure that everybody understand the open table format um and iceberg tables. So iceberg tables have been kind of taking the world for the lakehouse uh open table data formats uh because of their cool uh features that they come with. They have schema evolution. There is no hidden partitioning like hive tables. They come with built-in time travel um snapshotting. They also come with built built-in increased performance because they provide very fast filtering and scanning. Now some of the folks might already be aware of done of iceberg table because it is so popular uh and a hot topic in in the industry right now in terms of data engineering and lakehouse. Uh it also provides very flexible cataloging and that's one of the reason why Snowflake has adopted it because it does provide an open table format plus an open catalog uh feature as well and Snowflake has released its own open uh catalog as well in terms of REST uh API based cataloges. We uh also support positional deletes in iceberg tables. So that also comes in very handy when you're building a pipeline and when you want to actually uh make fast changes to your data. Now based on what we have for today, it's important to understand um how and what dynamic tables uh work and behave with iceberg table. So before we go there, just a quick primer on dynamic tables. Um Snowflake has uh introduced dynamic tables couple of years back. They are a declarative way to actually create an ETL framework and a batch-like pipeline or a streaming pipeline. They are really cool because they do provide full observability and visualization using DAG. Um they can uh we can use them to create alerts. We can use them to create data quality uh pipelines. You can use them to create any incremental or or full refresh pipelines between different tables, different base tables, even between dynamic tables as well. So if you create um a dynamic uh table which is actually a physical physically stored data similar to what we use it for malized view uh but has many enhanced capabilities including scheduling uh capabilities and orchestration cap capabilities as well. Um so dynamic table does come in very handy and we have been using it for many many years uh now. Um but very recently uh we have introduced dynamic icebrook tables which are dynamic tables but now but now you can build icebrook tables using dynamic tables. So we'll see that in in in a demo as well. Um at the at the core dynamic table provides three important features. one is it can store your results and let me turn on my um laser lights. It can provide you uh it it provides you a way to store the results. So it's not just like a SQL query overview but it actually stores the results. It can automatically refresh based on a time lag. So that's a very important feature because many times you receive data and then the pipeline runs whenever whatever maybe four hours five hours and you still have data. So instead of that you can do an automatic refresh as soon as your base table has data your u dynamic table can um store the data using a pipeline that you built using select statement. So you can add any query in the select statement. We do um support many uh different functions like window functions as well uh lateral joints as well. Um when when the time comes to actually query dynamic table there is no magic you can just use simple plain old SQL select statements to query the data based on your data freshness and lag you will get the data at that point in time. It also gives you a snapshot isolation as well. So u when even if the dynamic table is running behind the scene if it is already as materialized a previous result you get to see those results even if it is running behind the scene even if it's running the data pipeline behind the scene. So those days are gone where you actually have to clone your data because your pipeline is running. So people don't see different results that is now built in the snapshot isolation is built into uh dynamic tables. do not have to do all that. So it gives you a very good um set of operations so that you do not have that burden of operations overhead. Um when you actually build a pipeline using different tools maybe store procedures task uh or you have an airflow pipeline running some SQL scripts you don't don't have to do all that the infrastructure is gone plus the operational burden is gone. Now you get these things built in into uh something like dynamic table. So here are some key key highlights how other customers in of snowflake are using dynamic table for. So it's a very good tool for low latency freshness. So if you're receiving your data um let's say from a streaming pipeline and you want to build upon that data and present this to your customer, you can go as little as single digit second to to a minute. uh you can build declarative pipeline for something like iceberg table. You can build declarative pipeline with python support. You can um observe that pipeline through a single monitoring screen. You don't have to switch over from for example if you have airflow pipeline you have to switch over to airflow to monitor the pipeline see the logs if something has failed. But here in dynamic table you get a singular view of your DAG and of your pipeline through a a monitoring screen. Uh it also create a sort of dependency graph. This dependency graph is live. Basically you can click on these objects. You can pause your pipeline, resume your pipeline. You can see what's the code used in that pipeline as well. Change the lag as well. So at any point of time you get this view and then it's it becomes very easy for an engineer to to delegate this to uh this operational team. So you take away that pain of um you know handholding your operation team with the development team. So you can you get this you know separate segregation of duties for um different teams as well uh with that view. That's just one example but it gives a nice observability of your pipeline live on uh your screen. Um it supports joins window functions like I said before u aggregations and views. It gives almost everything um in your uh war chest to to create a pipeline. [snorts] Now with dynamic table um the support for iceberg table is one thing but we are also working on many different things. Um with dynamic table to to increase the power of uh dynamic table one of the few things that I like about uh this new power is immutability. So imagine you you're creating a table and um this is a live table. This is the dynamic table which you will incrementally refresh but you don't want to change certain data because let's say for compliance reason if if there if you have five years worth of data and you do not want to change last three years worth of data uh in that pipeline then you can make that region of your table immutable. That's a really cool feature. So it restricts any any sort of insert, update, deletes on on that region. You can now also uh do outer joins with with high highly performant uh dynamic tables. Um you can um operate on top of clustered keys. You can operate on top of mask table. So you get the same masking in dynamic table built-in as well. uh so all these are very cool features which uh which gives you more control uh on your data and also gives you high performance. So this is the this is the thing about governance that I was talking about if your base table or on your source table where you drive your dynamic table on if it has these uh governance policies those governance policies are easy to transform into uh dynamic table as well. So you don't lose that governance, you get that singular governance. And based on what we just uh saw, there are many many uh use cases of dynamic table. Some of them are listed um on your screen. Um all these use cases have been battle tested so far in dynamic table and they've worked great. U so um I'd highly recommend dynamic table with iceberg table or without iceberg table to to everyone. Uh back to you Gilbert. >> Awesome. Thank you so much, Prague. Um, you covered quite a bit there. So, I appreciate the primer. It sounds like dynamic tables pretty easy to quick up uh pick up because you're just defining them with SQL. Um, you can schedule them based on your use case and you don't have to use any third party orchestration tools to do that. You have a native way of doing that directly within Snowflake. >> Observability too, right? It sounds like once you set them up, you can go ahead and check out the DAG, see how they're linked, see the relationships with tables. That's pretty cool. One thing that um I tend to hear from builders out there when we talk about dynamic tables um you know frequently we'll we'll talk about why uh how they are declarative by nature um and I'm curious if you could just elaborate a little bit more on that. So uh you know in your eyes why is a declarative uh approach uh for pipeline so powerful? Yeah. So it it actually abstract away the pain, right? So for example, um compare this with uh let's say an ETL tool and it it's a GI based ETL tool where you're dragging and dropping and creating your um pipeline through um through some GUI and then ultimately you create a workflow and then you deploy that workflow and then orchestrate that workflow. There are multiplestep processes. But if you look at um a data platform like Snowflake and you draw ETL pipeline using a GI tool that ultimately creates a SQL and then pushes down to to Snowflake. If you have hands-on that SQL and if you can just use that SQL to create a pipeline that's magic that's something uh that that snowflake has brought to um to you now and that's what the declarative nature of pipeline is. You can declare your pipeline using a simple select statement. Uh it's basically a view that you want to see of your data and then snowflake make it happens create a physical uh stored table out of that view. >> Right? So rather than having a bunch of different scripts and steps that you might have to take, you basically have a pipeline defined in an object which is pretty cool and easy to easier to maintain I should say. >> Yeah. One more thing I wanted to call out is um they're incremental, right? So they're going to process only the new changes that have landed in the source data. Um I think you know that's that's been around since dynamic tables has launched. I think another cool thing that folks should consider um as as they use this feature is is immutability which you touched on a bit, right? >> So can you touch a little bit just quickly on how you can uh you know how immutability helps incremental refreshes? Yeah, I I see a very defined use case of immutability is is compliance. Uh there there are many other use cases as well um of um immutability. But let's take comp compliance for example. um you work in a in a financial organizations and you have to uh make sure that your data doesn't change for past year because you already reported let's say your quarters or your ears in your 10K and you don't want that data to change and you have a dynamic table pipeline. Now errors can happen because we are all human. So even with that error, you do not want to update your previous data sets or or historical data sets and that immutability through dynamic table gives you that tool to lock and secure that data even if anything changes in the source. U so that's where immutability comes very handy. it up. It it won't let you do uh inserts and updates or deletes on your immutable region and that can be defined by date that can be defined by any column or filter value. >> Awesome. Yeah, I love that. Um it's a simple wear clause, right, for most of these use cases which is really cool, easy to implement. So >> yes, >> cool. Um I think we're ready to to hop into our live demo. Parog, >> right? So you're going to be uh so Parag is going to be showing us a pipeline he's built with dynamic iceberg tables and he'll kind of showcase some of the features that he just discussed along the way. So I'll hand it over to you Pog. Thank you. >> Awesome. Thank you Gilberto. So uh before we jump onto the demo, yes, there is a hands-on lab available uh for this uh for you to try and test. Uh the link for uh this hands-on lab is here. It's it's my GitHub repo uh and all the source code is there and I'm going to walk you through this this source code and this uh demo architecture as well. Um so for this demo we're going to have uh two tables as our base table and they are iceberg tables. You can have them as snowflake tables as well but they are iceberg tables. One of them is products and other is orders. Uh think about a a simple uh e-commerce website where um you were selling your products and you're getting orders from different uh customers and you want to generate some uh metrics out of it. You want to generate how the my how my sales are doing. So you want to do some kind of sales summary report. You also want to enrich your orders table based on the product just to uh figure out if if you're not running low on inventory for for a certain product. So that's the goal layer that we are going to build. And this circle sign inside a a box is is our representative of a dynamic table. So we are creating a dynamic table but it's iceberg. So that's why we calling it dit dynamic iceberg table. U so ultimately your gold layer will be an iceberg table. Your bronze layer is already an iceberg table. What is the benefit of this? It gives you data in your control. It's uh it's there in the open storage format. Meaning it's in the iceberg table. At any point of time you can connect any external system without snowflake to this iceberg table. You do not need to connect to snowflake to read these tables. It gives you an iceberg built-in iceberg rest API based catalog. Many many tools and platforms support uh iceberg rest catalog. So you can connect to snowflake seamlessly. It gives you built-in manage data pipeline and in dynamic tables. So you can observe your tables, build incremental data. You can um you can have interoperability with the iceberg tables. You can centralize the governance uh access control. All that thing that doesn't come with iceberg table are provided by snowflake including a dynamic way to build your iceberg table. So let's just jump onto this demo and u while I'm presenting this demo if required I can switch back to uh this image as well just to u show you in which step of the pipeline we are. All right. Okay. So, let's go to the workspace. This is uh snowflake snow site. Uh in its full it looks like this. Um what I have done is I've went to projects workspace, right? And I'm going to minimize the panels that we don't need. And I I have opened up uh one of the the labs that that that I've built in uh my GitHub repo. So step by step there are many SQL files and they are numbered so you won't be lost. So you can start with uh setup then uh create source tables then create dynamic tables and then provide different operations for your dynamic table. What I've done is I've listed all these steps in a notebook file and Snowflake workspace supports both SQL and Python uh notebooks. So I put that in the notebook file because it's easier for me to demo and talk about uh the demo. So I put that in the uh notebook. I can share the notebook as well in the um hands-on lab. Uh I haven't but I can share it. It's basically the exact same SQL. So what do we need for our dynamic uh iceberg tables? Uh we first need a dedicated database uh and a schema and then um and a warehouse to to run our compute. So that's what I've created here. The database I'm calling uh DTI's demo and schema is demo. And then I'm using the warehouse that I've created earlier. Now this step is crucial because your data resides in a chosen bucket um customer chosen bucket you have to connect to that bucket and this is how you can uh let snowflake know that this is where my data should reside. So you you'll create something called as external volume now within few um sentences and within few uh properties. So you will provide your S3 bucket, you'll provide your role where Snowflake can use that role and connect to that u external uh volume and any safe keyword that you want to uh disclose. This way you can uh create the external volume. Now this is not just one step. Uh once you create the external volume, you have to let your CSP cloud service provider go back to CL cloud service provider and approve uh this external volume. So it's very secure. It's not just one way to simply just get the AR and connect but you also have to approve this connection and the step for that is also mentioned in the um in the file as well but it's super easy you will allow right so Snowflake and write to their uh that as S3 bucket this role should have write privileges on the S3 bucket as well once you create that. So let's just quickly run this just to see if everything is in place. All right. So it looks like we have our um database schema and external volume created. Let's move on to the next step. Now in this step um as as discussed we have a bronze layer to create like which is our product layer and the order layer [clears throat] and these two tables are actually iceberg tables. Super simple to create in snowflake. All you need is just create or replace statement. Everybody should be familiar of this. Um when you create an iceberg table you need to provide the external volume. This is the same external volume that we just created for catalog. it will be snowflake. Snowflake creates iceberg rest catalog by default. So for you to connect with any external uh platform or system like for example spark if you want to connect to a uh through another database let's say Oracle wants to connect to this uh iceberg table you can do that um as well any platform that supports iceberg rest catalog should be able to read this very seamlessly [clears throat] now within our base location um and let me just go to my base location that I'm using I'm using this bucket in uh in snowflake and let me just refresh right now there There's nothing in this um bucket. And what I'm going to do is I'm going to create my first table which is uh the products table. It's created and then I'm going to insert some data in it. All right. Looks like we have data there. So now we'll just refresh. And there you go. Uh we now have the icebook table created in my chosen bucket. And this table has two things two up two folders. One is data and the other is metadata. And uh metadata has basic um iceberg metadata with a files metadata.json file as well. Then we have in the data folder we have data set the data that we just loaded. So pretty cool. We have our first iceberg table which is products table. Let's move on and create another iceberg table which is my orders uh another base table that I want. Let's just create this and it's going to be in the orders folder. So let's go back to our S3 bucket. Go into tables and now we have orders bucket orders folder as well. And since there is no data there is only metadata. We haven't loaded this table. So we only see metadata. Let's load this table a little bit with some data. There you go. And by the way, I'm using uh a very small compute uh for this. And let's go back and refresh. And there we go. We now have uh data as well. All right. [snorts] So let's quickly um check the content of these two table because we're going to need that in our demo. So for products I have different um products listed. These are all fake products that I just created. Um they have a category. So for example, laptop is electronic category and then I have some furniture um category as well and then some accessories as well. They have listed unit prices and the create on date. And in my orders table I have customer ID which have ordered the product and uh number of quantity um for that product and some region information as well. Now one cool thing about uh the orders table is it marks the order status as completed and shipped. We're going to need this in the uh future. So I'm just going to highlight this [clears throat] order status uh for now. Um, next is let's check total number of rows. So in my product table I have 10 rows and in my orders table I have 15. Cool. All right. Now this is our first dynamic table. What I'm going to do here is I'm going to create a iceberg table using a select statement. And that's why it's called dynamic iceberg table because that select statement is basically uh trying to query these two tables orders and products to create my iceberg table. As and when I load new data to orders and product table my dynamic table will refresh incrementally only for this newly added data. All right. So let's let's create this. It's basically simple um math creating a total amount uh using the quantity and unit price because the amount is missing in my orders table. So I'm enriching my orders table with actually uh a product name, its category and the total amount. So a customer has paid so much for buying this uh product and it's based on the unit price um of the product, right? very simple SQL. Um so I've just created it. Now the beauty is um in the beginning when you actually create the dynamic table, it actually creates the data as well. So let's go back. There we go. Now we have orders detail and it has data and metadata. So let's run a quick query to see if um we have data in this table. So I'll just add a SQL and do a select star from orders [snorts] detail did right let me just quickly check the name it's order details not orders details just copy paste and then run [clears throat] there we go so now the enriched order table which is order details DIT has not just product ID but product name and it also has a total amount. All right, moving ahead now. Um, one of the requests that I get from my business team is detail about the product how we are uh how much are we doing in the in terms of sale for individual products? What category is is the top selling categories and all. So for that I need uh a fact table uh which has summarized uh results for all the products. So what I'm going to do is create this summary table with my um aggregated value for different fields of product and orders. So let's do this now. Again [clears throat] uh this is a dynamic table so it should automatically create the data. There you go. So if I go to order not the order details sorry the product sales summary I do see data and metadata. All right that's fantastic. So now let's moving on to different table. Now one of the requests that I got from my um business team is they want to see regional sales performance and for that we have to create metrics like um metrics per region for different products that they have sold. So I've created a different SQL and I'm I'm going to use this SQL for this dynamic table. So let's run it. Few things to to observe in all these dynamic table is it has something called as target lag. Uh target lag even if it is 30 minutes and let's say your base tables do not have data after 30 minutes do not have new data after 30 minutes this will skip. So even if you have 30 minutes or one one minute or 10 minutes if your base table doesn't have new data there is no point in running dynamic table. So you pay $0 to snowflake for that. there is no compute consumption even if your target lag is as small as one minute unless there is data in the base table unless there's change data in the base table. So that's a very cool feature of dynamic table that even if um you know all those cases where you have u you know data coming in let's say 2 p.m. it comes a bust of data and then 2:30 it comes bust of data but then 4 p.m. there is no data um then this dynamic table will have zero credits consumption there is no cost that you will pay even if your pipeline running every 30 minutes >> bar just really quickly I want to because I think that's a really good point uh for folks to understand in addition to that >> um remember this is a target lag so it's like a maximum amount you know of time that you're comfortable with that you want your dynamic table to lag the source by so um if you are using a dynamic table and say that you have set this to 30 minutes or 1 hour. An update can happen sooner than that as I understand it. Prague, right? It can be anywhere in that window of up to 30 minutes in this case. >> That's absolutely right. And and that's that's where the um that's where it's different from regular orchestrators or schedulers. Schedulers or chron jobs run at a definite time. And dynamic table as soon as it sees the data is full, the new data is full. So it's not going to start at the first row. as it sees there is more insert or merge happening in the base table. Once that is done, then the dynamic table is going to start. It's not going to start at every row injection. It's it's going to wait for that target lag. Now, within that target lag, if your injection is complete in 10 minutes, it won't wait for 30 minutes. It will just start the uh updates. So, it's really cool and very good point. Uh thanks, Gilberto. So okay this table is also completed and this table has quite a few queries because it's a regionalbased matrix. So each region have different matrix and revenue categories that we are building. Now let's quickly go back to our S3 bucket check if this table is also available. There you go. Now we have all the five tables, two base tables and three dynamic tables available. Now I talked about monitoring and um observability [clears throat] um in the pipeline. So how how does it look like? So if I go here um there you go. I now have a database and if I look at my dynamic iceberg tables I have two base tables. Now these are ice work tables but then in the dynamic table section I have three [clears throat] tables that I um that I created recently. Let's look at one of them. This is the view [clears throat] that I was talking about in terms of observability is it creates this dagike structure very similar to what you will see in uh airflow or airflow like um tools where you can see the um interconnection between a daisy chain between um your base table and your dynamic table. Now there are a few things that you can do here. You can either um refresh this manually. So think about an operational engineer. Something failed and then you receive new data in the orders or product table and you don't want to wait for that target leg. You can just refresh it manually. You can also from here see the details of the dynamic table. It it will give you a definition of that dynamic table as well. You can also suspend. So for any reason uh your your your base tables are in maintenance or your dynamic table needs some maintenance, you can suspend this pipeline and start again. when whenever required. So gives you a very good u summary view of your uh dynamic table graph in the refresh history. It gives you the pipeline when it will run whether it was skipped or or rerun [clears throat] all that is possible through through this view single view as well. Um it gives you up to seven days of view. Um gives you all the uh refresh tables. So it has a built-in uh observability dashboard as well. >> That's really cool. And >> today I learned you can refresh the table from the UI in in the DAG. That's pretty cool. I was I was running alter table refresh statements in the work. But >> that's pretty cool. And and just to give folks a heads up too, uh Barack, you just showed how to see this in the UI, >> you can do those exact same things. So refreshing and checking out the refresh history with SQL statements too, right? So this is available to you as well if you want to check it out. Um writing a SQL query. >> Absolutely. Yeah. So [clears throat] I have those uh SQL statements as well uh for uh to review if you want to run it. Um and it's there in the GitHub repo as well. Um this is one of the um SQL. You can just run the SQL to find out what's what was um happening in my um dynamic table and it has many details listed out because many customers what they do is um in terms of data pipeline they take the observability to a singular um dashboard somewhere outside of Snowflake. So this query comes in handy to pull that information from Snowflake. >> That's handy. And as you think about combining this too with um alerts and notifications, right? You could do that based on refresh history. Maybe you expected this to refresh incrementally or for something and didn't happen. Boom. Like you can get a notification, tie that up to Slack if you wanted to as an example. Um very neat. Very neat. >> Yeah. Yeah. Absolutely. And uh so I just insert few more records in our orders table just to see if my dynamic table is refreshed. And um here it goes. The three rows that I just added are now there in my dynamic table. So it it does it incrementally fast. Um as soon as you insert new record you can refresh your dynamic table immediately. Um now u in this section I'm going to talk about the immutability that that that we have that that's that comes built in with dynamic table. The only thing that changes so for example I have this u new dynamic table to showcase immutability. The only thing that changes in uh statement is this weird loss. Remember the we had a column called order status. Now what I want to do is create a analytic dashboard and it should not change if the order status is completed. So if there is an order where the order status completed uh even if you change something for that order in base table the status uh will preserve the data uh in dynamic table. So base table might change but the order table um but the sorry the analytical table will not change for completed status. So let's create this iceberg table. All right. And let's um check uh if if there is an immutable region. Yes. So there is an immutable region. This is the orders column. What I'm doing is trying to query that same exact table and trying to find out if it if the column is immutable or not. And what's the value of that immutability. So just a simple SQL query. It's it tells me that if there is auto state is completed, it's immutable and everything else is mutable. Very cool. So >> that's amazing. It also yeah it also can tell me the number of rows. So 11 rows are completed. So that's why it's immutable now. Um >> and this scales right really easily because you could imagine you have millions of records with order completed true. So you locked them up just like that. Yeah. >> Yeah. Um now here I'm going to insert two records. One uh two records in pending status. And then if you run this query, so this was there were five pending records. So if I run this query again should show me seven now and 11 are still immutable. Now if uh if you notice the order ID is same but it did not updated the the uh completed. So now let's update the order status to completed and see what happens to the immutable region. There you go. So if you look here because [clears throat] the order uh status is pending and we tried to uh update um a completed status it didn't allow you to do that because it is immutable. So um these are really cool features that are built into icebook table for you to actually write a SQL or for you to actually build this into your uh pipeline. It will take many different uh iterations. It will take many different stages to actually do that. You probably have to copy data somewhere then do a swap of some some other data set and then make your pipeline whole again. Uh so all these things now built into uh your declarative pipeline. Really cool. Very nice. >> You can also choose a fixed cutoff for immutability. Uh so in another example I have my sales cutff. So same example where I gave you about the um quarterly results. So imagine every quarter you declare your results wall street and you don't want to change that data. So for that you can make that region immutable and this is changeable. You can come back and change the immutability region to let's say next quarters can can be roll rolling. So you can make that all uh possible. Um and that is it for dynamic table. Now few things to um keep in mind that uh we have certain features that still do not work with dynamic iceberg table and one of them is backfill. Back fill is not allowed in iceberg table but it is allowed in the regular snowflake table. Um and those things I've listed out in the um readme file as well. So you'll see those uh limitations in the in the readme file. >> That's super helpful. Uh great job. That was a very comprehensive demo. Really appreciate how you covered both aspects. Talked about dynamic tables, iceberg and then how you can do iceberg dynamic tables which is amazing. Um we're going to be sharing out the uh resources um via the channels today. So on LinkedIn and YouTube. So if you are interested in those check them out. So, we're going to share the repo that Perog just walked through um that includes all the SQL that you ran. Um I think you're also going to be updating it uh with that notebook as well, right, Parog? So, so keep an eye out for that. Um we're also going to be sharing a uh quick start guide that you can use to get started. So, if you're interested in trying out dynamic tables, um building autonomous pipelines to feed kind of your AI agent in Snowflake, check that one out. Um it's fresh, it's brand new. Check it out. Give us your feedback. But you can get started with dynamic tables really easily. Um so I know we're almost at time so I don't think I don't know if we're gonna have time for a lot of questions. I do want to ask you one perog. It's one around um you know that I hear frequently from builders who um have built out pipelines today and they have built out many materialized views as an example and maybe this notion of dynamic tables. You know that's that might be new to them. So I'm curious for folks who are actively um wanting to migrate from m uh materialized views over to dynamic tables maybe um a couple of questions you know the first is you know how how are they similar how they different you know why why would why might I pick a dynamic table over a materialized view. Yeah. So, Snowflake does support both. Uh but materialized views can be created only on a singular table. Whereas dynamic table you can have multiple joins for any number of tables. That's one thing. Uh the [clears throat] materialized view pipeline is not so declarative. But the dynamic view dynamic table pipeline is uh declarative and incremental super fast because it keeps the track of the changed rows in the uh base tables. Awesome. Um, and what could a you know migration of a materialized view object into a dynamic table look like? Is it is it as easy as it looks of you know just taking that that uh definition for the materialized view and maybe um putting it into the definition for dynamic >> view. Yeah. No 100%. um the materialized views from other products and platforms like for example if you created materialized view in Oracle if you have created materialized view in Niza those materialized views can we can just copy the select statement and then do the same thing in dynamic table just paste it in dynamic table and you can do it um so drop in replacement u but one thing to keep in mind if your modularized view is super complex you probably want to break it into a couple of dynamic table and daisy chain them together to get to your effect or to get to your goal layer. That that makes it easier for dynamic table to to peacemeal and digest the information and then build your final outcome. >> Awesome. Love to hear that. Um thank you again for the demo. Really appreciate it. Thank you so much for being here, Barack. We're at time so we're going to go ahead and wrap. Uh again, folks, uh if you have any more questions, ask them on the channel. We'll go back and answer them asynchronously. We can also provide the links to the GitHub repo and to the quick start. But just want to say thank you again, Perog. Uh thank you uh viewers for tuning in. Appreciate it. And I will see you next time. >> Thank you. Bye. "}
{"url": "https://www.youtube.com/watch?v=bLmXh36FbUo", "title": "Introducing the 2026 Snowflake Data Superheroes!", "transcript": "Good day. I'm Amy Leno reporting live from Snowflake [music] headquarters. Good news around the world today as moments ago we confirmed the call for new leaders has been answered. The 2026 class of Snowflake data superheroes has [music] finally arrived. From every corner of the globe, these champions have risen to the challenge, demonstrating an unstoppable drive to solve complex data problems [music] and a passionate commitment to empowering others in the AI data cloud. Our sources confirm these data superheroes are the builders, [music] mentors, and visionaries needed in today's world. Their impact [music] is already being felt, and now we can finally reveal their identities. Let's hear from a few of them now. I'm Christopher Scott and becoming a data superhero means growing with a community that's defining the future of data. I'm fired up and ready to level up what we can build together. This year, I am most excited to help the community bridge the gap between AI experimentation and [music] real scalable business outcomes. Turning data into action, that's the mission. I am Guy and becoming a data superhero means focusing on the reality of snowflake at scale where architecture has to move from a concept into a system you can actually trust under the world. This year I'm more [music] focused on the shift from AI that just helps into AI that executes. That means going beyond the experiments and [music] making sure that AI in production goes with the proper oversight, control costs, and onure. A lot of the most interesting snowflake work is happening right now. Let's build the future together. Woo! Truly inspiring stuff. To our new data superheroes, on behalf of the entire Snowflake community, welcome. We couldn't be more excited. Your mission to amplify your impact and push the boundaries of what's [music] possible begins now. I'm Amy Leah Lesna. See you soon in the Snowflake AI data cloud. "}
{"url": "https://www.youtube.com/watch?v=S5WdT6LgNN0", "title": "SAP And Snowflake Zero Copy Integration Product Demo", "transcript": "Uh good day everyone and thank you for joining me today. My name is Sanjay Nag Manglam and I'm a product manager at Snowflake. Today I wanted to introduce the zerocopy integration between SAP and Snowflake and tell you about the customer use cases it helps solve. Now I'll talk about three things. I'll tell you a bit about the integration, what it is, where can you use it, how does it work. Next I'll show you a demo and I'll walk you through a couple of interesting use cases. And lastly, I'll wrap it up with some homework for you on how you can get started. So with that, let's get started. I'm super excited to introduce the SAP and Snowflake partnership and integration. Now, this integration enables customers to harmonize SAP and nonSAP data at scale while optimizing total cost of ownership across workloads. Customers can leverage zerocopy birectional data access and data and AI teams can work with semantically rich SAP data products in real time without added cost and complexity of ETL pipelines. This unified data foundation allows customers to build AI and machine learning applications fueled by trusted SAP data products and grounded in the context of all their missionritical data ensuring accurate, reliable and trustworthy AI outcomes. Our goal with this integration is to meet customers where they are and we will deliver two offerings to the market. The first designed for SAP customers who don't have a Snowflake account is SAP Snowflake, which brings our full AI data cloud to life as an SAP solution extension from advanced analytics to ML and data engineering applications. It puts the full Snowflake platform directly in the hands of SAP users. The second is for existing Snowflake customers. SAP BDC connect for Snowflake is a birectional integration enabling existing Snowflake customers to bring their mission critical SAP data into Snowflake with zero copy unlocking high impact AI use cases like real-time supply chain optimization. Let's double click into each offering. The first SAP Snowflake makes Snowflake's AI data cloud available as an SAP certified solution extension for SAP business data cloud. It brings Snowflake into SAP business data cloud's open data ecosystem and business data fabric. This empowers customers with openness and choice. SAP Snowflake delivers the full richness of Snowflake's AI analytics, data engineering, and collaboration capabilities as an extension to SAP business data cloud. The result, of course, is a powerful combination of SAP's semantically rich data foundation and Snowflake's flexible and scalable platform, allowing enterprises to share, enrich, and analyze real-time data across their ecosystems without data duplication. Now for existing Snowflake customers, SAP and Snowflake are offering SAP business data cloud BDC connect for Snowflake. This is a cloud service enabling birectional zerocopy data sharing with the Snowflake AI data cloud. Now companies already using Snowflake can leverage SAP BDC connect for Snowflake to integrate their existing instances of Snowflake with SAP BDC for seamless birectional zero copy access. This gives Snowflake users realtime access to semantically rich SAP data products without data duplication. Now in both options, the integration unlocks birectional zerocopy data sharing between SAP BDC and Snowflake. In other words, realtime secure and seamless access to SAP data directly within Snowflake. This approach simplifies access and preserves business semantics, making it easy to harmonize SAP and nonSAP data across the enterprise without data duplication and accelerates AI and machine learning initiatives. The result is lower costs, uh, lower complexity, greater trust, and high quality data with faster insights that empower datadriven agility all under the unified governance and security framework. Snowflake is known for. Our vision here is to equip every team across the enterprise with access to insights, recommendations, and automation. Now through the development, implementation and adoption of AI powered applications, every team can interact in natural language with applications, enabling them to drive transformations resulting in accelerated growth and improved efficiencies. Here are some examples uh of interactions our customers wish to have with their enterprise data. Well, what I'll do is I'll show you some of this integration in action with a demo of one example use case from that sales box with a red border. Now, for my demo, I'll deep dive into a sales use case and show you some customer analytics. I'll do a quick lap around sharing some data products from SAP BDC to Snowflake. And then I'll query those shared data products via SQL and semantic views. And lastly, I'll wire up the semantic views to a Cortex agent to help me analyze some business KPIs for my customer data product joined with financial data in the journal entry views data product all via zero copy integration. Now in my demo here, I want to analyze business KPIs from my customer and journal entries data sitting in SAP S4 HANA. SAP business data cloud makes data available for example from SAP S4 HANA as curated BDC managed data products with rich business semantics and thanks to the zerocopy integration with snowflake I can easily consume these data products and rich semantics [clears throat] in snowflake to perform my business analysis I have an existing snowflake account so I'll use the SAP BDC connect for Snowflake to integrate it with SAP BDC for zero copy data sharing The integration between SAP and Snowflake relies on the catalog integration capability in Snowflake for zero copy data sharing from SAP BDC to Snowflake. I can easily create a catalog integration in Snowflake and enroll it with the SAP BDC to establish a trust relationship between Snowflake and SAPBDC. And to create my catalog integration, I need an invitation link from SAP BDC. So my journey begins in the S SAP forme portal where I provide my Snowflake account URL to get back an invitation link. Now I already done this to save time and I've created a catalog integration and already enrolled it with SAP BDC. I can begin sharing the data products I need from SAP BDC to Snowflake. So now I'm here in the SAP BDC cockpit and all these are the data products available for me to share. Now the data products I need are called customer and entry view journal entry. Let me go to my favorites. Now as you can see they both came from SAP S4 HANA and I can simply click the share button on the data product and then select uh which SAP BDC connect for Snowflake I wish to share it with. I've already done this too uh and I've shared two data products that I need with my Snowflake account. So let's switch to Snowite and see what this looks like. I'm the Snowflake OpenFlow connector gallery and we've created an OpenFlow connector for SAP BDC. Now, everything I'm going to show you with OpenFlow is possible with straight SQL, but OpenFlow makes this easier and a bit more fun. So, there's my OpenFlow connector for SAP BDC. I've already installed it into a runtime and configured it. So, let's quickly see what that looks like. So, this is the connector and it's a zero copy connector like I noted earlier. Doesn't do EDL. The connector discovers data products shared from SAP BDC and automatically creates catalog link databases in Snowflake. Now I've set a few parameters here uh including my catalog linked integration name etc. and also a database name for where I wanted to create semantic views. Cool. So the connector has been running. So let's see what it did. So here are the catalog link databases uh that were created customer and entry view journal entry. Let's just describe customer here and let's go take a look at the tables in customer. And I can query these tables just like any other table in Snowflake. Now notice each time I'm doing the query, uh, Snowflake is actually fetching data on demand from the object store in SAP BDC via zero copy. Let me run this query here that joins data from customer and entry view journal entry. Now my SQL example here shows how to create a CASS with a join between tables in the customer and the entry view journal entry database. And then I can query that catas just like I would any other catas. Now here's the fun part. The integration also automatically created semantic views in snowflake from the core schema notation CSN interrop for the data products uh shared from SAP BDC to Snowflake. Now I told the connector to put uh the semantic views it created in a database of my choice. Let's see what that looks like. I'll click on semantic view details and this is what the semantic view looks like and it represents the facts, dimensions and relationships automatically created for the two data products that I shared. Now what can I do with the semantic view? Well, semantic views have the rich business context that I need about my two data products. So now I can build a cortex agent with this. Now, it turns out I've created a simple Cortex agent with these semantic views. Now, this agent is enabled for Snowflake intelligence and it uses the Cortex analyst tool with the semantic view underneath it. And I've made some sample questions to help me analyze business KPIs from my customer and journal entries data products that originated in SAP S4 HANA. Now, let's see what this looks like in Snowflake Intelligence. I'll click on some sample questions here. Let me click on the first one. So, it's actually using the cortex analyst tool under the covers u which then uses my semantic view uh to answer my business uh question here. There you go. It was able to answer my question and also represent it in a bar chart. All right, let's ask us a different question. And this was the question that I was really interested in. And there you have it. uh Snowflake Intelligence was able to answer my question about how my customer base has grown over time based on the Cortex Analyst tool and the semantic views that were created. So cool, that was it. So here's what you saw. Snowflake Intelligence could answer my business KPI questions because it used my Cortex agent that was powered by a semantic view that was automatically created from the two data products that I shared from SAP BDC to my Snowflake account with our zero copy integration. We invite you to try the zero copy integration between SAP and Snowflake yourself and give us feedback. So homework for you, contact your respective account managers to get started today. Thank you very much for your time today. "}
{"url": "https://www.youtube.com/watch?v=l9MisysRVZU", "title": "Keyless GitHub OIDC Auth With Snowflake Workload Identity Federation", "transcript": "In this two-minute hacker mode demo, we are deleting the secrets. I'm going to show you how to use Snowflake to authenticate using OIDC. It's keyless, it's secure, and it's fully automated. Let's dive in. Let's look at the problem. I'll run my task to list warehouses. Look at this. Costc demo warehouses are set to auto assessment 1,800 seconds. That's 30 minutes of wasted credits. First the fix. I wrote this simple snowflake script. It scans for any warehouses matching the pattern and forces auto suspend to 60 seconds. Second, the security. How do we run this without passwords? In the setup.sql, I created a service user mapped to an OIDC subject. This links my GitHub repository and the branch to Snowflake. Third, the automation. Here in the GitHub action because of OIDC setup notice there is no secrets configured here it just asks us to be a governor bot role that we granted to the service user earlier and does the work time to run it I'll trigger the action right from here the CLI watch the lock it sets up surflex CLI action the connection via IDC enforces a policy the run is green let's verify I list the viruses again and that it is all viruses glammed down to 60 seconds secured, automated and purely command line driven. Remember snowflake is an infrastructure. It's time to treat it like a code. And that's a demo. I'm Kame Sat, lead developer advocate at Snowflake. If you want to try this yourself, check out the description below for the docs, demo sources, and other useful links. Thanks for watching. [music] "}
{"url": "https://www.youtube.com/watch?v=E6c_2B1KxT0", "title": "Learn about all the capabilities that Snowflake Cortex AI can do #ai #analytics #snowflake", "transcript": "So uh Snowflake Cortex AI is easy, connected and trusted. So at the very foundation we have a bunch of L&M models uh from various providers. So OpenAI, Anthropic, uh Meta, Mistro, DeepSeek and our very own Snowflake architect models. And uh these are pretty great when you want to do things like uh you know processing text to scale or maybe you want to do uh semantic analysis um and and so on, right? So these are pretty great from that perspective. But uh enterprises these days don't want to just have like a chat model. What they want is the ability for their uh employees or users to be able to talk to their data. And for that we've got state-of-the-art retrieval functions. So this is for uh unstructured data retrieval which is the cortex search services. And think of uh you know the ability to build to sort of build raglike applications and that's where this would help. Uh another one is the cortex analyst service which helps with structured data retrieval. So think of uh text to SQL uh features. Um in addition we also have the multimodal AI powered SQL functions. So these are cordex AI SQL functions and uh think of uh you know translate where you can translate from one language to another or uh transcribe where you want to do from audio into text and so on. And I think there's a bunch of about 10 or 15 functions uh underneath that umbrella. And because we trust sort of the reliability of these features, we went ahead and built uh our own functionality on top of that. So we have document AI which is essentially for document passing. And then we also have uh Snowflake Copilot which essentially is a coding assistant for SQL as well as Python. And uh very recently we announced uh Cortex agents and the agents API as well. And this sort of provides you with that single agent multiple tools uh orchestration plus research uh sort of a functionality. And uh on top of that we have uh snowflake intelligence which is sort of like a UI wrapper uh on top of cortex agents. And what this provides in addition is also the charting capabilities as well as citations and some other fun features. So together all of this comes underneath uh snowflake cortex AI and uh in addition because it's part of the snowflake uh snowflake secure boundary so none of your data leaves outside of snowflake and essentially all of these models are hosted within snowflake itself. So there's no third party API connections that are required and that's why you get out of the box arbback guardrails uh evaluations and monitoring and of course the gateway as "}
{"url": "https://www.youtube.com/watch?v=Nq0W98Y5EMQ", "title": "What is Snowflake? Why Developers Love It (2026)", "transcript": "Every day, billions of queries are run across\npabytes of data to power AI applications, train ML models, and deliver real-time insights\nto millions of users. Behind it all is Snowflake, a fully managed multi-cloud platform for any\ndata workload. From pipelines and analytics to AI and ML, Snowflake gives builders the tools\nthey need to build data inensive applications at scale without the infrastructure overhead.\nThat means faster cycles, less grunt work, and more business impact. Let me show you why\nso many developers trust Snowflake for their mission critical data workloads. If you're a\ndata engineer, you know that building reliable, scalable pipelines is the first step to\ngetting value out of your data. Snowflake makes it easy by seamlessly integrating the\nthree critical stages of every data pipeline, ingestion, transformation, and delivery.\nFirst, get your data in. Snowflake OpenFlow lets you ingest any type of data, structured or\nunstructured, in batches or realtime streams, and from any source. With your data in, it's\ntime to transform it. Whether you're running transformation in Python or dbt projects\nand SQL queries, Snowflake has you covered. You can build your transformation workflows\nusing user-defined functions, stored procedures, Dynamic Tables, and more all in your preferred\nlanguage. With Python, you can use familiar data frame APIs with Snow Park, Pandas, and\nPySpark to run complex data transformations. And all of your data processing happens\ninside Snowflake's powerful compute engine. Your code runs next to where your data already\nlives, which means less data movement and no separate compute clusters to manage. It's faster,\nmore secure, and incredibly efficient. Finally, you need to deliver reliable, high quality\ninsights. Delivery in Snowflake goes beyond tables and views. You can build fully interactive data\napplications for your consumers with Streamlit or go a step further and distribute data sets to\nother developers using Snowflake Marketplace. All developed and shared securely from your own\nSnowflake account. You can go further with your insights by building agents and complex\nagentic workflows in Snowflake intelligence. These agents empower business users to get answers\nfrom structured, semistructured, and unstructured data just by using natural language. Snowflake\nIntelligence helps you identify and analyze trends, generate visualizations, and get instant\nanswers from your data. If you need powerful, fully managed AI services, then Snowflake\nCortex is the foundation you can build on. For example, Cortex Analyst lets you query your\nstructured data with SQL using natural language. It supports semantic views, meaning you can\ncreate custom mappings between your business concepts and the tables and columns inside your\nSnowflake environment. This enables your team to do accurate data exploration at scale. Cortex\nSearch provides a low latency hybrid search engine for unstructured data. It's perfect for building\npowerful rag applications and agentic workflows, helping you create an intelligent search\nexperience on top of your data. What if you want to use generative AI directly in your queries,\ndata pipelines or applications? Using AI powered SQL functions, you can get perform tasks like\ntext summarization, translation and even sentiment analysis on top of multi-modal data sources. You\ncan even invoke your preferred LLM with a custom prompt using your data as context. These functions\ntransform complex AI workflows into simple, powerful SQL statements, unlocking the power of\nAI for data engineers and data analysts alike. This unified approach to the AI means less time\non infrastructure and more time spent on building and scaling your powerful AI applications all in\na single secure platform. Developing your next major application. Snowflake provides an ecosystem\nof tools and the infrastructure to build scalable production grade apps all while ensuring optimal\nperformance at every stage. For data exploration apps or customer facing dashboards, Streamlit\nin Snowflake makes it easy to deploy data apps directly in your Snowflake environment using\nPython and SQL. Snowflake native apps allow you to build and deploy full-featured applications to\nthe Snowflake marketplace where other developers within Snowflake's ecosystem can install and use\nthem directly in their own Snowflake environment. And for transactional workloads, there's Snowflake\nPostgres. Snowflake Postgres gives you the full power of community Postgress without the overhead\nof managing it yourself. It comes ready out of the box with handy extensions to do things like\nsemantic search with PG vector or geospatial queries with PostGIS. Snowflake Postgres also has\nfeatures like automatic backups and failover with high availability so you can scale your largest\ntransactional applications with confidence. The rich ecosystem of Snowflake drivers and\nAPIs gives you programmatic access to your data as well as to AI like Snowflake Cortex. Taken\ntogether, Snowflake provides everything you need to launch your high performance data inensive\napplications. As an ML engineer, your job is to build and deploy models. Snowflake ML Ops platform\nhelps you do it faster with full endto-end support for Python libraries and APIs. You can train\nand deploy models directly in your Snowflake environment using your favorite libraries like\nScikitlearn and XG Boost and even bring in your own frameworks. Feature engineering happens right\nwhere your data lives in both SQL and Python. For resource intensive jobs, ML jobs allows you to run\nworkflows on distributed compute nodes with fully managed container runtime. You can work with your\npreferred IDE like VS Code or Jupyter notebooks and dispatch your code to Snowflake. This lets you\nleverage powerful infrastructure including GPUs without setup headaches. Snowflake's platform\nstreamlines the MLOps life cycle from feature stores and managing model versions. With Snowflake\nmodel registry to batch and real-time inference, your models can consume the same clean, governed\ndata as your analytics teams. Even better, you can deploy models as userdefined functions. This\nmeans any team member can get a prediction with a simple SQL query. Truly democratizing access\nto ML within your organization. At Snowflake, we're deeply committed to open source. Our\nengineers contribute to projects, collaborate with communities, and incorporate open-source and open\nstandards into our platform. We're championing lakehouse architectures through deep integrations\nwith Apache iceberg and Apache Polaris. This is about freedom and interoperability. You\ncan write to and query iceberg tables within Snowflake and maintain compatibility across\ndifferent compute engines without rewriting data or losing historical information.\nIt's truly vendor agnostic data access. Managing your cloud data platform should be as\nsimple as managing any modern application. That's why infrastructure as code is fully supported\nthrough our comprehensive Terraform provider, letting teams manage accounts, databases,\ncompute resources, and security policies, all with familiar DevOps workflows. And of course,\nStreamlit itself is open source, meaning the apps you build can leverage the entire Streamlit\necosystem from community components to custom visualizations while running with Snowflake's\nscale and security. Our open- source commitment doesn't stop there. Open source is embedded across\nthe entire snowflake platform with scalable data engineering using Modin robust ingestion based on\nApache NiFi familiar app foundations with Postgres efficient machine learning workflows with PyTorch\nand Ray and more effective LLMs with TruLens. This open approach allows you to adopt Snowflake\nincrementally, integrate with existing tools, and maintain the flexibility you require to evolve\nyour architecture without vendor lockin or data migration overhead, all while benefiting from the\ncontinued innovation of open source. Powering this entire data experience is Snowflake's unique\nplatform architecture that scales as you build. To start, data storage is completely separate\nfrom Snowflake compute resources, which means you can scale each independently based on your\nneeds. Storage is automatically compressed and optimized for you. Data in tables is automatically\ndivided into micropartitions, compressed using advanced algorithms, and organized for optimal\nquery performance. Compute resources range in size from extra small to 6XL, giving you access to\npowerful compute as you need it. When not in use, your compute resources auto suspend, so you only\npay for what you use. Snowflake runs natively on all three major cloud providers. This means\nyou have flexibility when developing your cloud strategy. Your data pipelines, ML models,\nAI apps, and more all work the same regardless of which cloud provider you choose. Security is built\ninto every layer of the platform. Access to your data objects is controlled through fine grain role\nbased permissions giving you complete visibility into who accessed what data and when. Row access\npolicies, masking policies, object tagging, and other industry-leading features help you\nensure the highest levels of data governance and compliance for your SOFK environment. So\nhere's what this means for you as a builder. Rather than spend time on infrastructure, data\nmovement, and general setup challenges, you can focus on building and creating value all within a\nunified fully managed data platform that supports your data workloads and tons of complex use\ncases. So whether you're building data pipelines, training ML models, creating intelligent\napplications, working in open data ecosystem, or analyzing massive data sets, Snowflake\nprovides a single platform that scales with whatever it is you're building. If you're ready to\nbuild something amazing, start with a free trial account at signup.s snowflake.com and check it\nout for yourself. What do you planning to build? "}
{"url": "https://www.youtube.com/watch?v=2R6UfaNrD8I", "title": "Did you know you can extract data from images using Snowflake Cortex AI? #ai #snowflake", "transcript": "Listen, I talked about images. You can actually provide an image to AI extract and pull out the details from there. So, here I have an image of uh the chalkboarded uh menu that this restaurant has and then pull out the daily specials. I get the coffee price and figure out what the cheapest dessert is. all with a SQL statement which is beyond nuts in my opinion. "}
{"url": "https://www.youtube.com/watch?v=KKs39D-BFyM", "title": "What is Snowflake Openflow? #snowflake #dataengineering", "transcript": "So what is OpenFlow? So OpenFlow is Snowflake's firstparty service for global data movement across all of our customers data estates. We really are complementing Snowflake's move into a universal platform and AI data cloud and OpenFlow is the perfect complement. You can think of it as a Swiss Army knife to that universality because OpenFlow handles all forms of data and we can ingest data from wherever it might Love. "}
{"url": "https://www.youtube.com/watch?v=QrkbCM9bZ-I", "title": "[LIVE] Vibe Code Data Apps with Replit + Snowflake", "transcript": "Heat. Hey, Heat. All right, we are live. Good morning, good afternoon, and good evening for folks who are joining from literally all over the world. I see a lot of cities from the comment section here. Welcome to the episode. Today I am Veno Disami. I'm a developer advocate at Snowflake and we have a super exciting episode for today. It's all about wipe coding data applications with snowflake and replet and you don't have to write any code like bring in prompt your way out of building data applications using streamllet I mean replet and snowflake but to help us all with it I have a special guest today who is Manny who runs community at the replet team hey Manny welcome to the show but I also owe the audience a brief introduction about who you are and why you're here and what are we going to talk about today because I do think you have a very interesting background like for folks Manny's on the community team at Rivet formerly a data scientist of course everybody is a recovering data scientist in this world at least and he also owns an AI agency he's an investment professional and he has worked with large enterprises to implement AI and analytic solutions so currently he is on a mission to bring Y coding to the world making it possible for every single one of you out there to turn ideas into real data applications. Very cool. Welcome to the show, Manny. >> Hey, VU. Happy to be here. What a great chat we have going on here. Hi, everybody. Good to see you. Excited to be chatting with you. Um, as you mentioned, I'm a former data scientist. I've been in the data world for a long time. I've always been a big fan of Snowflake and I'm just surprised with what you can do now with Replet and Snowflake compared to what I was able to do when I first started my career. And so I'm super pumped to be here and chatting with you. >> Right. And I see folks here, I think the regulars saying no hat today. I didn't know that was a thing with Manny. That's good to know. [laughter] >> Do you guys want me to bring the hat? I got my uh >> I think my my super cool uh replet right here. >> That's it. Now, >> but I thought this is more of a professional bunch, you know, data professionals that I'd leave the hat off, but you guys let me know in the chat if I need to bring it back. >> All right. Yep. We still want that swag while it lasts. But I was thinking, tell us, Maddie, what are we here for today? Why are we talking about wipe coding today? I know why coding has been the rage for what a whole year now. >> Yes. I I think you know um the last year or so vibe coding has really gone mainstream and if you think about what's happening on the software engineering side around what people are doing with building with AI and vibe coding it's been quite the shift. I think back to a statement that Dario, the founder of Anthropic, made a year ago around 90% of all code being automated by AI. People saw that and they were skeptical. Some of them even laughed at Daario. Fast forward 12 months and I think that's pretty much the consensus in the software development world that you have to code with AI. You have to vibe code or else you're going to be left behind. And I think that is a preview of what we're going to get on the data science side of things and on the data analytics side of things. I think up until now it's been somewhat difficult because to do good data analytics work you need to get data into your environment and AI needs to work well with that data and that historically has been pretty tough. Over the last couple of months, the Snowflake team and the Replet team have worked really hard to fix that. So that now we have a connector that allows you to bring your Snowflake data, bring it into Replet's vibe coding environment and start vibe coding your own data applications. And again, I think back to when I started my career, you know, I had to learn R. I had to learn uh um SQL. I had to learn Python. I had to set up my own development environment. I had to download my own packages. And that was just always like a big hurdle and really tough to get started with my uh analytics reports and the data science models that I wanted to build. And now I think about what you can do with vibe coding and tools with Snowflake and Replet. I'm like, \"Oh my gosh, this is such a gamecher.\" Not only for us data professionals, for me, you know, if I want to spin up a quick prototype now, I'll go to Replet and just ask Replet agent to build it out, get a quick look at it, make some changes, ship it if I have to, and I can I can go into the code if I want to get a little bit more detailed. But I think this is also going to be a big game changer for non-technical people. So leaders, executives, managers that maybe don't have the technical aptitude to go in and program something for themselves, now they can work with tools like Replet agent to get custom applications, dashboards, reports, and automations. And so I, you know, again, you know, VIP coding is a big deal. It's going to be a big deal for the next five years. I think it's gonna, it's already a big deal on the software engineering side, and it's definitely going to change the way that we do data analytics and data science. Absolutely. And I know for a fact that all of us when we started write coding like in 2025 was more about oh let's just try it out what happens is it fun just exploring what you can do with it. But 2026 I tell everybody is the year if you do not get on the bandwagon you are going to get left behind. It's not for the curious you know early adopters anymore. You can't just play around and like see what it can do for you. you have to start use AI like I I don't know how do I tell people like this is it if you get left behind you are getting left behind so this is the year you want to get serious about using AI tools and get on board with white coding and so I also see some comments saying you know I've never done it how easy it is and everything do not worry we have Manny here to show us exactly that and we will build our first ever data application with replet using white coding With that said, Manny, do you want to talk about why are we talking about it today in particular? I know you did say there was some snowflake connector from Replet that I hear. >> Yes. Let me start sharing my screen and give you all a little bit more context here. So, let me just make sure my screen is up. Cool. Yeah. So, we talked about by coding, how it's different, Daario. And again, the big issue historically has been how do I get my data into my coding environment so that AI agent can see that and start helping me build things out. And so now we have a Snowflake connector that allows Replet agent to see what databases you have access to, pull data from those databases, and start building out your own custom applications. So, with that said, I thought I'd just jump into a demo and show everybody what what this is all about. First, I will say that there is a bit of a um admin um setup that has to happen both on the Snowflake side and on the Replet side to enable this functionality for users on your Replet teams account or enterprise account. I'll touch a little bit more on this a little bit later, but just know that we're rolling out a detailed developer guide on how to do this. So, if you if you don't catch all of this today, don't worry. We're going to cover it in that soon to come developer guide. So, I just want to walk you through this. So let's say I I go into replet and let's say that I wanted to build out a new say sales report based on data that I have in snowflake. I would go into replet and drop a prompt here. You can I'll look it over and if you notice this prompt is pretty pretty casual. It's not very technical. It's saying hey build me a regional sales performance report. And let me zoom in on this so you can see it a little bit better. I want to be able to see a map that shows different regions. I want to see um details like top revenue, top orders. I want a bar chart. I want a line chart. And then I want to be able to click into some of the regions and show some information, but pretty casual, not very detailed. And what I want to make sure I do is activate the snowflake connector. I can do that from this prompt or I can do it a little bit later, but I'll just do that here. And the cool thing here is that I could just do a slash command and search for my snowflake connector and bam, I have it there. And I just want to make sure like make sure to use the snowflake connector. I don't know if you have to be that explicit, but always best. And then we can click on start. And so that is going to kick off this build. And what you see here is that now I am in the build mode within Replet. On the left hand side, you see my conversation with Replet agent. Here's where that initial prompt went into. And on the right hand side, you're going to start seeing Replet put together the preview of this particular build. And what Replet is doing, it's starting to write the files for this particular project. And so if you ever want to go into the code for this build, you can always go here on the lefth hand side and check that out. You don't have to, but if you want to get more technical, more hands-on, you can do that. And you're going to see replet agent start taking a couple of actions. It's going to start configuring that Snowflake integration. It's going to start writing the um um the different API uh commands and files to be able to pull data from Snowflake. and it's going to start putting together this application. Now, this initial build will take anywhere between 10 to 15 minutes, I'd say, but I don't want all of you to wait. We can come back to it and I can show you what the initial output looked like from this. But earlier today, I ran this prompt to speed things up a little bit. And here is what I got from that initial prompt. So, here is my regional sales dashboard. I have a couple of tabs here with the information that I'm looking for. And I have a map of that information. That's the what I asked for in the prompt. I've got it broken out by region here. And then I also have a chart here as well. I can also do things like change the date range here. And automatically that data is getting pulled in from Snowflake. And that data is getting refreshed in real time. So after your initial build, you'll get something like this. And again, if you want to go into the code, you can always click here on the upper right hand side and you can see that as well. And all this code belongs to you. So this is your own uh this is this is owned by you. You can upload it to GitHub to a repo. You can download it if you want. You can run it locally if you want. So after the initial build kind of going back and forth, you might want to make some changes on this particular um build. So for example, I don't like that this chart is um a little bit off and I want to change this. So I can do something like maybe grab a screenshot of this. go into replet agent here and I started a new conversation here um previously and I can just ask replet agent to make this change and have that have the instructions be kind of in a natural way and this is kind of the big game changer of working with AI previously you needed to know like the exact syntax to change the exact code to put in but everything now is just much more natural much more conversational so I can say something to the effect of hey can you fix this chart um I don't like that. Um, the y-axis is a little bit off. Can you maybe have the y-axis be between two billion and 5 billion to have this render a little bit uh more neatly and I'm going to just drop that in there and let agent make that change. So, agent is going to see that and um is going to start making that change. Here's another really neat thing about Replet is that it gives you the ability to roll back to a point in time. So if for whatever reason either you put in a wrong prompt or if agent makes a change that you didn't want to happen, you can always roll back to this point in time and everything reverts back to that that place that you that that you had before it made the change. So you can see here pretty quickly it already made that change. So now I have 5 billion to two billion. Again, if I wanted to change that, I can do that. Same thing here, like I can interact with this chart here. And you know, the other thing here is I might want to be able to click into one of these regions and have more detail on each one of those regions. Same process. I'd start I I keep it in the same conversation, but again, I do a screenshot here just to make it a little bit more clear for agent what I'm looking for and say something to the effect of, hey, um, on this map, um, I'd like to be able to click into a region and get more details on that region. So maybe a table gets pulled up with some more detail for that region. Um, can you help me build this out? And so one other thing that I that I would suggest is, you know, if you if you want if you have some questions about um a change or a feature and whether or not agent can do it or whether it's a good idea, one other thing you can do is move into our plan mode. And so this allows you to have a conversation with agent about the change that you want to make before you actually make it. And I really find this helpful, particularly when I'm working on complex features or if I'm doing something new. I want to have a conversation with my agent before it actually makes a change. So, see here, you'll see agent start thinking about this request, start thinking about what needs to change, and then um give me some suggestions here. So, it's like, hey, great news. Let me uh zoom in here a little bit more so you can see this conversation. Um, great news. um I already have this feature built. So here's what you need to do. Uh if it's not working, miss additional columns. Okay, so by try clicking on the Okay, so let's see. So it's telling me it's actually already working. So let's actually see that and I can say look say like nope, actually I'm unable to click on a region on the map and get more um details. Can you maybe suggest a change to make this happen? So, I'm dropping that in there. And I'm using a transcription service to um go from voice to text, or you can just type this in if you like. So, um while that's cooking, let me show you the initial build um that the prompt that I put in earlier and show you one key step that you're going to make before you get into um this build to start iterating on. So you see here that on that first prompt that I put in, it's now asking me to authorize the Snowflake connector. And so here's the the the key the key thing that we're featuring today is this ability to connect to your Snowflake and bring that into Replet. And all I'm going to do is set up that connector and then this agent is going to be able to um set up that connection to my Snowflake database and start pulling in that data. If I compare this to what I used to do in the past to to set up a connect a connection to my database, it's just night and day. Now it's just one click. Previously, I had to set up APIs, all this complicated O and now it's just one click and it's all set up, which is really, really neat. So, let me go back here and I can see that agent now has a plan for me here and says like, okay, yeah, that's what I can do. I can make some changes. So now it's going to ask me if it wants to if I if I want it to start building. And I can say yes. Now it's going to transition from plan mode into build mode and start making those changes. And that's pretty much the process. That's the vibe coding process. Step number one is enable your snowflake replet connector. Step number two is give replet agent that initial prompt. And then step number three is iterating off of that initial prompt and asking replet agent to make different changes based on what you want to see, what you want to change. And I think this is a really neat thing about vibe coding is that it's so custom. You can build something particularly to what you want, what you care about. You can change the different graphics that you have here, the different charts you have here. I introduced a dark mode here just because you know I I like that a little bit better um by prompting agent. You can change the different filters here. Maybe you want to add a filter by region. Um maybe you want to add another filter by different accounts and you can modify all that. This is the cool thing like all of this is custom and and built to specifically to what you want. Now there's one other step that I wanted to show you all. I wanted to show you how you share this once you're done with your build. How would you share this with your team? How would you share this with the world? But first, Venu, happy to field any questions that you might have or any questions that you see coming up from the chat. >> Yes, I do have a couple of questions. I think the latest one from Rosano Clayton, it's about if we're doing data transformation, how can we export the data back or does the connector allow us to do CR operations? Is it read only or are we able to write back to the data tables and whatnot? >> Great question. Currently, it's read only. Um, and that's by design. Vibe coding is still a relatively mo new mode of operation. You know, imagine if you were writing back to your Salesforce u excuse me, your sales your snowflake databases and you're you're writing back incorrect um uh changes. And so, currently it's just read only. You pull in the data into your replet environment. Replet can see your your your data. You can do transformations within replet. >> So if you want to transform data and and and have it for a particular application, you can do that within the build in replet. But writing back changes to Snowflake currently is not allowed. And that's that's by design. That might change in the future. I think as you know we we all get better with vibe coding and as we have better controls in terms of how we make changes to those databases that might change but currently it's just read only into replet but again you can make you can you can have those um changes be or uh updates and transformations be made on the replet side for your application. >> Perfect. And another question is as my data gets updated this dashboard also gets updated and stays you know continuously updated right >> that's correct yes that is the really neat thing about this as your data changes on snowflake this will change as well and then if you um want to just make sure that's the case I have a button here that says refresh my data that'll automatically make sure that I'm pulling in the most recent data from that snowflake database >> got it fantastic And I I know this is probably a little more about the snowflake connector. There is questions about the semantic model or the semantic file but essentially you know different business teams have different definitions for you know what what do the KPI mean and what exactly is the definition of it and then all of these context the business context is stored in this you know semantic layers it sounds like and is replet able to use the semantic layer to you know build these reports and dashboards and whatnot >> that is a great question let me table that and let me take that back to the engineers and I will include that in the follow-up or in the developer guide that we roll out because I think that's a very good question um around including that in on the replicite as well. >> Gotcha. Yeah, I think my hunch is that it is but yes, we'd be more than happy to check with engineers and report back on that. And yeah, and there are a few more questions on the replet side of the world. I was going to ask you is this free? Can I just use this mostly connector from my [clears throat] free account or do I need enterprise or any other specific versions of it on RevNet? >> So, unfortunately, it's not free. You'll have to use it from our teams account or our enterprise account. And so on the teams and enterprise account, what's really special about those accounts is that it allows an admin person to one authorize the connector and two to control who gets to use the cont connector and what capacity. So if you're an admin on replet for a for a teams account or an enterprise account, you can say that this team has access to these databases and can only use it within these applications. And so that is enabled through the teams and enterprise account plans. And so you'll need one of those plans to take advantage of this connector. >> Awesome. And another question is about I think it was from a while ago, but something about the plan mode costs versus build credits. Are they all the same or are they being charged differently? >> Yes, the the So if you want if you No, they're all it's all plan mode and build mode. th those are all charged in the same way. Um what you'll find is is typically you'll get um um more token costs when you're in build mode because you're making more changes to the code base and so you have more tokens that you're going to be using up. Whereas in plan mode, you have less tokens, less context, and so it's going to be less expensive. I also think that starting with plan mode tends to be a little bit more costefficient um as you're building because you know it's almost like when you're adding a new addition to your house you want to sit down with your architect. You want to lay out a plan of what you want to do before you have the contractor build something out. I think that that's that works well in vibe coding as well. You want to take some time to plan with your agent. Make sure that agent knows exactly what you want. make sure that um that that you're giving it the right directions and then you have agent build it out. But in terms of like actual usage, they're they're build in the same manner. But I recommend starting with plan mode and then going to build mode particularly for more hair types of features and things that you're building out. >> Gotcha. And I think there are a couple questions around the snowflake side of the world. I think not not even snowflake but more so well the dashboard is built but how do I now interpret this? How do I understand this? So is there a way for me to see what tables in snowflake were used or were queried to bring you know these numbers to me? Is there a way for me to sort of point those tables back maybe from the >> Yes. Abs. Absolutely. So the way that I would handle that is by prompting agent here. So we can say something to the effect of um can you give me well before I do that another helpful tip is to start a new conversation when you have a new topic with agent. It just helps agent stay on track. So can you help me understand where you're pulling this data from? Um what databases in snowflake are you using? What tables are you using? Give me some context here. and then I'll drop that in and then agent will pull that up and give us a little bit more context there. If you'd like, you can also build out a new tab that gives you some more clarity on that depending on what you're looking for. You might also add that to the documentation. So, you can add a a markdown file here that gives you some clarity in terms of where that's getting pulled from. But for for my for in this case, I would just ask agent to give us that information. So here you see the database, here's the schema, here's the different orders, and here is all of the details on this. And then what's nice about this, again, this is pulling directly from Snowflake. So you can continue to have a conversation about this data. So if I'm new to this data, I can continue to ask about this. So I can say, >> can you tell me a little bit more about the orders table? Um, give me kind of a quick summary of what's in there and why it's important. drop that in there and now like I have like my own data analyst here that is go it's going to go through that data bring it back to me and explain it to me. So um you know one one thing you know it's one thing to use replet agent as a builder which you you see here but then it's another thing to use it as an assistant. So here it's pulling the data it's explaining it to me and it's helping me understand it. The other way to use replet agent is with help in understanding different components. So if I want to understand how this is built out, how you know what's the underlying technology on it and I want to learn more about that, I can just prompt agent and agent will explain it to me. It's like it has all the context and it acts as a tutor of sorts. So particularly for non-technical people, if you're looking to get started by in terms of building your technical aptitude, this is a great way to do it. So you might say like can you give me um an example of this of the SQL that I code that I would use to pull information from the order table. And so if I want to learn about SQL and how to structure those um that code and and write that out, you know, agent can help me with that as well. And again, it I can just go back and forth with agent and ask questions and learn more about this. So three ways in which you can use replet agent again to build that's the fun part that's what I like um to uh learn more about your data and then also to help you understand more technical concepts. >> I think this is very cool. I feel like inadvertently you also touched one of the questions that came up which was so I see this dashboard but how do I make sure the numbers that I see in this dashboard are right like how do I validate it? And I wanted to say that one way to do it is you could literally ask a ripple agent and be like, \"Hey, give me the SQL query that you use to come up with these numbers.\" And then well, ideally, we're thinking of this experience for a non-b businessiness user where I don't have the SQL expertise or I don't need to have the SQL expertise. So they could just go talk to their data, get the answers they need directly without being bottlenecked by a data person. But then if you are a data person trying to build this you know dashboard to share it with the business users right you want to make sure the numbers are right and you want to validate it you can absolutely ask replet agent to create these SQL queries and you could literally manually see if this is the query that would give you know the numbers that you're looking for like some sort of a human in the loop validation before it goes out for you know wider sharing with other business users. It really is like you decide how you want to use this. Like you're a business user, just go get your answers in natural language, but if you're a data person, you can do this additional layers of validation checks and whatnot. >> 100%. If I was going back to my data scientist days, I would use Replet to build out the front end like I'm doing, but then I would also get the SQL queries and run it in Snowflake myself to make sure that I'm getting the right numbers or the the numbers that I'm expecting. Or the other thing you could do is you can write a couple of um test cases and again if you want to say you know if we go back to replet new conversation and we can say something to the effect of you know I want to make sure that you're pulling the right data from snowflake can you suggest a couple of tests that you might write that you can run on a regular basis that just would make sure that you're pulling the right information and something that I can validate. Drop that in there. go into plan mode and then have a conversation with agent and then you can have a number of unit tests that can also be a part of this codebase that you can run on a regular basis. You can have an admin panel that where you as the owner of this application can go into and get the outputs from those different tests to make sure that you're getting the right output. So a couple of different ways to tackle this. And here I'm getting some suggestions here in terms of what we can do like a sanity check, regional count, all these different things. And then again, I would ask agent to build out unit tests for me to run and so that agent can check its work too, but also for me to see it. And then I might also ask for how I might double check this on the snowflake site just for me to eyeball it, check to do an eyeball check on it to make sure that it's um giving me the right output as well. >> Yes, I think this is very cool. And there is a bunch of questions about the voice to text that you're using. Is it reflet or is it an external tool? And I think this is more of a side quest, but folks would love to know. >> No problem. This is my favorite way to vibe code. So I use a tool called Super Whisper and it's always on and I just I just I turn it on. It transcribes my voice and it just pastes what I say, the transcription [clears throat] of what I said into um the prompt as you just saw it. Um the transcription service within Replet, we're talking about introducing that. That's coming soon, but for right now, Super Whisper is what what what I use and I use it in everything. So, it's just like it's you know, I think part of the part of the tough part of having me in the office is that I'm always yapping to my computer. So, they're like, \"Agent, do this and make this change.\" Like, \"No, you got to do this.\" And so, people probably think I'm a little bit cookie, but that's my favorite way to viode. And here's I mean, this is what I really love about vioding is that if you notice, I'm I'm just asking for things in a very casual way. It's like I want take this map, change it, make it better, give me some more granularity. If I go back to my map and you if you remember that change that I asked earlier around adding more um granularity when I click into the into the region and now here it is. So now if I click into that I see all of the customers for that and I even get a nice little chart. I didn't ask for that agent filled that out for me. Very nice of agent. So I can scroll over here now. I see the Europe region. Now I have all the detail here and I just explained to agent what I wanted in a very casual type of way non-technical and agent built that out and I think this is one of the fundamental shifts in terms of what's happening with coding and programming is it's going from like very detailed very specific code that you have to put in to now the focus is on the functional explanation of what you want. So I mean for me it's been about getting better about describing what I want to AI and focus on the output not necessarily the code itself and um it's just it's just so much fun. >> Yeah. I think one definite tip that I want to share with folks who are trying out for the first time or if you've not white coded is that I over this holiday the holiday break started to build my own personal website. I'm like, you know, this one thing that you keep putting back like, oh, I would come to this later and it never happened. During the holiday season, I started building out a website and one thing I absolutely learned was that if you don't speak the language in a way the agent understands, it's going to be hard. You're just going to be finding, oh my god, what is the word for that? Oh my god, I'm saying this. Why are you not understanding? There's going to be just lot of back and forth. Use the plan mode. Tell it to do something and ask it to summarize the understanding. like hey this is what I want you to do now summarize what you understood before it even goes and builds something right so the more information it gets from you the more context it gets from you the better the output is going to be and the better your experience is going to be it's just like I mean think about it right even if you're talking to another human giving more context would be helpful same here right if you say I want revenue maybe if you have 10 revenue tables it may be helpful to give more specific revenue instead of product revenue or a region wise revenue. Just give it any additional context that you think is needed for that, you know, agent to be able to give you any relevant answers. >> Great tip. And one other thing that's been helpful for me, uh, that I don't think enough people took advantage of is the fact that you can drop in screenshots. You saw me do put in a screenshot earlier. And you can drop in a ton of text. So, one thing that you might do is you can start off in something like um chat GBT or claude and have like a big conversation about what you want to build and have it like very detailed and then copy paste all of that text into your replet prompt and have replet agent use that as reference. You can drop images in there, big text, you can drop in documentation. So for you know if you're if you want a connector or a connection to to to to an application that isn't available in our connector suite. So I'll show you where you can find that. So if I go to replet >> and then I go to integrations. These are all the integrations that we have. Snowflake here is connected. But for whatever reason, if you want to tap into something where we don't have a connector, you can just grab all of that documentation, copy paste that, drop it into replet, and replet will have that context to build that for you. But when you can, I'd suggest using the connector here. >> Gotcha. I think this is a good question. Can you quickly scroll through the list of connectors? Because there were some folks who are like, what other connectors are there? What other input data sources can I use? As you can see, there's like tons of options available. So, go do check it out. Yeah, absolutely. I'll tell you my favorite one so far. Obviously snowflake. Um, but I've also really liked notion. So if you go to my website made also vibe coded on replet, I have these examples of different things that I have built and all of this detail lives in a notion database and I'm using the replet connector to bring in that context into the website. And the reason why I do that is because I don't whenever I add a new thing that I built. So I'm going to the next thing I'm going to add here is introduction to snowflake and replet. I'll just go into my notion database, put in the title, the description, the link, and then it's automatically going to be updated on my website with me without me having to do anything on the website and a thing. So super convenient. So really like the notion connector. Google Sheets is so clutch. Google Drive obviously um one drive um these have been very very very popular. >> All right and there's one more question I know we have been building dashboards but we haven't published yet. So can you talk us about when you publish this dashboard where will this be published? How do I access it? How do I share it? >> Absolutely. This is one of the things that people love about Replet. So in Replet, not only can you build your data apps, but you can also share them with the world. You don't have to host them anywhere. You don't have to worry about deploying a database. What you see in the preview here is going to be deployed. So let me show you how to do that. You can always ask agent um or you can go here to one of our tabs and look for publishing. So these are all the details that are um that um are are going to pertain to this u to publishing this app. And I [clears throat] have a couple of different options here. I can deploy this as a private application, which means that only people in my team and folks that that I give access to will be able to view this application. And so you might imagine having different applications for different groups at your organization, one for marketing, one for sales. You can control that access via the private option here. You can add a password to it so that only folks with a password can see it. Or what I'm going to do in this case is I'm going to publish this publicly so that I can share this URL with you all. And so what Replet is going to do, it's going to package all of this up. It's going to do a security scan. Here's another great thing about Replet. We put a lot of emphasis on security. So things like this where automatically your build is going to get scanned for any vulnerabilities, any issues and then it's going to go start building this out. But in just like another minute or so, this URL will be available and I'll be able to share it with everybody in the chat. And that's how that's how you share your applications in Replet. This is a really really convenient part of Replet. Otherwise, what you'd have to do is you'd have to send this to some hosting service. You'd have to stand up your own production databases. You have to do authentication. You just a bunch of like nuance things, the DevOps things that people don't want to worry about. All of that is handled for you directly here within Replet. >> Awesome. I think there is just one more question about I think connecting to databases and storing data. I don't want to say that triplet stores like data but it's more like are there >> so sorry >> yeah and and I was looking up in this instance I am not storing um data I don't I don't have a database but there if you're doing transformations because we can't write back to snowflake you could stand up a new database ASE with those transformations so that they're still available through your application. In which case then it's very it's super easy to stand up a database within Replet and it's native within Replet. There's other platforms out there where you'd have to go externally and set up a new database and set up the configuration. That's not the case in Replet. If you if if there's something that requires you to have your own database, agent will build that out for you and it's hosted here on Replet. you can see it and it also gets pushed to production when you deploy. >> Gotcha. Okay. I think that's very helpful because I was going to say for just read only I don't think replic would store the data but then when you do have transformation that makes sense but I love that you don't have to go and create your own like you know database or like really think about installing anything else and just keep that going. >> I'm dropping I'm dropping the link in the in the chat. Um, >> did it go through? Okay, I think it went through. [sighs] >> Oops. >> Right. >> And so there like in what, like less than 40 minutes, we went from an idea to making some changes to deploying and now that app is live for you to share on the web. I think it's just awesome. [laughter] >> No, it is. I think I think there are two types of people here. I want to say, right? One is I'm a business user and I need a report for my QBR or for my weekly update or whatever. I have to slack my data person. I'm like, can you quickly pull this data for me? Can you quickly create this report for me? And then like you go on waiting because you don't know when that's going to come back up. There's this whole long, you know, wait and hope and pray cycle that you can avoid. Literally just connect to your snowflake database, use replet, ask what you want, get your answers instantly. probably doesn't take more than 5 10 minutes for you to get to your answers. If you're a data person, you can think about using it way more, you know, powerfully than just, you know, building oneoff reports and dashboards, right? You're a data person, you can think of what kind of questions keep coming up very frequently. Why do I have to keep context switching between every question and you know generate custom reports for each of these? Think about building a report and sharing it with different users on the business team and see how you can optimize. There is so much more that you can do. >> Here's another really great thing about Replet because it's browserbased. You can open up multiple Replet agents and start building multiple projects at once. So, this was me last night as I was thinking about what I was going to show all of you and I was kind of going through different ideas and I had five different ideas and I said, you know, why don't I just build all of these at once and see which ones look the mo the most interesting. And so, I just opened up five different browsers. I put in the different prompts and I had agent build all of those out. And so, I think this is sort of a glimpse of what you're going to see in the future. I think you're going to have multiple agents working on different problems and different types of solutions. Then you're going to go and you're going to review them. It's like, \"Yeah, I like that one. I like this one. Make these changes here.\" And do all this stuff. So, you're going to have an army of data analysts and and data agents working for you. And this is a really neat thing about Replet. Again, because it's browserbased, you just open a new tab, you go to replet.com, you put in a new prompt, and it just starts clicking on these, and you can just review the work after it's all done. That was pretty cool. >> No, this is Oh my god. Are you even kidding me? This >> So like if you're So again, if you're a busy data analyst and you know, you've got all these requests coming in. Okay, like you know, you can spend you know, maybe like the first pass you do is just generate a whole bunch of prompts and just drop them into all all your replet agents and then have you know do have them do a first pass and then you pick up where they leave off and you just continue to work on stuff. just just the iteration cycle, the prototyping cycle, it's it's just so much faster now. >> This is this is fantastic. Okay, I think we are sort of close to the end of the episode, but I also know there's tons of questions and folks, we are coming back with a follow on episode. We will dive deep into what other apps that you can build or really answer all the questions that you have asked if we we were not able to get to it today. But there are two things. one, what is your background? Have you already wipe coded with replet? If not, go check it out, test it out, and then let us know in the comments. We are going to, I promise you, between Manny and I, we are going to read all the comments, get your questions, and answer them in a following episode. We will, you know, schedule it for the upcoming weeks. But before we get to that, I want you to go build out your first ever data app with Replet and then come back tell us show us the feedback like tell us that was  and you need to improve and we are here to listen to you and make your experience better too. So let's keep that going keep building and Minnie any last word before we wrap up? >> Yeah, I just I just want to say you know we're early on in uh vibe coding for data analytics and data science. I think this is a big first step in in that trend and helping you do that. Um, this this Snowflake and Replet connector, but this is a long-term trend. This is where things are going. Um, go and build something. Start vibe coding with Replet and Snowflake. We're eager to hear your feedback. As Veno mentioned, this is a newer um, integration that we've built out. I'm sure some things can be a little bit better, but we're only going to improve and make this better for you. And we want this to be a must-have tool in your toolkit because I think it's just going to empower some of you so much more. Uh going to empower many of you to do so much more. So, go and build something and then let us know what works, what doesn't work, how you're using the connector, what you're building, and just eager to see what the community builds. >> Yep. Like I said everybody, if you have not been W coding, this is your sign to go test it out today, right now. Spend 10 minutes. Doesn't take a long time. Let us know how your experience was and we will see you again soon in another episode. Thank you so much. Thanks everybody. "}
{"url": "https://www.youtube.com/watch?v=va3p5s-r4rg", "title": "[LIVE] Use Snowflake SQL to find a needle in a haystack", "transcript": "All right, we are live. Good morning everybody. I am Veno Disami, a developer advocate at Snowflake. And in this week's live, we have a very exciting subject. We're going to talk about the search function at Snowflake and how to efficiently search for specific strings across your Snowflake environment. You could quickly identify a specific string and search for it across databases, schema, tables and columns for various reasons of course you know faster troubleshooting, data discovery, compliance, governance and whatnot. But to talk about how to efficiently use search function or any other functions to do search effectively at Snowflake I have with me Himmanu Kpal who has 30 plus years of experience working in different database technologies starting from inform SQL server Oracle and using several tools like obnoxio data stage and so on and currently he works in the higher education space helps migrating the ELTL process from data stage or Oracle to snowflake and that with that said Himmanu very excited to have you back again for another episode on a very interesting topic. Hi Vinu, good afternoon. Thank you for having me here and yes I am excited >> right I remember because the last episode that you did was one of the highly engaged episodes with a lot of questions if I remember and thanks for taking all the questions for us and I am expecting no pressure no pressure at all for today's episode but I was just thinking before we get started on the search function tell us a little bit about how exactly did you decide to write this blog that you wrote and like how are you working with the search function on a day-to-day today. >> So we know you know I was uh thinking of a way of you know if somebody comes up and says you know this is my value and try to see where this string is used. Mhm. >> So one simple step would be you know going through all the tables and writing a select for all the columns you know all the columns for a table and all the tables in the schema and all the databases in your account. >> Mhm. And uh I was going through snowflake documentation and came through this search option and I said okay you know what this is good because this had an option of where you could just give the table name and uh say search for this string and it will give you the values though it did not satisfy the completed requirement it but at least you don't have to go or write a dynamic SQL going through each columns for a table. That's why >> that's why I said okay let me try this. >> Got it. So you needed to search for specific strings from the entire database like across databases and across schema and of course writing a select statement for every single column and then like extending it for a whole database and you don't know how many tables and schemas you have in a database. Sometimes that can blow up really fast. So you chose to sort of write your own procedure. >> Yeah. So what I did is you know the today's demo is going to go through that. What I did was uh created uh one first procedure where it will search through all your databases all the schema in the database and all the tables in the database >> and if and give you the count of that particular value and store it into a table. Okay. I see >> with and what I'm doing is you know having a batch ID generated for your each run. So let's say today I start my run and my batch ID is number one. >> Mhm. >> Then I call my second procedure passing the batch ID. It will go read this table uh where I'm storing the data and go through all the all the data. And here I'm going the second procedure I'm going to do a select and do a wear clause. First procedure I use a search function and second procedure I do you know simple where which anybody would have done that way. >> Got it. Okay. And I think we will see both of them in action and see how they may be different and when to use what and what are the performance consideration or even cost considerations and whatnot. And I know you have uh a few slides for us so it's easy for us to follow through what you're going to be showing us. >> Okay. Yeah. >> Do you want to go over them? So let me share the slide. >> Yes, I think we see the demo screen UI. >> Yeah, >> let me share. Mhm. While you pull that up for the folks who are watching, if you have any questions or if you already are using the search function effectively or if you have any other specific requirements for which you're writing your own custom procedures and whatnot, tell us in the comments and send your questions in for him too. Okay. Uh we know I think you're able to see the presentation correct? Y >> you have you have a requirement and I'm for our example I'm just taking a string which is basically SSN. Okay. So you know normal circumstances I would have written a select statement going through each column for a table and then you know repeated the for all the tables in the schema and all the schema in the databases and then all the databases in my account. >> Mhm. >> So snowflake we can use this search function which can search from a variant object and array columns also. You know this is a good thing. >> Basically what it does it you know it tries to break breaks break the your search string into token and in uh this function has four parameters and one of the parameter where you can specify you want to search exactly the whole string or any of the characters that are present in the string. >> Great. And you know this is this uh this is a search function and first parameter will be how you want to search whether you want to search >> for a particular column name or you just give star and you just give the string. The second parameter is your string. >> Mhm. The fourth parameter is your search mode which will tell you whether you want or that means you want to tokenize your data or your string >> and you want to do an exact and or if paraphrase okay like in I'm just doing a select search king for the value king in this >> uh the string is king and it will return a true okay >> and the second example this will return a false. So it returns true or false. >> Mhm. >> And here there are the parameters and that's it. I think uh I will go to the demo. >> Mhm. Yep. We can get started with the demo. So it's very interesting. It's a super simple function. It just takes four parameters. Two of them mandatory, two of them optional. If you want to go into the analyzer mode in specific search mode, you can do that. If not, simply just give the name of the column you want to search in and the actual phrase that you're looking to search and interesting. Yes. Now let's see the demo and how this works in action. >> Okay. So you know I will this is let's say this is my table. >> Mhm. And I want to search for this string. >> Mhm. >> And this is my search parameter. >> Mhm. >> And I am saying, you know, C star, which means C is an alias for the table. And this is my search. >> Uhhuh. And that's going to return me. So if you see it has returned me multiple rows. >> Mhm. >> And if you see here there is only one column or one row which contains this value the com the complete string whereas other might be having a partial. Mhm. >> So in my second example, I'm doing with the search mode equal to and which means search for the complete string >> and this is going to return to me only one row because only one row and one row had multiple columns that's a different thing but one row >> had the in it. >> Got it. So when you do the search mode and it really is looking for all of the the entire match honestly >> and the entire match yes >> and search is uh you know it it it is case insensitive so you know it won't whatever you pass it will look for upper and lower cases also. >> I see. >> Okay. So what I have done is we know um I created one procedure very simple one it will go >> this procedure assumes that you know you have access to all your databases and all your schema because uh you know or or if you want you can just filter out by by saying search only for a particular >> Mhm. >> uh database. >> I see. Yeah. specific DBN schema. >> Mhm. >> Okay. So, sorry. And this is my second. Okay. This is my procedure where I'm passing my string. >> Okay. And it does a very simple loop where it is going over through all your databases. >> Okay. Ignoring sample data and the snowflake database. >> And this is this part where I have commented where you know you can filter out saying my search only for this database. If you want you can you can go for a schema level also but for our example I'm just going through our through all the databases where whatever are there in my account >> and then I go in a loop where I go I select that particular table. Oh you know what before that I select all the schemas. >> Mhm. >> And then in the third one I select in my third loop I select all my tables. in that schema. So once I have all my tables in the schema, I just go loop going through all the tables and running this SQL which is going to say select count from your this where your search mode is equal to this and I'm doing in my example this is with and otherwise you know you can remove the and and if you want to search for any values and search has one more option you know if you if you want to search multiple strings That means you know let's say you want to search for this and you want to search for another string you can just add it with a comma and say that this is my other string which is 2 3 4 5. Okay. >> Mhm. >> Now once you have those your select runs and you capture that data into a table. >> You data into a table. So I am inserting it into a table which is called search table data count where a batch ID which is generated every time you run it captures your database name where you are running for the table schema name the table name and the row count and this is >> this is for your debugging p purpose I'm just capturing the SQL ST string search is one of the parame values that is captured. So it goes in a loop and it captures that. So this is the first procedure and the second procedure is once you have your uh all the tables captured. Second procedure is pretty simple. It is doing what you would have done under a normal circumstances which means you could you would have written a SQL saying that where uh search for all the columns column one column 2 column n. Okay. >> Mhm. Second procedure, you pass in the batch ID and it is going to reuse this batch ID and read from your table where you have stored your search data table count. Okay. And then it searches and same thing it goes again captures the database name the schema name and then then it goes to the information information schema and looks for your data type because this is important when you try to search for a data type which is not text variant or an array or an object you get an error. So I am filtering out only for this particular tables. Okay. >> Once I do that and then it is simple where we just go and generate the SQL. >> Mhm. >> SQL where you are searching for this count. Okay. And this is also inserted and stored into a particular table which is called search table column count. So that this table has your detail for each and every columns wherever it was searched. >> And I think that that is those are the two procedures. Procedure one captures all your table and procedure takes that whatever was captured in those table reads reads the table one by one and >> uh goes over that >> loops through them. Yeah. >> Yeah. >> And I think Yeah. I think this is great. >> Um I have a question. >> Yeah, >> sorry before you proceed further. I feel like there is this one question from Tuan who's trying to understand is the strength of the search function essentially searching for strings across the views, tables, schemas and database. I know we covered this but maybe it's good to reinforce or reiterate that. >> Uh yes it is. uh in this example it is going through all the tables but uh you can put a filter and you can go through all the views also. >> Mhm. Got it. >> Yeah. >> Is it fair to say the search function essentially helps you search for a specific string in a table or a view. But if you want to do the same thing for an entire database or entire schema which has multiple tables or multiple views, what you have done here is essentially write a procedure that sort of loops through all the tables in the schema or all the schemas in the database and really you know tackles this one step at a time if that's helpful. >> Correct. >> Right. >> Yeah. >> Awesome. All right. Carry on with what you were going to say. >> Okay. So you know I will just go through a a demo of this and let's say I want to search for this particular string >> which was there in our example. So what I'm going to do is call my first procedure which is named as search data in all the DB and that's it. So let's run this. >> Mhm. Okay, succeeded. Now there is one table where it is storing you know all the batch ID. I think it has couple of columns. So I will just get my batch ID for my run and I it is 103 you know it ran today. Okay. So I'll take this number and 103 is my >> batch ID and pass. Oh you know what before that I will show you what was captured by this. So if I go and table count this will be the list of tables where that data was present. And if you see here the row count will tell you how many you know which tables had one more than one rows. Okay, >> got it. >> I will take this one3 and run this. What it is going to do is going to go loop through a loop of this 103 and search for all the tables. and uh you know uh when the date the sample code and the whole demo has been uploaded into the medium article also which will be shared in the post. Okay. So this was completed. Now let's see this is my table which is storing the data >> the accounts and if you see here I I should have added this row count as the first column but there are some tables where the row count or the some table where the column count for that particular value is zero. So what I'm going to do is I'm going to just say that uh order by I just say where Okay. So if you see I just gave give me only rows where the row count is greater than one and this is my these are my tables and the list of columns where this particular string matched >> and the row count was greater than zero. So this is you know this is what I want and once I have this list I know my that my that particular string is present in the following columns tables schema and the database. >> Mhm. Yep. This is perfect and yeah I think like you said this is great demo. Firstly, all of the contents of the demo is available in a medium blog and I see that we have also posted the link to Himanchu's medium blog in the comments for folks asking for that. Please check it out. And yes, one last question. I'm opening up the floor for questions and comments for folks who are watching. But before we get there, I have my question to Himanchu. I know we talked about using search versus just using you know wear clause and searching for strings in a wear clause and how did you like how did you decide between one over the other or what should I know when I'm going to try this out today. >> So basically you know this is useful when let's say you have a 200 column table >> Mhm. >> and you want to search for that particular value. >> Mhm. uh one option is you know you just write dynamic SQL where you go through all the all the columns and put this value your string in the wear clause and try to get your data >> or can use this search. So this will be uh beneficial and uh I did a small test and I did not see much cost difference between using a search and using a wear clause. >> Yeah. >> Yes. Yeah. I I know. Although if you sort of look up under the hood of how the wear clause I mean searching for a specific value in a column using a wear clause versus running it through search and essentially when you put the same condition in a wear clause and look for text match. Snowflake is trying to use the micropartition metadata and use that for pruning like the min and max of specific column and whatnot to do this filtering in wear class. Whereas when you do it in this using the search function, search is a proper text analysis function. It has a tokenizer. It tokenizes the text that you're giving in and it by default takes care of you know the case insensitivities or if you have you know few words prior and after and there is so much more you can do with search function because of the nature of it right it has a tokenizer built in and it is a proper semantic search and so you may want to explore search if you have like a full-blown text analysis workflows that you're thinking of similar to what is doing here that head. I I think I will maybe give a few seconds for folks if you have any questions before we let go of Himmanu. I know he's been doing a lot of side experiments too, testing out where versus search and we probably will hear more from that very soon. It looks like yes, Dwan's questions answered. Mark, thank you so much. And Shahab has a question. and he says, \"How does it deal with masked data?\" Have we tried that out before? >> I have not tried with masked data. >> Mhm. I guess >> masking masking policy is role based. Am I right? So I think it should not if if you don't have access, you won't be able to see it. >> Mhm. Yeah. So masking. >> Yeah. At least that's how from the security point of view if you are if you don't have access you will not be able to see it. >> Mhm. Yeah. I think that makes sense honestly cuz we already don't have access to this text under the hood. So what what is under this mask? I don't think if we will be able to but with that said maybe it's a good time for us to go test it out. So, Shahab, if you do end up testing it out with mast data, please let us know in the comments or I don't know thing Himanchchu, he's always excited to hear from fellow Snowflake experts and learn more. I know he talked so much about continuous learning and he's been in this field for a few decades and he's still like trying out new things and doing weekend experiments, coming here, sharing his lessons with us. So, yes, firstly, I like the spirit of learning and Shahab, if you do get to test it out, let us know. And with that said, I think yes, Mark also thinks it probably does not consider it in the search. I love how the community sort of comes in and like we all learn from each other. Thanks Mark. Thanks Shahab. And yes, with that said, we are at the end of the episode. Firstly, thanks Himmanshu for coming to the show again and sharing your learning and wisdom, can I say, with us on how to use Snowflake effectively. We truly appreciate this and yeah thank you so much for folks watching there is a medium blog link in the comments please do check it out and I don't know like comment let him know how useful your medium I mean his medium blog was for you so he can keep on writing more and share his learnings >> okay thank you thank you Vinu thank you thank Mhm. "}
{"url": "https://www.youtube.com/watch?v=pCOAiBPgnrA", "title": "Build an Open Lakehouse with Snowflake and Microsoft OneLake", "transcript": "You have data landing in Microsoft OneLink from dynamics, from real-time intelligence, and from virtualized shortcuts. Now, you need to figure out how does Snowflake use this data without copying it, breaking governance, or creating yet another pipeline. I'm Eric Tott, a partner solution engineer covering all things Microsoft. And in this video, we're going to walk through building an iceberg lakehouse architecture with Snowflake and One Lake, looking at how it works and how to evaluate if this model fits your data strategy. Every conversation today eventually lands in the same place. How do we use AI across the business? How do we embed intelligence into applications? How do we move from static dashboards to real time AIdriven decisioning? But AI is only as good as the data you give it. This is the guiding vision of the Snowflake and Microsoft partnership. Microsoft is the central business applications that users interact with all day, key to their daily workflows. Meanwhile, Snowflake sits at the center of the d enterprise data estate where data is engineered, governed, shared, secured, and scaled across clouds and business units. Microsoft focuses on how people and applications interface with intelligence. And Snowflake focuses on how the data is prepared, governed, and delivered to agents. That creates an entirely new kind of pressure on the data layer. It's about building a data foundation that can safely power analytics, applications, and AI all at the same time. And that's why we're building integrations across every layer. PowerBI and Snowflake working together for governed analytics. Open AAI running directly against the data in Snowflake. Power Platform and C-Pilot accessing enterprise data for automation and decisioning. And Cortex agents bringing AI directly to where users are seeking intelligent insights. To support all of this, we need a solid architecture at the core. The data layer must be open, reusable, and trusted across every platform and engine that uses it. And that's where the iceberg lakehouse fits into this joint vision. The challenge is making them work together without duplicating your data estate. For years, the default answer was replication. Move the data, sync the data, and store another copy. And with that came higher costs, delayed insights, and fractured security models. And governance had drifted out of alignment. This is exactly what Apache Iceberg was designed to fix. Data becomes open, governed, and directly accessible by Snowflake, by fabric, and by any engine that speaks the iceberg standard. Before we jump into the architecture, it's important to clarify you do not need a multiplatform lakehouse to integrate Snowflake with Azure. There are already proven integration patterns used today. PowerBI directly on Snowflake, data factory for pipelines, and copilot or applications calling Snowflake directly through APIs. These approaches are mature, fast to implement, operationally familiar, and for many teams, they remain an excellent choice. Multiplatform interoperability is not a replacement for these patterns. It's an extension of what's possible. It does introduce additional architectural layers, more coordination between platforms, and more governance and operational planning. But what you get in return is long-term flexibility, deeper interoperability, and a more futureproof data foundation. As real-time data, multi-engine access, and cross-platform AI become more central, the iceberg lakehouse becomes a powerful architecture to support your future growth. So now we've talked about the why, and now we need to talk about the how. In this sample architecture, we have data landing into one lake through virtualization, mirroring, real-time intelligence, and pipelines. One lake makes it easy to collect data from disparate sources in a variety of ways. This forms our raw data lake. When data lands in one lake, it's stored as parquet files. What's important is those files automatically receive delta and iceberg metadata. This is what allows fabric engines, which are primarily delta based, and snowflakes's iceberg engine to operate on the same data sets. Now, for Snowflake to read the data in one lake, we use a feature called catalog link databases. This leverages the one lake iceberg rest catalog APIs to allow Snowflake to access both the iceberg metadata files and the underlying data files making the data appear in Snowflake like it would any other database queryable by both SQL and notebooks. From here, Snowflake can process the data like any other data engineering workload, merging it with data already ingested directly into Snowflake native tables along with data from Snowflake data sharing. Data is modeled, business logic and metrics are added, and curated data sets with semantic views for both analytics and AI are generated. This curated layer is built using Snowflake's managed iceberg tables back in one link. Once again, making the data accessible by both Snowflake's iceberg engines and fabrics engines. Intelligent Cortex agents are accessible across the Microsoft AI estate and power Power Power PowerBI can run off of Snowflake across all of its modes and any downstream Azure services using one lake can operate on this data. Let's see what this looks like in action. We have data in one lake. An Azure app registration is set up to enable OOTH access and this is used to create a catalog integration in Snowflake. The catalog integration is a named account level Snowflake object that stores information about how your table metadata is organized. In this case, it's telling Snowflake that the OneLink table APIs are the source of truth for the iceberg data in one link. And then an external volume is created. This tells Snowflake the data files are being stored externally. So the catalog integration is for metadata files and the external volume is for data files. This automatically creates a multi-tenant service principle that needs to be added to fabric to allow Snowflake access to the data. Now with everything linked up, the catalog link database can be created in Snowflake. And now the lakehouse tables in one lake show up in Snowflake just like any other data object. Once in Snowflake, this raw data is modeled into a gold layer using dynamic tables, enriched by data brought in from Snowflake sharing. And we want this curated database to be written back into iceberg in one lake. In fabric, a connection is created to snowflake with the multi-tenant service principle being given access from the snowflake UI under ingestion and add data. One lake is selected and the relevant connection information is filled in. Iceberg tables are created with the curated gold data sets and from fabric you can see a snowflake database has been created with the tables inside of it. This makes all of the data available within fabric. Once the setup is done, additional catalog link databases and snowflake manage iceberg can easily be added as well without repeating the connectivity steps. And there you go, a shared iceberg lakehouse that lets Snowflake and one link operate on the same data with a unified open foundation. A clean unified design built for modern analytics and AI. It's a powerful architecture designed for the future. Thanks for watching. "}
{"url": "https://www.youtube.com/watch?v=SzZOOAxzKdE", "title": "Everyone should learn to code. Watch the full conversation over on @SnowflakeInc  #ai", "transcript": "I think here's the sea of snowflake writing code. One thing I think we need to get all of the world to is I think everyone should learn to code. Not just folks like the three of us, but >> the bar to coding is now lower than it ever has been. That's right. People that code, be it cos and marketers, recruiters, not just software engineers, will probably get more done than ones that "}
{"url": "https://www.youtube.com/watch?v=12_GZ1r3Bs0", "title": "Andrew Ng's advice for future developers in the world of AI #andrewng #ai", "transcript": "Yeah, I think this is a wonderful time to build something you're passionate >> about the the set of things that are now possible to build with less time, you know, great low cost is much greater than ever before. So, I think >> uh computer scientists, CS majors, I think uptake on unemployment is because um >> I think our universities haven't adapted the curricula fast enough to get involved coding. Even I can't hire enough people that really know AI. But on the flip side, we've reached a point where I think as was saying, it makes sense for everyone to learn to code. Don't code by hand. Don't do the old way. Get AI to help you to code. "}
{"url": "https://www.youtube.com/watch?v=F3E-Z1NYfhA", "title": "An Overview Of The Features of Apache Polaris (Incubating) 1.3", "transcript": "I'm Danica Fine with Snowflake and on behalf of the Apache Polaris open source community, I'm excited to announce that version 1.3 has officially been released. As expected, there's a lot to be excited about. So, let's dive in and see what changes you can expect to see in this latest version. First up, Polaris now has support for Apache iceberg metrics reporting. For Polaris admins, this means that they can now more easily see critical iceberg metrics within Polaris own logs. To enable this functionality, set the Polaris service reporting logger level to info. It is disabled by default. In addition, you can take this functionality even further by implementing a custom reporter to send metrics to external systems for analysis and monitoring. Also, in this release, users will find a new integration with open policy agent to streamline policy management. This added support enables users to delegate authorization decisions to external policy decision points, making it easier for organizations to centralize policy management and implement more complex authorization rules. To get started with the open policy agent integration, set Polaris authorization type to OPA within your Polaris configuration. Another set of impactful changes have been made to enhance catalog federation. In addition to new authentication types for credential vending and a new configuration to allow credential vending, users will also find brand new SIGV4 authentication support, as well as location-based access restrictions that can be used to block credential vending for remote tables outside of allowed location lists. And an exciting new development for users is the general availability of generic tables, which were introduced in version 1.0. With those new features out of the way, it's time for some rapid fire updates. Building on changes from version 1.2 where Polaris added support for S3 compatible storage that doesn't require STS, the Polaris CLI now has a no STS flag. Also, the requirements around S3 ARN were relaxed within Polaris, enabling it to connect to more nonAWS S3 storage appliances. Speaking of, a new integration with Apache Ozone has been added with a quick start for folks to use to get started with this storage provider. The Polaris Python client has been updated to accommodate running on Windows. A check sum has been added to the Helm deployment. This means that when the config map has changed, it will automatically restart and client.reion is no longer considered a credential property. This was updated to align with changes that were made to the iceberg rest catalog API. To round out this release, we have a few breaking changes that users should be aware of. First up, the Eclipse Link persistence implementation has been completely removed in this release. The default request ID header name has been updated from Polaris request ID to X request ID. Support for AWS key management service on a per catalog level basis has been added to Polaris. The community is currently working on per table level support work. And in version 1.2, to you learned that the legacy management endpoints at metrics and health check had been marked deprecated. In this release, those legacy endpoints have officially been removed and you should now use the standard management endpoints at Qtrics and Q Health. And with that, Polaris 1.3 is in the books. The final Polaris release of 2025. Personally, it's been exciting to me to see all of the work that the Polaris community has done over the course of this year, and I can't wait to see what's in store for 2026. So, what are you waiting for? Get out there, try version 1.3, let us know what you think, and get ready for another great year for Polaris. See you next time. "}
{"url": "https://www.youtube.com/watch?v=WOLGCPCbFAQ", "title": "Snowflake BUILD London 2026", "transcript": "Get ready builders. Snowflake build is taking over the intercontinental O2. Get hands-on in labs. [music] Dive deep with Snowflake experts and connect with over a thousand of your peers. See how the world's most innovative [music] teams are shipping groundbreaking apps in the AI data cloud. This is Europe's biggest day for Snowflake developers. Don't wait for the future. Build it. [music] "}
{"url": "https://www.youtube.com/watch?v=FxqFEx-JdJw", "title": "AI Agents For Querying PostGIS", "transcript": "Well, I want to thank our next speaker, Brendan Ashworth, who is duly waited around here till the end of the post day. Um, thank you very much for your patience and waiting. Uh, Brendan is a co-founder of Bunting Labs. Um, and if I'm not mistaken, Bunting Labs was doing GOAI before GOAI was thing that people were talking about a lot. I remember fondly I think you did some very very clever digitization tools um that that I saw being uh being promoted as GOI back before everyone was talking agents. So uh so I'm glad you're still doing it. Haven't given it up, right? Still on the goi train. >> And you're going to tell us uh some useful and practical things about goi agents if I'm not mistaken. >> Yeah, that's right. Um >> awesome. >> Great. Take it away. Thank you so much for inviting me here. Um, let me get my slides. Can you see this? Okay, >> sure can. >> And you can hear me? All right. >> Sure can. >> Awesome. Uh, thank you so much all of you for being here um and accompanying my talk. Um, and Liz Paul, thanks so much for the invite. Um, so I'm going to be talking today about AI agents for quering postress and AI agents. You know, this is um I want to get started on the right track, which is that when people talk about agents, they mean all sorts of different things. It's really just a buzzword at this point. Um, and I don't want to lean into the buzzword side of this talk. Um, I think it's posters day really. We're celebrating um a great technology here and I think being really um technical and uh precise is really the move. And so when I'm talking about agents, I'm not talking about them in the way that most companies and enterprises or social media talk about agents. Um which is this thing that's coming for everyone's jobs. That's really not the point here. An agent uh just means that you're giving feedback to the LOM as it accomplishes a task. And so we're not talking about the human replacement uh concept that a lot of companies are going after. It's really just that an agent is doing using tools to accomplish tasks on behalf of someone normally a human. Um and it's getting the feedback from those tools and then using them to uh accomplish tasks. Um, and so we're going to dive really deep uh into how AI agents impact postGIS, how they impact spatial data, um, and how this all really comes to together in practice. Um, so to start, I think maybe a history lesson would be useful. Um, a long time ago there was just humans and then they created maps and the map was just the physical paper with the map data on the paper and then along came the computer and some very smart people put the map data on the computer and it turns out this was very useful and I think we're entering a new paradigm today where AI is now on the scene and there's an open question in GIS spatial data post as to what this new interaction action looks like. Um, are we in for a new um, experience of working with maps and spatial data? And I think this question actually has uh, questions within this question. Um, are these new platforms? Is it being built into an existing platform? Uh, are there new technologies that we still need in order to take advantage of this? Uh, how much is the user experience actually changing? Is the same map experience and spatial and tabular data, are they the same? Uh, can you still give the user low-level control of an AIdriven map software? Um, what happens to the jobs? Who what about data privacy and ownership concerns? Every time you use AI software, it's being sent to somebody else's server. You know what? What's up with that? Don't you want ownership of your data? Your enterprise wants ownership of their data. And at the end of the day, who owns the LM? I think there are a lot of very uh compelling questions in this and we really only have a couple of um instances where people have offered a view of what the human AI map in iteration interaction looks like. Um this is really just this is just open AI chatbt. Um I asked it for some coffee shops near me and it gave me a map. I think early on people were skeptical of the fact that of the idea that LLMs could give you maps. Um, but I think it's definitely uh apparent now that if you give an agent an LLM the right tools, it can give you sometimes great output. And so why am I talking about this? Like what do I know about human and the AI and the map interaction? Um, what is Bunting Labs? Uh well, I've actually I've built a couple of AI agents for GIS. Um the first one is a product that we call Q. Um QE is an AI copilot inside QGIS. So it's embedded in the sidebar. Um and it actually has access to the same tools that you do in QGIS. And so you can ask it to do things here. It's asking for an intersection and it's running geoprocessing tools. Um and it's actually giving the output in the window. Um we also are working on Mundi now. Um Mundi is an AI webGIS uh that actually is built for more close uh human AI interaction. Um and one of the best uses of Mundi today is in interacting with postris and spatial databases. Uh Mundi is open source. You can actually go and check out the code online and deploy it on your own servers. And so the uh million-dollar question here is will AI agents change interaction with spatial data? Are maps going to change forever? Is posters going to change forever? Um I'm of the opinion 100% yes. And that's not just because I, you know, uh it's not just because I've already led myself to believe this. I think there's some very compelling evidence for it. And I think if you've ever gone to ChatgBT and maybe copied in your database schema into the input text box and then you get a query back answering a question that you originally had and then you get some response back from postris and then you copy and paste that into chatbt. The key insight here is that you're actually acting as the agent there. your knowledge, your intelligence is being added on top of an LLM to actually write SQL queries, ingest all of the data that's uh coming out as a result and iterating on that feedback. And so the pitch of AI agents isn't that you can do um the pitch of AI agents is not to automate uh large swaps of things. It's to do it faster and better. And I think that's really the core concept behind Mundi. Um, so here I have loaded a postress database with 111 tables. This is actually a uh freely available database online that describes oil and gas uh exploration uh in the offshore area um I guess east of Norway. And this database contains all sorts of complexities uh in terms of schemas in terms of uh relations between different tables. Um probably only about 15 of these tables actually contain spatial data but you might have 10 tables that relate to the spatial data column um to the spa to the table that has spatial data. Um and here we're using Mundi the AI WebGis to actually autonomously uh query the postress database and it's developing an understanding of the schema that we have and also and the metadata we've attached to it and uh answering questions and adding data from the posters database to my map. Um and so we've given it a bunch of tools. I'm going to dive really concretely into what's actually going on here. Um, you can see we can zoom in. Uh, all of these tiles are being served directly from PostJS using this query that we've created on the fly. Um, and perhaps [clears throat] it's going to go ahead and set the symbology for us. And these are data driven symbologies based on the attributes um on the Postgress tables. So that's kind of an early window into what's happening and I want to dive really concretely into what makes an AI agent for post tick. There's a lot of nuances and uh opinionated design decisions that go into um making a useful AI agent for exploring spatial and tabular data. Um, I'm going to go really technical here and I'm going to try and make it accessible to an audience that's really familiar with posts and perhaps less familiar with the um, uh, vocabulary and functionings of LLM. Uh, please feel free to drop anything in the Q&A um, and I'll either answer it during or after my talk. Um, but with that in mind, I've chosen seven really key important um, topics to think about as you look and work with AI agents for GIS and spatial data. Um, and to cover this flow real quick, uh, you really just start with the user query. In the beginning you might be giving a really really simple question. What's the most valuable company in the world right now? And then the agent is just um designing it's it's planning and then using tools to answer your question. So it might be querying a uh postgris table with the companies in it and then um looking at the attributes uh the columns on those tables. It'll reflect and then answer your question. This is the same flow that all chatbt has right now. Um, and I think there's uh nuances to turning the chatgpt um.com interface into something that's really useful for spatial data today. The first one is actually formatting the database schema. And this is this sounds really boring. Um, if you look at the way that you might currently describe your database schema, you might be in Psql, um, and you might use like the slashd or slashd plus. Um, and it turns out that these are very verbose and they uh have a very um they only describe certain parts of your database with lots of characters. And when you put uh text into an LLM, you're actually charged by the quantity of text you're inputting. Uh those are called tokens. And the more tokens you put in, the more you're charged. Not only are you charged more, but you're charged more every loop of your agent. And so if you have a complex uh request that you're asking some sort of AI, um the agent might be doing 10 or 20 loops. And so every time you're getting charged for the uh database schema that you sent first uh which is really this base level of information about your data. Um and how to present that really clearly is actually kind of difficult. Um because uh how you present the schema is opinionated. Um how important and how should you emphasize the foreign key relations? Um how do you want to describe all of the types? uh is it important to say the SRID um of these geometries? Uh there's kind of a whole bunch of things. It's really um a lot of experimentation goes into it. And what we've what we've actually done is when you first connect to a postress database in Mundi. Um again the code for this is actually open source so you can go online and take a look. um is we query and take a look at your tables to understand example values and to understand the schema of each of these tables and then we generate a database documentation. Um this is actually useful even outside of GIS and AIGIS we generate almost a Wikipedia page for your Postgress database. Um, and this describes it has an erd diagram that kind of describes uh the relations the foreign key relations between different tables and it also has uh summaries describing what the important parts of your databases and what you can focus on. Um, there's a lot of nuances to this as well. Um, we uh it's really important to allow someone to attach metadata like if they have a PDF document um describing what these things are. Um that's super important to allow. The next really interesting one is choosing the right LLM model. Um there are so many today that you can actually choose from. Uh they basically you know you get more and more options as the days goes on. Um important factors include which cloud provider you are uh more amenable to. Um many people often choose the uh uh model that was designed in their country which is pretty uh interesting. Um we work with a lot of European clients and they really prefer uh like kind of European sovereign AI. Um and some other interesting things to consider are how we talked about the system. We talked about um the most important input to your database to your LLM in the beginning is the database documentation. Whenever you hit the LLM again, um some will charge you less for uh text that they've seen before, which is the database schema. Um uh some charge you a lot less. That's really important. Okay, now let's talk about postress. Um, you'll notice that I just put in postress here. And that's not because this is postress day. Um, postress is actually I found to be the best thing to put inside the agent loop. And here's why. Uh, query latency is really important. Um, if you have a a simple uh query on your spatial data that executes in 10 milliseconds, that's golden because you might be iterating on that 10 25 times. um and the latency just multiplies every time and the user will get increasingly frustrated if your data is taking forever to load. Um I've had worse experiences with ductb in the agent loop partially for latency but also um there's another kind of interesting uh insight here. LLMs generate better SQL query for post than they do for duct DB. And to understand this, you actually need to understand how they train large language models. Um, when they first started training large language models, they would train it on the corpus of the internet. And then that was more or less all of the knowledge that they accumulated. And so some of it was Postgress, some of it was Postgis with the ST spatial type functions. Um, some of it was kind of like more of a duct DB um, uh, style SQL and all of that was kind of mixed in together. The AI labs realized that competency and SQL is one of the most important characteristics of an intelligent LLM. And so they created a sandbox for executing Postgress uh, SQL queries. And so this is how LLM AI models are trained today. After this uh initial period of learning, they insert an LLM into a sandbox and they give it positive or negative uh rewards. um they give it positive or negative feedback to indicate that it's having success in querying a Postgress database or it's having failures. And they also do this with Postchris. And so um all of the like deep functionality that postris has accumulated over 24 years um the looms have trained on and when duct db fails to have certain spatial type functions for example um the entire query fails and it has to rewrite it. This is pretty difficult. Um, and so as not just because it's postress day, um, postress is definitely the most useful thing to put in the center of this loop. We also have um, if there's any sort of if it encounters an error, the most simple is like a query syntax error. Um, but if you're not getting the right result, if you there's some sort of a further processing that needs to happen in order to answer the user's query, you need to handle the failure and get this information and put it back into the LM. This is probably [clears throat] the most simplest part of this loop. Um, but you're really just asking yourself, how can we get more information about this problem and give it to that? Um, here you can just copy and paste the error message, but I think some really smart ways to do this are surfacing more of the database schema that's relevant to the last error and giving it back. I guess it's this is cut off a little bit, but um there's often schema ambiguity in Postgress databases, which is really interesting. So here let me give you this is a real example from the posters database that we're using. So here I've asked it what are the most productive fields by oil output. [clears throat] It's going to query my query my posters table and it's given me a table with on the left let me see if I can pause it on the left it has the field name and on the right it has rec recoverable oil in uh million cubic meters. This is really interesting because this database in particular has ambiguous units on its metrics. And so this is actually described as a number type and the LLM is inferring here that it's a million cubic meters. Um this is kind of one of the more nuanced aspects to AI agents querying um postress data and that is um not only has it I don't really want to call it a hallucination because it could be correct in this instance um but it's certainly inferred this unit type and it has not made it clear to the user that this was an inference. It's just kind of declared it as fact. Um and so dealing with ambiguity and presenting all of the um important units to the LM in the beginning is really really important. Finally, we want to make the resulting query interpretable to the user. Um this is quite um important because for users that are of lower um familiarity and actually users with higher familiarity um the LLM can present things in a very compelling way and you need to actually ground the results in the query that it wrote to make sure that it's ex answering your question exactly as you intended. Um the way that we do this is we actually surface the syntax highlighted SQL queries directly in the sidebar in Moody. Um and so on the right these are the AI generated um SQL queries. Uh you can see that it's highlighting the um functions that it's using. Uh and these layers are then added to the layer list which you'll be able to see in the next one. Um these layers are then added to the layer list, added to your map uh and made interpretable. And so you can actually uh explore the map with data loading directly from Post. Um in Mundi, we actually uh serve tiles directly on Post and we have a read replica that's um uh can autoscale to a high uh level of uh concurrency at least for this demo. And so here it's using here it's using the layer list on the left. So this has um all the postress layers that we have loaded. Um I actually have clicked on one of these fields and in the bottom left it's highlighted. Um so it has the name and the amount of oil. You can see that there's not a unit associated with that recoverable oil attribute. Um, and also in the text box where you communicate with the AI agent, um, there's a cursor icon with a, uh, selection that indicates that the AI agent can actually see your selection. And so if I were to ask a followup, for example, um what are the closest fields to this selected field, it knows the unique ID on the row that I have selected and then it can query um subsequently following up on that. And so the last step in building an AI agent for PostJS is actually security and it's one that nobody's really talking about despite it being really important. Um, if we were trying to take AI agents, if we were trying to prevent AI agents from taking over the world, um, the really simple things are executing queries and read only transactions. Okay, boring, really simple. Everyone should be doing that. But what if someone could steal your data? not um unreently there was a vulnerability in Microsoft Copilot I believe that actually allowed you to um extract somebody else's private data um from their workspace without their permission and that's due to what's called the lethal trifecta. The lethal trifecta is something you must consider when creating an AI agent. And it it has these requirements, access to private data, untrusted content, and the ability to externally communicate, but it only makes sense with an example. So, as an example, consider that I might have a um an AI agent that has access to my Google Drive and my Gmail, and somebody sends me an email that says, \"Hey, can you send me um the bank statement in Google Drive?\" If the AI agent sees that the name on that email is Brendan Ashworth, it might indicate or it might make the agent believe that it can access my private data and externally communicate the uh bank statement from my Google Drive via Gmail to this third party. And so [clears throat] if you have all three of these available on an AI agent, you're actually vulnerable to this attack. Um, a lot of AI agents uh suffer from this in the real world today. Um, we avoid the lethal trifecta by not allowing you to externally communicate. Um, so the user must take the data from the map and do something else with it. Um, but this is really important. And so finally just before I conclude I want to briefly mention MCP. Um I think people talk a lot about MCP and what it could mean for spatial for postress. Um and I want to say that MCP is useful but MCP doesn't it's not a um the holy grail of building AI agents. Um it's a very narrow tool that allows you to connect tools to LLM. Um, and it's not the uh end all be all. I think especially for geospatial and for quering spatial data, a really important aspect of this is the output and handling um both visualizing spatial data on a map for a user and handling the queries and presenting that uh faithfully to a user and MCP doesn't deal with any of this. Um, so I think building I think building AI agents for geospatial and for post is super important. I don't I think MCP is not actually that important. So with that said, thank you so much for listening. Um, thank you for inviting me here actually to share this. Um, if you're interested, you can try Mundi.ai and if you want to talk to me directly, you can email me at brendanbuntinglabs.com. Thank you so much. >> Thank you, Brendan. That's a standout talk and great to get down to the basics. Defining your terms. Imagine that. Defining your terms. What is an agent? Um, it seems pretty magical to me this idea that I just uh ask I I tell it something and it comes back and it looks at what it got and then decides to do it and then it decides and then it decides. What is the stopping condition? what what causes this thing which could apparently go on forever from not from not going on forever. >> Yeah. Um so there's two really interesting parts of that question. Um so the first one is in practice people actually um normally set a max number of steps. >> Um often it's as many as 200 and they say okay at 200 you are doing something wrong let's stop. Um, but I think a really interesting uh way to look at it is what happens when it gets it wrong and it just, you know, it keeps on going. Um, >> uh, and this was especially true early on. um if you asked certain AI agents to um solve really difficult math problems like math problems that the you know uh pretty much most mathematicians thought believe are like unsolvable for decades >> um it would just it would keep thinking um which is really interesting um >> yeah I mean I don't really know if that's it's it's interesting it's interesting >> um and uh and a question from Phil Barty. Um, I'm curious about your postgress posture specific sandbox uh statement. Um, what source uh is this based on oh yeah so this is um uh this is based on my familiarity with the space. Um I don't have a particular source. Um the technical term for this is RL environment. Um so this is when they're doing reinforcement learning. Uh and the way that this kind of works is the AI labs choose a number of RL environments to build. >> Yeah. >> And then they train specifically for these tasks. Um and so it's not that um like I believe based on my experience that they haven't made a duct DB RL environment >> but they have made a postJS RL environment >> and that's based on my experience using SQL written for both >> right >> um but it's not that this is a permanent handicap of these models rather I think in the future it'll be able to do both and kind of context switch. >> Um, but I've had real world problems with this >> as of the last few months. >> Yeah. >> Um, >> yeah. >> So, when an LLM adds a layer, the Mundy say? >> Yeah. >> What's going on behind the scenes? There's there's a new tile layer somewhere, I guess, that's pointing to some custom SQL, which is what just popped out of the back end of the agent, and that all that stuff is being stuck. Yeah. So >> when there's like this concept of a a SQL layer from Post in um there's a couple of different aspects to it. Um first of all there's the tile layer. So we serve the tiles directly from the poster server um based on a kind of uh I guess using that as a subquery. Um and then uh we filter it for the attributes that are actually used in datadriven symbology. And so if [clears throat] the agent has written symbology that depends on certain attributes in that layer, um we hide the columns that are not used for uh the symbology to reduce bandwidth um and processing time on the server. And we only serve for example the unique ID um something for a label and something for um like you know some sort of number driven symbology like population density. Um, >> yeah. >> How do you get your LLM generating symbology that your web client is happy to consume? >> That's also an agentic loop. Um, yeah, because it's really easy to generate incorrect symbology. Um, and most of that is around detecting early that the symbology is wrong and then capturing that information and feeding it back to the LM. >> Um, there's a lot of nuances with that. I feel like I could also give a talk on generated >> and is this your own symbology language or is this a specific map? You get a little bit of >> Yeah. >> leverage from that. Yeah. >> Yeah. Yeah. >> Do you uh ever wish you could like fine-tune your model to know more about some of the domains you want it to speak or is >> Yeah, fine-tuning is definitely a double-edged sword. >> Yeah. [clears throat] Um because it gives you capabilities now but it gives you debt later >> um because if I fine-tune something to be more performant for example on ductbe um and another model in the future becomes more intelligent in a dimension that I'm interested in I am then faced with this choice of losing out the f losing out on the future intelligence at the cost of this very unique um feature. set that I want now. Um, and so that's that's definitely like a live uh trade-off. >> Yeah. >> Yeah. >> Well, uh, Brendan, I want to thank you again for coming to Post Day and sharing your knowledge. Um, it's been an amazing talk and and I've learned a great deal, which is which is the whole point of Post Day. I get the world to come to me. Tell >> tell me the important stuff, but I've learned the important stuff. Thanks again. Thanks so much. Thanks. "}
{"url": "https://www.youtube.com/watch?v=EEeHX65p-ak", "title": "Linear Referencing Using PostGIS", "transcript": "Yeah, Bruce Frindle, thanks for coming. Bruce is a manager at the Mile High Flood District in Denver, a water resources engineer. Been using Postess for hydraulic modeling, stream network analysis, environmental management, um, and as far as I recall, a longtime member of the Postess community. Your name tickles my brain all the way back to the early days. >> I started using PostJS in 2003, I believe it was, and some web mapping applications, which were a lot of fun. and then somebody named Google Maps came out and kind of [laughter] killed that effort. >> Uh thank you very much. Uh I'm going to talk about a nice little side project I did um for our flood warning system and it's using linear referencing which I've always enjoyed but never used in a lot of ways but uh this was a really fun one to do. Uh again I'm with the model high flood district as Paul mentioned uh and our district uses a lot of models uh hydra we call them h&h hydrarology and hydraulics to estimate the flood potential. So if it how much does it rain, how does that translate to stream systems and flood planes? That's what you typically see is a flood plane says, \"Oh, this is a 100red-year flood plane. That's the results of that.\" But we also can use them for daily forecasts and we wanted to use something that would be very easily viewed on a map to uh take a look at [gasps] um because this is the kind of infrastructure we're looking at. You have this nice bike path and a stream as you can see meandering through that. uh you notice the bridge near the top, but when you get a severe rainstorm, these features have to double as a large uh flood conveyance structure. And those two pictures were taken about two days apart. So, we can have very high flows with some very severe rain. And we want to get a handle on how these stream systems will react based on uh we we model them the um the storm sores the uh open channels and everything and bridges and we want to model that to take a look at it. [snorts] Uh how we do that is something called swim which is storm water management model. It's a very wellused and uh well w well established model. It's uh free open source. There's a python version of it. There's a couple PC versions of it that do really well and it simulates um as it says rainfall runoff. Rainfall comes down, hits buildings, hits grass, hits various channels and it converts that rainfall into a uh storm runoff. So we can get some kind of ideas here. Uh this kind of conceptual plan doesn't show the open streams. That's kind of the outside of the drainage pipe. And that's what we want to talk about is the various stream networks that our rain system or drainage systems convey water into. Uh it's not this this is a different kind of swim. Um so the typically how you use it is you define the various schematics in which we call them links and nodes and the nodes define where things come together. They receive runoff from drainage basins. they receive runoff from an upstream channel. And this is typically how it uh works, but it's kind of hard to follow it, especially when you're trying to look at a map and see exactly what's going on. So, this is what we do if we've got it geo referenced by this means the coordinates of the various nodes are in a generally good location. This is what the schematic looks like. Some of it can be pretty good. Some of it can be very confusing. And as they're an underlying stream network that we have in our GIS systems, they go through great pains to align and digitize. And we want to use the stream network as the reference for these schematics so they become much more clear and easier to understand. Uh first off, what we know is the swim model uses an interesting data format. It's all one big text file. And when we use coordinate systems, it's just a space delimited xy coordinates. So there's no uniform uh interface to to take a look at them that we can instantly dump them into a GIS format. But what we can do is just do a copy paste and copy paste into Excel works very very well. I had to use Excel because Brian, our Mr. Excel person, didn't use one this year. So I have to do it this time. And what you can see here is you have the node ID, the X and Y coordinate systems from the swim input file. And then in red I have an additional column that I add. And that's that is our stream code, our notation of this is stream. This node is on stream 44.40. And u that's how we tie it into that. Here's the hard part. That's why it's in red. You kind of have to look and see of all the schematics and what they're talking about which ones are on the stream. When they're on the stream, you tell it which one it is. So, you put that in as a text field. Also, you have to identify maybe a tributary. You put it on the tributary so it all lines up pretty well. But at this point, it's in Excel and you've got all the information that you need. And then, of course, you do the magic of Ogr FD FDW. Um, so we can just go into Excel uh as a foreign data wrapper, tell her what the the uh server is, just the location of the spreadsheet and then do a uh foreign tre create the foreign table that identifies the nodes in this particular spreadsheet. Of course, as you know, it's very easy. You just point to the you go to the directory where your spreadsheet is and tell ogrdwin info and say, \"Build me a foreign table.\" And all this is basically spit out for you. That makes it very easy. And then now you are linking directly to the Excel spreadsheet with the same information. So now you've got the note ID, you've got the XY coordinates and the stream that it's located on. We also have our stream layer uh and this is in a EZRI file geo database. Our GIS shop is EZRI shop and we have to work with that. But we with uh ogrdw it's very easy to point to a file geo database. We can even do a read write with it if we want to. So now we have identified the code stream and a GIS coordinates for it. Do the same trick. We uh locate uh import that as a foreign table. U again very easy to do. It handles open file geo databases very well. We put all the information in there and then we have the streams layer. So now we have the streams layer and our data nodes or swim nodes both in post as a foreign data table. Well, the first thing we want to do is we want to snap the XY location to the nearest location across the stream. So we want to put it right on the nose of where it is. So, we select the node, we put it in the correct um make a point out of the correct set shred, do it as a shape from the the nodes, then we snap to the closest point in the stream layer. Uh so, we get a a point. Then we get from that point we pull the xycoordinates back out of it and uh grab the ones we transform snap to it from the uh ones that had a stream notation. The ones that didn't have the stream notation just return the same ones that you had before. So we get uh back again the same ids the xy coordinates if they're not in the stream snap to a stream we get the same ones that we did before because we used the left join. If we do point to a particular stream location, then we get the updated stream locations. At this point in PG admin, just copy paste and throw it back into Excel. So now we have those at the locations of the various streams. We have them back in uh Excel which we can then put back into the swim input file. Now the other thing is the conduits. These are the locations. They have a from node and a to node. So we have the name of the link. We have the from node, the two node, and uh again it's the same thing. We do a copy paste, put this into a u another tab into that same Excel spreadsheet, and you have the ID, you have a from node, a two node, and then a stream. If this conduit is along a stream, we tell it which stream to snap it to. Again, a little bit of difficulty, tedious type thing, but this is uh all worth it when we put it together. So now let's actually do some linear referencing. Well, we get the uh links again from a uh foreign table from OGRFW, FDW. And then here's a multi-step process. Don't worry if you it's hard to follow. I'll have I'll be freely giving the code away. But first, you get the from nodes. You find you find the node you're talking about. get the x and y coordinates from that other table and then tie it to the stream. You have the point start and the point end. So you get those two from the xy coordinates. Then the points xy uh I do CTE because it makes it much easier to follow. So you make a point for each one of those. So now you have a starting point and an ending point. Then you do what I call uh points m or this is where you're actually doing what's called a line locate point. So take a stream at a point find the percentage of where it falls on the line. So you have a starting point in percentage and ending point in percentage. Now you have the stream segment and a starting and ending location along that stream. So we do what's called a line substring. So we take the shape of the line the from measure to measure or location along the line and that gives you a substring along the line from those two points from the various nodes. So we have everything we need for the link. I throw in here as you see this ST simplify preserve topology. If your stream segment has a lot of points and you don't really need them all for a visual, you can do a simplify. uh this one with zero it handles all of them but in my case I'll just say uh you can cut that down if you have too many points then you have a shape from the substrings now at this point you have a shape and some swim things can use that as a shape it can convert it to a shape file but I'm going to go directly into the import file and I'm going to explode those substrings and we'll select that and we get all the XY coordinates for every one of the ID IDs. So we've dumped the points. We have all the XY coordinates along the stream segment and we have the ID going according to that. And at this point we have it in exactly the same format that we need for the vertices in the um swim input. So here's the text file. We dump it uh back into there. And now we save that file for the swim run. Now our model has all these geo reference and it's snapped to the stream segment. So on the left is the conceptual before we had it. This is just the straight segments between the nodes. Uh you can kind of see you kind of get a good idea. And that on the right it's actually snapped to our official stream loader. Um very easy to do. Once you're done with that, you just save the a copy of that Excel spreadsheet and if you ever have to go back to it, for instance, you're updating the model or you're updating the stream segment, you just simply go back and rerun it with that uh with that information. So it becomes visually much more intuitive, much clearer. Here's another example of if you look over on the left, I don't know if you can see my mouse, but where is this one going? Is it sliding down to the left? Is it all right? Is it up to the top? Whereas in the stream segment, once we georreerenced it, it it's clear where it's going and where the various information is. At that point, we can visually see what's going on. We know where it is. It's very accurate. And uh that point becomes pretty easy. Um this is why you want to automate it and [clears throat] do it with post. Uh it's great because we have all these models. Some of them have been georreerenced. Some of them have not. In the upper leftand corner you can see one that's very very uh rough and conceptual as a schematic. Others toward the right we have a much more detailed model that we can take a look at. From that we can easily put this in a web application. So we can say okay today we are forecasting at this location a flow of 840ome cfs then we can take a look at how much that happens when it was when it's forecast when we ran it uh everything that we can just do a pop-up menu from that because we have that all those attributes in gojson from this uh accurate depiction of the stream. Um and the fun thing is no postcrist tables were harmed during this application. No postcrist table were used in this. Everything's done. We pull it from Excel and we pull it from an S3 geo database through ogrdw and just do the queries against those foreign tables. Get the results back in PG admin or any other result and put it back into the u input files that we take a look at. Uh if you want to find out more about ge linear referencing there's the great all this information and all the examples are used it's a great little link to the uh introduction to postgs uh and it's on the workshops still there still a great resource to do it so that was our mission this is our mission statement and we use uh all sorts of GIS data in including postgs uh to do a much clearer more uh easy to understand location of where we're talking about for our various models and our various flood threats. So with that I'll take any pre any questions uh and go from there. Thank you very much for your time and your uh allowing me to present on this. Well, thanks for coming Bruce. That was a beautiful uh beautiful little talk. I got uh I assume that makes um the ctography a lot more pleasant and the opportunity to make uh prettier UIs a lot more >> practical. People really get bent out of shape when the lines don't line up. [laughter] Is there any reason you uh chose to go through Excel or >> It was just easy because uh with the the text delimited um t space elimmited text file it just copy and paste went in there so much easier and then the other thing was as we found out to copy back into Excel save uh then we copy from that going into the um uh text delimited files sometime PG admin or sometimes supposed just puts the double quotes around text fields and that was a problem for us in the admin. So, it just it's just a simple easy way to do it. >> Yeah. Awesome. Um I [clears throat] don't see any other questions and I don't believe I have a question. >> Okay. Well, >> thank you very much again. >> Thank you. Appreciate it. "}
{"url": "https://www.youtube.com/watch?v=CHJyVKJrigs", "title": "Building Agentic Experiences With PostGIS And CARTO In Snowflake", "transcript": "Hi, my name is Ryan Miller. I'm a solutions engineer with Cardo and excited to talk to you today about uh building agentic experiences going from natural language to geospatial re reasoning uh with postgis and snowflake uh you using the Cardo platform. As I mentioned, I'm a solutions engineer with Cardo. Been with Cardo uh for a little bit uh more than a year now. uh spent my entire career in GIS working for agencies like the Air Force uh Department of Energy at a solar startup um and excited to to chat with you today about this topic. So, as we go through uh kind of this this particular topic, I'm going to kind of use a a a use case um that basically takes us from kind of the business challenge, which is really very simple question uh to kind of a complex answer. and how that complex answer uh translates into kind of the solution that I've developed for this particular talk which is leveraging our agentic stack uh which incorporates uh snowflake uh postgis uh which are both accessed through cartto I'll briefly talk on the kind of the data processing capability that that I went through to kind of prepare the data sets uh and then spend a little bit of time on the steps to to kind of enable the agent or configure the agent so that it can be used for end users to kind of interact with these different data sources uh and ask the different types of of questions uh that that we'll see and then uh conclude with a demonstration kind of show you what this looks like within Cardo uh the types of questions and responses that that the agent is able to to uh provide and kind of show you kind of a hands-on working uh live demo of this capability. All right. So, the simple uh business question that we're going to kind of tackle today is is um somebody comes to you and says, \"Hey, I want to be able to find uh vacant land parcels in California that have between 10 and 20 acres of buildable area uh that are within a quarter mile of a trans uh transmission line. So, this could be for um a data center project um or a renewable energy project. again kind of the the purpose of this question is kind of provide an example of the types of of queries that uh questions that may get asked and then ultimately the types of queries that these these would transform into. So if we kind of break this down a little bit like what does this mean um technically? So we start start thinking about okay vacant land parcels in California well California has about 12 a.5 million land parcels which uh consist of of different land uses different land ownerships. Um so we've got a a large parcel data set that we've got to deal with uh from a processing standpoint. Uh the component about well but we want a buildable area. Uh well, what does that mean? Well, we've got topographic constraints. Uh we've got hydraologic constraints. So, uh think of we can't build too close to a river. We can't obviously there's wetland challenges, riparian areas, uh that all need to be factored into buildable area. And in this particular case, uh we want to be able to kind of do cross data queries. So, parcels within a particular distance of an of a feature. So could be a transmission line. Uh it could be really any sort of geometry that we want to bring into our analysis. Uh and then in this particular example, I've got kind of two different uh databases or data warehouses that we're leveraging. Uh one is Snowflake, which allows us to kind of process the data at scale. We're executing these these queries against these um large data sets. And then we've got kind of a more of a transactional postgis postgress instance where we're taking our selected set of parcels. we're saving them into a a Postgress instance where maybe we have some downstream uh operations where uh whether the commercial real estate uh organization wants to take those or the engineering um organization wants to take those those subset of records and do uh additional analysis to kind of evaluate them incorporate them and in their kind of uh processes. So kind of we're focused on this this middle ground of of se using an agent to help kind of filter out these um these uh different uh these parcels based on on these constraints that that we've talked about here. So we kind of break this down. There's kind of four different steps we'll be going through today. Uh first one is is on the data processing side. Uh so I've set up a processing pipeline utilizing Cardo workflows uh which basically uses uh a combination of the Snowflake functionality. So out of the box um Snowflake has a uh geography data type which uh we leverage uh and then they have um kind of a core set of spatial operations and spatial functions. And then Cardo brings their analytics toolbox components which kind of takes those those core sets of components uh and kind of rounds out the toolbox of features to allow you to have a a more robust set of capabilities to allow you to have um kind of a a full set of of spatial operators. Talk a little bit about um the infrastructure or the Cardo instance that I created. So basically it's a a builder map uh that allows is connected to both a snowflake data warehouse and a Postgress postgis instance kind of allow you to have this uh dynamic interaction between uh both data resources and and allows you to kind of move subsets of records between them. In the types of of questions that we're going to answer, we've got to have multiple layers. we don't know kind of the distances or the different types of of questions that the um end user is going to want to uh ask. So whether it's a distance from transmission lines, uh power lines, really any sort of feature that we've added to the map. Um we've got those in this particular examples, but we want to be sure that those tables are are optimized so that you know spatial queries can perform uh efficiently. I talk quite a bit about kind of the agent builder process. So how do we build the agent which is embedded in our builder application, our map-based application and we need to educate the agent uh on the types of of interactions and give it instructions on how to perform kind of some of the core sets of of use cases uh that we're going to enable through through Cartto. All right. So, a little bit on um the processing pipeline. This is a a schematic of our our workflow environment that kind of allows you to build out these uh geospatial data processing um activities. So, we've got kind of our data sources on the left. So, those would be things like as we mentioned parcels, topography, um hydrography, uh streamlines, riparian areas, etc. Um, and we're doing different sorts of operations like buffers, uh, filtering out maybe based on attributes. Uh, and then we're basically, uh, differencing them. So, we've got our parcels, we're subtracting out, um, those those streamed buffers, the areas that exceed certain slopes, etc., and sending it saving a end result data set that kind of has these uh, constraints removed. That would be our our buildable uh, areas in this particular example. Uh this process all operates on on snowflake. Uh it's fairly efficient. A complex set of operations like this on 12 million parcels with a extra-large snowflake data set uh take takes about um about 45 minutes. So you relatively speaking uh that's pretty pretty efficient, pretty quick. Allows you to execute these these large scale um geospatial capabilities very efficiently. All right. So we start thinking about kind of the uh infrastructure in this particular example. So as I mentioned we've got kind of two different uh data resources. Postgress where this stores this acts in this particular demo as a persistent system of record for production. And so we take those those saved results and maybe there's some downstream processes that leverage those those parcels for other activities. Um so it really kind of stores what the agent finds uh making them available for other types of applications. And then on the snowflake side, uh this is where we're really kind of processing the data at scale utilizing kind of some standard geospatial functions um within uh area uh where these are operating via the queries generated by the agent where it will filter the records based on um both the attributes that are are part of that the source data or looking across uh different layers to um kind of do those kind of spatial joint spatial queries. Um and these are both accessed through Carlo's builder map uh which basically provides you kind of one interface to multiple data sources to kind of implement uh and execute these types of operations um kind of through a a single single uh agent agent interface. So the types of examples in this is of types of layers again that we're going to show here. There's really kind of the we've got our source data which is the parcel layers like as I mentioned about 12 and a half million parcels but then we start to bring in these other um features and and layers that we want the uh agent to be able to reference. So, uh, in the particular question that, uh, we started with, well, we want we're going to need where those electrical lines are. Uh, and then we're going to have some sort of distance. Obviously, these could be any type of of geometries that we're would want to bring into the analysis, but basically allows you to perform these kind of cross uh, table um, queries that allow you to kind of filter out records based on on the different uh, geometries. So now spend a little bit of time talking about kind of the agent um saying kind of which data source they they operate against. So we think about things like the the distance or these kind of crossstabular um execution. So those will be performed in in snowflake. So I have both the parcel data sets and then the other geometries whether they're the lines from the transmission lines, the points from the um power uh generation facilities substations. U so we allow you to kind of do those types of of queries across table. Similarly with straight attribute queries where we're executing a wear clause or the agents executing a wear clause based on on the particular properties of those um polygons and filtering out those kind of on the fly. And then we talk about the the saving of the parcels. So we found a subset of of parcels and we want to keep those and move those over to a a permanent set of records. uh and that's where we're basically quering from Snowflake and moving those over to Postgress aware we can do uh different types of analyses and then once we have those in Postgress the agent can um interact with those um parcel polygons inside of Postgress give me a summary give me details about individual parcels show those on the map all of those capabilities can can happen um inside of of builder again so the end user user is leveraging kind of both of these uh environments seamlessly. Um I actually added some context to let the end user know what's going on in what data warehouse for demo purposes but really kind of leveraging the the capability of both systems uh efficiently and and uh uh kind of behind the scenes or invisible to to the end user. So when we think about uh how do we develop an agent um it's kind of like prompt engineering u using a map. Uh so it's got a number of different components. First one is kind of the agent configuration, right? So these are are the types of um experiences, steps, uh boundaries that you uh apply that tell the agent basically, you know, here uh is what we want you to do uh or importantly what we want don't want you to do, how we want you to do it, uh etc. also have the ability to bring in tools and I'll talk a little bit about that in the next couple of slides and kind of these are the the capabilities that the agent has access to um to do different types of of of operations and those could be both a spatial or or non-spatial operations. Obviously we've got a model that needs to be uh accessed. So this is where you'd say which model do we want uh want to leverage through this process and then um the types of prompts to get starters. could maybe give the end user a little bit of background. Uh here's a good place to start. Um kind of how do you get going with the the interaction with the the end user? And we'll see that here uh in a second. Kind of drilling down a little bit kind of the the tools that um the agent has access to kind of fall into two primary categories. Um first one would be u kind of data processing tools. So uh the agent has access to workflows uh and queries to the connected data sources. So as you're configuring the agent, you're adding the data sources, you're telling the uh agent, okay, you can query these uh data sources or not. Um and it can do uh powerful tasks like data enrichment, uh statistical aggregation, uh those types of activities. And then we think about like user interaction tools. So this is how you uh collect input from the user. So that could be um drawing a a mask on on the map to say focus on this particular area or use kind of contextual information like um the show me use the the um save the parcels that I'm looking at which would then basically um the agent would be able to to reconcile that description and say okay I'm going to use the the area viewport or the area of of the map map extent to process and save those those an answers. So really kind of tying kind of the contextual information and the geospatial reasoning that the agent is able to to perform to kind of give the end user the experience that they're they're looking for. Talk a little bit about um server side tools and um MCP tools. So uh you can define kind of these geospatial processes that uh the enduser or or the developer defines. So you're uh ensuring that the agent has access to kind of consistent cons controlled uh processes and outputs uh for the information that that the uh agent is going to communicate uh to the end user. So particular examples around site selection uh data enrichment uh risk assessment again leveraging these these workflows exposing them as MCP tools and then access um by the agent. The agent can also uh dynamically uh generate SQL uh using these connected data sources. So, uh, this is what we'll see today. Kind of on the-ly filtering, um, giving me summaries, uh, computing averages, applying different types of spatial filters, a lot of capabilities here that the agent has the cap, uh, ability to, to formulate these queries, execute these queries, um, and kind of give the user that that experience um, of doing some sort of spatial analysis with not really understanding a lot of the complexities behind the scene. briefly uh MCP tools as workflows. So uh workflows what you're looking at in the background there, these are components allow to kind of string together processes. You build out these processes as kind of you define. So this is exactly the series of steps that you want this process to implement. You publish these workflows as uh MCP tools. Then over on your agent you say okay when you're going to do this particular activity you need to use this particular workflow. So then the agent has uh passes the required information to the workflow. The workflow is executed. It generates a a set of results which then go back to the um the agent and the agent would return the series of of answers or data driven directly from the process. That pretty powerful capability um that allows a little bit more control over how the agent generates the the answer that it ultimately telling uh the end users. um these tools that the agent has access to uh kind of um allow you to capture inputs. So they're kind of capture the inputs from the users and then present the outputs um back to the user. So collect inputs as we talked about whether it's a a region on mask uh capturing pass in coordinates pass uh an address and um does some sort of geocoding. But then once it does the process as you'll see here in a few moments kind of takes those those outputs uh adds those back to the map and styles them uh so that you get kind of a subset of of results and then that you can use for kind of further further processing. Um yeah so a lot of capability and interactivity that you have. You can the agent says here's the results and it's in a color that you don't like. you can say, okay, can you uh recolor those to instead of maybe blue, I want those as yellow. Uh the agent has the capability to kind of interact with the data that's that's rendered on the map and a lot of a lot of flexibility there. All right, so now the fun part. Let's uh let's jump into a brief demonstration of kind of what I've built and um kind of show you kind of how this this looks. All right. So, let me stop presenting that. All right. All right. What we're looking at here is our builder application. Uh builder allows you to take uh data from the data warehouse and and visualize it on the map. So, in this particular case, we've got data uh that we're looking at from both Snowflake and Postgress um and presenting those data spatially. Uh but then also on the right hand side we have uh different types of widgets that you can uh add to allow you to filter those those attributes of those data and kind of have an interactive uh dashboard. All right. So the first thing I want to do is kind of show you kind of that constraints activity or constraints process that we implemented. So let me kind of uh zoom in here. Let me make a little bit more space. Uh add in our uh constraints for our slope layer. Uh let me look right along the fringe here. Uh and then add in our hydrography data sources and then our uh suitable uh parcels data sources. So as I zoom in here, uh these layers will will start to appear and basically allow us to kind of see kind of the interaction between these uh different um data sets. So we've got as I mentioned our our slope constraints, we've got our hydrography constraints, which are wetlands. um distance from rivers and riparian areas. Uh and then we have our our parcels uh constraints. And so if we kind of look at how this these three uh sources interact um you can see our our blue areas are hydrography constraints and then we've um kind of removed those nonbuildable areas out of our our parcel. So basically after running that process that we went through um we're only left with the portions of the the the parcels that are buildable. So that's kind of the starting point uh that we're using in our uh agentic uh sets of activities. All right. So let me kind of zoom out kind of restart reset uh the map um and then we'll start our our agent uh interactions. Um all right. So if we uh this is basically how you access the agent. We'll start with the basic question of uh what can you do? Um and so basically uh using the instructions that I provided to the agent uh it will tell us the types of activities that it can do. Uh so it can find parcels based on different sets of criteria. Uh and then also it can save parcels. So once we found the parcels that we're interested in uh we want to save those uh in this particular case to postgress uh it can do that as well. So I'm going to follow up with the question that we had in um our presentation. So basically the land parcels in California that have a buildable area between 10 and 20 acres that are within a a quarter mile of a transmission line. And so this is going to formulate and execute um the query. But before it's going to ask if I want to uh further filter based on the market value. And I'll just say um include all parcels. Um so it's going to formulate the query uh using the parcel data set uh and then that quarter mile distance of um these transmission lines over uh the state of California. um take those results uh add those uh results to the map and then style them and zoom the map to that particular uh location. Um all right, so it found uh 2,250 parcels that match this criteria. And so up over here on the right, we should start to see um a new layer up here. And you can see where it will style it. Uh so yep, here's our vacant land parcels. Um it added it. So now it's going to style it. it colored them red. Um, and then it will zoom the map to kind of the the maximum extent uh of the the the the data set. And now it should give me a result based on on the market value. This will kind of give you a short summary along with adding those records onto the map. All right. Well, I want to further kind of drill into this um set of data to identify just those partials that are in San Bernardino. Um, so let me ask it. Uh, do you have any parcels in Sanber Nino? And so basically what it's doing, it's taking these 2,000 plus parcels uh, executing an attribute filter. aware the parcel has an address uh that contains um San Bernardino. uh and hopefully it will find a set of records that uh it will then add to the map style and uh zoom into um all right so it says yes I found 17 parcels um and so basically it red removed the previous uh set of of results and now it's adding just these 17 parcels uh to the map um based on on those parcels that are in inside of San Bernardino. So you can see up here on the right it's saying okay we found vacant land parcels in San Bernardino. Uh it's adding a zooming to the map. Uh zooming in. Let me turn off our um our slope constraints layer. Um and uh basically said okay here's the top five um parcels based off of different types of of criteria. All right. So we can see a few let me turn this off a little bit easier to see few different sets of parcels around. There should be 17. Let me focus on this kind of cluster of three parcels here. Um so basically we've got um different tool tips that you can tell the agent to add um the different types of interactions we want. Uh and so now we've got these parcels and we want to save them uh to our Postgress database. Again, everything so far has been uh processed inside of of Snowflake. So before I I ask the agent, let me just make sure that my parcels table is empty. It is. All right. So let me say um uh can you save the parcels I'm looking at on the map? And the way I've set up the agent is to Okay, before we save, let's add some sort of uh note or context about these particular uh locations um and uh follow up with these owners. All right. So basically what the agent is going to do at this point it's going to say okay um it's actually not uh 17 but uh it should just be the three. Um, so basically what it's going to do is take the the map extent um and do a a filter based off of uh the map extent and say okay let me get this subset of records and um query the attributes from uh snowflake and then move those records into uh into um the postgress instance using snowflake. actually have identified the three that are visible on the map. Uh now each of them will be selected um and then saved. It just take a minute. So it's basically going to iterate and say okay so I've successfully saved this parcel uh and then the other ones as well. Um it it can say do we want to add those back? Uh let me go and just confirm that that's exactly what it did. Yep. Here's our our three different um parcels that it added. Um let me ask it uh uh questions about the say parcels now. So this will be executing queries directly against Postgress. Um how many parcels do we have saved? And so it should tell me that we have three saved. Um and so again you can kind of see where you've got these different uh data sources in play um where you can kind of share them. uh um share the agent can have access to both a uh data resources uh for its particular uh use cases. Uh so one last question can you show me these? And so basically this will query the uh Postgress database add those um parcels uh to the map style them just like it did before. So this would be just those three that we have uh saved inside of Postgress. So hopefully this gives you kind of an understanding of what this looks like. Uh slides gave you a little bit of background along with um kind of the live demo. happy to take any sorts of questions you have about the way that I've I've set this up. Thank you. "}
{"url": "https://www.youtube.com/watch?v=-1M-ge2pLzg", "title": "QGIS Cartography Tips And Tricks", "transcript": "like to welcome our next uh our next guest, our next presenter, Michelle Tobias. Hi Michelle. >> Hey everybody. >> Hey. Um yeah, I last ran into Michelle at Phosph North America where she was teaching people uh how to make maps with CQIS, how to make maps with pens. Um she's a geospatial data specialist at UC Davis, a ctography expert and an author focused on map design in Python. And uh feel free to take over the um take over the screen. She's here to tell us uh tell us about how to use uh how to use QIS to make better maps, cgraphy tips and tricks. [snorts] >> Awesome. Hey everybody. Um, I'm super excited to talk to you about this. Um, it's one of the things that I absolutely love and was something that I was actually working on this morning with some ctography QGIS. Um, so, um, you might be thinking like, hey Michelle, this is PostJS day. Why are we talking about QGIS? Um, and the answer is because you can connect PostJS databases to QGIS and it's a great way to visualize your data. So, um I'm just going to get into uh some of the things that I think about when I'm making maps and also things that I teach folks who come to my workshops and you know ask me how do I fix this map? Um so these are some of the key concepts that I think are really important. So I think it's really important to tell the story. So before I do anything uh in terms of cgraphy and making a map, I really think about like what do I want my readers to learn from this map when they look at it? You know, what should they be getting from this? What claims am I trying to support with the map that I'm making? Um and in general, like what's the story that I'm telling? Uh another really important thing is to minimize. We are only going to keep the things that are absolutely necessary and everything else we're going to get rid of. And finally, focus on communication. Does my map communicate well? Am I telling am I telling the story that I want to tell? Um, and am I communicating the things that are important? So, let's dive into these these three things separately. So, telling the story, first up, we really need to define what story are we trying to tell? Um, why are you making this map in the first place? We're not just throwing layers, you know, onto um a map canvas and saying there we're done. We're really trying to make sure that we understand why are we making this map and what should the reader learn from it? So before I do anything else, I'm answering these questions. So the example map you can see on the screen um is a map that I made for a professor at UC Davis to go into a book about um political actions that were happening around cinemas in Paris in the early part of the last century. And so um the story that we're telling with this map and uh the story that the um professor asked me to tell was how can you get from one cinema location to the other using the metro line. I guess the the story behind this is that there was political actions happening at these two cinemas and the assumption was that you couldn't have um the same person couldn't have caused both um events to happen uh the way that people were claiming because it was too close together in time. You couldn't get across Paris to get between these two sites that quickly if you're using the road network. But the answer is you can do that if you take the metro line. So that's why we are making this map and that's the story we're trying to tell is like you know you can do this on the metro and it's actually pretty quick. So then we need to once we figure out what our story is we have to figure out what data do we need to tell that story. So I will sometimes just actually make a list of everything that needs to go into the map. Um I'm going to think about what's my focus data, what's my background data, um and do I need to do any data processing. Uh so for example in in this example map the focus data is the cinema locations uh the metro lines and the metro stops. The background data is the streets like that's less important here but we still need that for people to be able to understand this map and sort of orient themselves and kind of get a sense of scale as well. Scale bars are great but most people like they're not that intuitive. Um so so those are the the data sets that we need. I don't need other data sets here for reference. I just need these four. Um, but also with this map, we needed to do some data processing. I had the cinema locations were provided as addresses. So, we needed to do some geocoding to to put those addresses on a map. And then because these were addresses from like the early part of last century, we also needed to check really closely for accuracy. Um, because some of the street names had changed. So, um, there's a lot more than two addresses in this data set. Um, but there's only two on this map. So, there was some that needed to be done by hand. Okay. So, the second concept is to minimize. So, less is definitely more when you're making good maps. Um, and we're going to focus on two aspects of that today, which is our fonts and our map elements. So, starting with the fonts, we want to maximize readability. And the way we're going to do that is we're only going to use one font, maybe two at most. So don't don't go wild with the fonts. You're going to really really try to use just one. Um you can use different font variants like bold and itallic white, things like that. Um and you can see some examples of um the different variants of Proxima Nova. That's just the font I was using in this presentation. Um, but you can kind of get an example of all the different types of variants you can have with just one font. So, you don't need two fonts because you have all of these variants of of the one you're choosing. Um, we're also going to tend to pick a sans serif font um because they are v visually nice and simple. Um, so like you can see the font I'm using here in this presentation is sand sif. It's very clean lines. If it was a serif font, it would have like think about like Times New Roman as a classic Sarif font. It has little like wings or little flicks at the ends of like uh the letters like if you think of like the capital T in um Times New Roman, it has little swooshes on it. Um and that just adds more visually. So it's better in most cases to choose a sans Sarah font. Um not that there isn't place a place to use Sarah fonts or novelty fonts, but um mostly we're going to choose sans saf fonts right now uh to make it nice and clean. The other thing is font size. So tempting to use teenytiny fonts because then we can fit more onto our map, but it's really hard to read a font that's smaller than eight point. So don't do that. I I honestly try to keep my fonts 10point or higher if I can. Um because otherwise if I start at eight, I'm going to want to go down to six and that's just not going to be readable. Um, some things with QGIS specifically that are helpful to know is when you're doing working with text, you have a lot of options for how to style that text, um, that you may not have thought about before. Um, and these kind of these tips are going to give you some other ways to make your fonts look different even though you're going to only have one font in your map. Um, so when you're for example doing labels uh in the um the layer styling panel, you can uh label your your uh maps, you know, the way that you normally would, but uh there's some kind of I feel like hidden features in here that are easy to miss. Um so you've probably seen this text tab uh where you change the the name of the font and then you can change the font variants like to bold or itallic or things like that. Um, that one's I think pretty pretty well known. Um, but some other ones like the um formatting tab, the second one um from the left, I use a lot of the features here. So, letter spacing is one that I really like. Um, I will space my letters out for things like state names or country names. So, it's the same font, but I'm changing the amount of space between individual letters. And that just kind of gives it a different feel. It takes up a little more space. Um, it just looks nice for for big names like that. [snorts] Uh, the other thing you can change is word spacing. So, essentially this parameter changes how wide your space bar character is. So, if you hit space bar, it's going to make a wider um a wider space than it normally would if you change this and make it bigger. Or you can make it smaller if you want the words to be closer together. Um, so two different things, letter spacing and then word spacing. Another one that I really like is this wrap on character uh parameter. So a lot of times I've got long labels and I need to break them up. So you can either like put in you know a character that you know is is kind of nonsense. Like sometimes I'll put in like three asterisks or in this case um in the example I'm showing I'm wrapping on the character um comma and so it it actually replaces the comma with an end of line character. So it wraps the the text and that can be really helpful if you want to do things automatically and um not have to do a lot of editing of your attribute table. And then the final thing that I like to um change in this particular interface is the line height. So in the example here I'm showing I'm making the line height larger. So I'm increasing the space between the lines. But a lot of times I actually make it smaller so that I can visually group the the lines that are for the same label. I feel like sometimes the default with QGIS is that it's a little bit taller than I want and visually it might look like two separate labels. So I'll decrease the line height and then they kind of smoosh together a little bit and they look like they're one thing rather than two separate labels. Um and then finally um in this uh labeling uh tool uh the rendering tab is really important because it has the parameter for what to do about overlapping labels. And my personal preference is to tell it to overlap without penalty. I wanted to pile up all the labels and put them there regardless of if they sit on top of each other because I want to hand place them. Um, a lot of times I don't like the automatic labeling for a map that's, you know, professional. If, you know, if it's just like a a quick reference map, I I don't care uh that much. But if this is going in like for example a book chapter, it's it's got to be right because it's going to be there forever. So I want it to label everything and then I hand place the labels. There's some really nice tools in QGIS for moving labels around. Um but I will say if you have to do a lot of label moving, get a gaming mouse. It's going to help with precision. It's going to make a lot less work on your wrist. So um yeah, highly recommend the gaming mouses for cgraphy. Um, so the second part of simplifying things is um, map elements. Map elements to me feel like clutter. So I know like when I used to teach intro to GIS, we'd always say like, you know, you always have to have a title, you have to have a north arrow, you have to have all these things. I'm going to tell you now, you don't need all that stuff most of the time. So we're only going to keep what we absolutely need in our maps. Um, you know, and figure out, does the reader get this information somewhere else? Uh, and consider the context of the map itself. Like is there more information in the text of like if this map is going inside of a report or inside of a book chapter for example, does it get context from that text that someone's going to be reading and then referring to the map? Um, so in the example map that I have here, you'll notice that this is a map, but it doesn't have a scale bar. Uh this is a map of Arizona, but there is no Arizona outline here. There is no title. There is no north arrow. And the reason for that is that this appeared in a book chapter that was about um Arizona wines. So there were other Arizona maps in this chapter. And so people could reference those to understand the relationship of uh the spaces that we're talking about. Um, and they've seen maps where the the American viticultural areas, the AVAs that are represented here, they show up on other maps. So, they have a a sense of the spacing here. So, we took out all the extras. And partially that was so that we could zoom in more on this and give more detail because it's a really complicated map. There's a lot going on here. It didn't need more stuff. Um, so we do have a legend because otherwise you would never know what all of this stuff is. Um, but we took out all of the things that were not necessary to tell the story because because it needed to get a because it's a cluttered map. It needed to get simplified and also it had that context uh of the text that goes along with it. So we could take things out. Um, and this the point of this map is to show that um the places where the wine is being produced, the AVAs is different from where it is being consumed. So the lines connect the um businesses that sell the wines to the places that are making the wines. Um which you would know if if you had actually access to the book chapter to read it. Um and so finally um we're going to talk about communication. Every map element, everything you put in your map needs to communicate precisely. Um so we're going to talk about two things, visual hierarchy and background data. So visual hierarchy um is kind of the way our brains work. It's helps us understand how we notice things um and how we don't notice other things. So the first thing I like to talk about is contrast. So things items with high contrast compared to the items around them will convey convey more visual weight and also importance. So, if you look at the example image that I have on the screen, you will probably quickly gravitate to one or the other of the lines within each of the boxes. So, probably on the dark background, your eye is drawn to the lighter line, and on the white background, your eye is drawn to the darker line. And that's because they have more contrast, and so they have more visual weight. You're going to notice them first. Our brains are wired this way. So, we're going to work with that. We're not going to work against it. Um, and sort of a secret uh thing that's going on with the these images is that the lines here are the exact same colors between the two sets, but they look very different because of the um the amount of contrast and the background uh color behind them really changes the way we we perceive those those shades of gray. So again, high contrast means in our brains more attention, more focus, more importance when it comes to a map. [snorts] So, we're going to work with that. The other thing, another thing about visual hierarchy is size. You know this, like you notice the big stuff first. Like the text on the screen in the sort of right side panel, you read that sentence completely inside out, I'm sure, because big text jumped out at you. Um, so larger things again have more visual weight. They convey more importance. Um, this just the way our brains work. So, again, we're going to work with that. Um, so you'll notice, you know, if there's wider lines or bigger circles, you're going to you're going to notice that first. So when you make a map, work with that. Uh, so that, you know, you make the big things the important things, and then your reader notices that first. Um, the final piece of visual hierarchy I'm going to talk about is color saturation. So again, saturated lines uh or saturated colors, ones that have more intense, more vivid colors as compared to like pastel colors are going to give more visual weight and and feel more important. So um the richer the more intense the color, the more attention we're going to give it and the more it's going to uh feel more important. So we want to use saturated color sparingly. Um, and I know when you start up like QJIS, it just picks random colors and often they are all very saturated and that feels very overwhelming. So, the first thing you can do is mute down your colors and it'll be a little bit easier to to work with. Um, and the concept of of color saturation is is related to contrast in terms of visual hierarchy. Sort of the the more contrast, the more saturated something is, the more attention it's going to get. Um, and then one of my pet peeves is background data. Um, sometimes people talk about this in terms of like base maps, but I think that term gets used to mean a lot of different things, so it's not super precise. Um, but whatever data you're using in the background of your map. Um, I want to remind you to check your license information. So even like tiles and things like that that you can just plunk in your map. Um like quick map services and QGIS is great because you can get a really quick background map, great reference information, but check the the license on that data. You may or may not be able to use it for your purpose, especially if it's a commercial purpose. So check that before you just decide to use any any base map or any tile service. Now for static maps, most of the time I am going to grab my own vector data and style it um and work with that rather than tiles. Um I think a lot of pre-made tile services and also air photos contain way too much information for most of the stories that I'm trying to tell with my maps. So I'm going to grab my own data like OSM data is great um and style it the way I want it, filter it to only the things that I want to see. So, I actually spend a lot of time on my background data that really is not going to get noticed very much, but um really tailoring it to what you need is helpful. And the same thing with interactive maps where you probably do need to use a tile service as as your background um you know base map kind of data, but make sure you're choosing a a tile service that is uh conveying what you want. So, there are some really nice simple ones out there that don't have a lot of information. um versus like like I always think of like dropping like a Google maps layer in there where you know like when I'm making a map of Paris do I really in you know the midentury of mid part of the last century do I really need to know where all the modern McDonald's are? Probably not. So you want to think about like what is is that background data giving you too much information. Um, and finally, I wanted to go through a couple of examples of maps that I've made in in QGIS. And then I think the last one, um, I'm going to show you. I did some work in Inkscape to to put some things together, but um, just to show you some, um, examples of work that I've done and show that you can make some pretty pretty decent maps in Q.J.Sis. I I do most of my cgraphy work um, in QGIS. So, this is an example of a map. If you've done my ctography workshop, you've made something similar to this. Um, this is lake monsters according to Wikipedia zoomed into the the Great Lakes area. Um, and you can really see how visual hierarchy is working here. We've got we're working with contrast. um where the important things, the lake monster names are black um and they're bold and they stand out and then the other information like the lakes which are important for this um and the state names which are less important kind of have um a gray color to them. They are more background information so they have less contrast. We're using a lighter shade of gray gray for those. Um here's an example using color and how saturation can be really helpful for conveying information. And so these are um the potential places that the Central Pacific Railroad could have been placed um kind of in the Tahoe area in California. Um and so the background data here, the topo lines, the watershed boundaries, the rivers, the lakes, those are all kinds of shades of gray or kind of bluish gray. Um but the rail lines are really saturated colors. They stand out. They're bold. U because that's the point of the map. We need that other information to understand the context of the rail lines, but the point is the rail lines. So those are the ones that we use with the saturated colors. Um here is another map from that set from the Paris book. This is the map that was going to go in the frontest piece. So like right when you open the book um for reference um so you could see all of the different um cinemas and where they were in Paris. Uh so you can use that as a reference as you're reading the book and just figure out like you know any given location is is clearly placed on this map for reference. And then finally this um this map is um from the publication about my QGIS plug-in literature mapper explaining how it works um taking some of the data that came out of that tool and turning it into not only a map but also um a graph of sorts to understand uh how latitude relates to publications. And again, you can see the it's a grayscale map, but we're using differences in in contrast, how much how dark things are. Um, helps separate out the information so you can see it clearly on top of a you know, California coast is pretty complicated. So, um, having the points be dark helps helps make those stand out. [clears throat] And then, um, you know, changing the, um, the point color based on the year also. So, that is what I have. I think we've got time for questions and I'm I'm happy to do that. >> Lots of time for questions. Um questions in the uh in the chat. Um Stefan Keller asks, \"Is there an open source alternative web app framework to EZRE's story maps?\" >> Oh, um yeah, there's lots of them. Um I like Night Labs. It's pretty simple, but um that that one's one of my favorites. I I don't really build a lot of story maps in my work. So, if there's other other things folks can recommend that are are listening today, pop them in the chat. Um, yeah. >> Um, I was wondering I was really taken by the example maps you just showed whether you think like whether you feel like black and white ctography is like a good learning exercise to like deliberately force people into a stylistic box they have to fight their way out of. Yeah, I mean sometimes like I I also uh dance and do choreography and sometimes there it's really apparent that if you put restrictions on what you're allowed to do, you can it actually can spark more creativity. So, like you could do grayscale, but you could also just do like, you know, decide like, oh, I'm only going to make a map in shades of green, for example. You know, um it can really help uh help you kind of dig into some of your creativity. But also, if you like I work on a lot of academic publications, so things that are going in like journal articles. And a lot of times publishing in color costs more money, so we end up doing grayscale just to keep the cost down. I was actually gonna ask you about that Paris map. Like how hard was it to resist making the sand blue? >> Oh. Um, like this one? >> Yeah. >> I don't even think we put water in this one. [laughter] >> Oh. >> Um, but yeah, it's but we I don't have a choice. Like if I was going to put water in here, I probably would make it just a really light gray so that it, >> you know, was different. But yeah, but that really wasn't um wasn't a part of the the necessary story on this. Um, so also it's this one, these maps, a lot of academic publications I'm doing are like 4 by4 in so they're really small. [laughter] >> It's like definitely like you're probably seeing it on your computer screen way bigger than it got printed. So, um, >> is that part of your >> process to like squeeze it down so it's on your screen similar to how it's going to be on the page? >> Yeah, like Q.js JS will let you um like when you're using the map composer when you set your page size there's an option to um size it to be one to one scale so it'll figure out how big it is in real life and so then you can really see like wow I need to make my fonts much bigger >> right >> cuz like zooming in you're like oh there's so much room and then when you zoom out to one to one scale it's like oh gosh I can't read this anymore. Yeah, teeny tiny. Awesome. Um, you had some pretty strong thoughts about titles, legends, and scale bars. So, uh, >> um, yeah. [laughter] >> What's your take on lead lines? >> Uh, oh, like when you're labeling, like the, >> you know, I use them sparingly. Sometimes you can't avoid them, but, um, I think also like folks should realize like I, like I said, I'm making really small maps usually because of the, um, the kinds of maps I'm being asked to make. So try to avoid the lead lines because they just add one more thing. So you know when it helps with clarity they're great but mostly I try to not do that. Um yeah and they also get tangled really easily in QGIS if you have them crossing and stuff they become really hard to read. >> All right. I don't see any more questions. Thank you so much for taking the time to to come and tell us about CUGIS and cartgraphy. Michelle. >> Awesome. Thanks. I'm glad to have been here. "}
{"url": "https://www.youtube.com/watch?v=bOFcq_qQBxo", "title": "How State Farm Uses PostGIS", "transcript": "So our next speaker is uh is here with us now. Michael Keller. Good afternoon Michael. >> Afternoon. Are you guys able to hear me? >> We can hear you. Uh Michael is a senior engineer at State Farm managing high volume enterprise spatial data infrastructures and critical risk systems and was a technical lead in building the State Farm geospatial platform as a service. Um which I assume will be a large part of what he's going to talk about. I'm really looking forward to hearing the story of Postess at State for Farm and your adventures with it. Michael, >> sounds good. Thank you. So, yep. Today I'm here to talk about specifically how State Farm uses Post.js. [clears throat] So, a little bit about myself. Um, as Paul mentioned, I'm a senior technology engineer at State Farm um within our enterprise technology department. I've been within um State Farm and then within the geospatial field as well for a little over 10 years at State Farm. I have built our um internal geospatial platform as a service and today I continue to help the team that maintains that platform as a service um that's completely powered by PostJS in the background. So that's what I'm here to talk about today. So to give you guys a little bit of insights into um what our platform is, it's called the State Farm Mapping Portal or SFMP for short. Um it's the sole geographical platform that we use at State Farm and it helps us um take the ability to take either spatial data or non-spatial data, the larger majority being nonspatial data, get that into our platform that allows us to do a multitude of different things. um whether it's responding to a disaster or catastrophe, doing things like territory design for a multitude of different departments um and then things from like an underwriting perspective, doing risk assessment and many many more different ones that have continued to evolve over the years. So, SFMP itself runs natively within AWS. Um that allows for us to have a highly scalable environment depending on time of day or specific analysis that people are looking to do to scale up and scale down infrastructure to help meet those needs faced with is we have all this great data and content within Postgres and Postgis. Um but the big question is how do we actually make that easy and available for consumers to connect to. So most of our consumers at State Farm have no coding knowledge, no SQL, nothing whatsoever, but they have a lot of geospatial questions that they want to get answers to. So how can we make technology available for them to get those questions answered? Um so today I want to cover it from those two different lenses of one how do we technically make all of this happen and then two what types of questions are we getting from our business partners and how are they able to do that within our platform. So here is a highle architecture um extremely simplified down of how everything works within AWS. So on the left side we kind of have that little computer monitor which represents either um our consumers whether they're employees at State Farm. Um it's another application within State Farm or it's some sort of non-human account connecting to our platform. So all of that is fed through a route 53 record which then gets passed on to an application load balancer within EC2 and then to a Fargate cluster which holds a multitude of different APIs that um kind of comprehend the entire platform itself. From there we have elastic cache clusters and RDS Postgress clusters with postabled to help answer that question of where um majority of the time we're hoping um those API hits go to elastic cache where we have a lot of those answers already stored and available just to get that quick response time from a better user experience perspective. But depending on what it is or if it's a one-off type analysis that will go off and go directly to our Postgres instances. Um this is more of a visual perspective so that you can see what does our whole platform look like and what I did in this case as well as I drew a nice big box around what encompasses Postgres and Post.js. So working our way from kind of bottom to top um first is we have all our different data connections. So we have a lot of different data at State Farm or that we get from different vendors or things like that for State Farm. So first off we have a lot of different data within other ADVS data warehouses. Um so we have ways to get that data ingested into Postgres. We also have we're a very old company so we have data still on the mainframe so we got to get that into our more modern platforms in AWS we have ways to get that and connect into Postgres as well and then from there working with um things like Ezri API endpoints OGC WFS endpoints or OGC feature endpoints and then also your standard just HTTP streams. So thinking of things like the USGS earthquake, um they just have an HTTP endpoint for Giojson data. We can go out, connect to that and get that into our database. From there, we have a whole analytical suite built on top of all that data to um help people start to analyze that data. um some of those are just using the native PostJS functions while other are more custom written UDFs in order to get the task complete as well. So things from like a suitability perspective, where is the best place to put an agent's office? Where can we start a new marketing campaign? Where's the most suitable area for that? Those are things that we can do from that suitability perspective. Um enriching data. So if we get claims, which ones are um near a wildfire or within a wildfire, doing things like statistics of how many policies do we have in a certain geographical region or what is the average coverage amount, how many claims happen with any specific area or event type. And then of course we can do things like impact and modeling. So what is the impact from a catastrophe whether it's a hurricane um a hail stom a wildfire whatever it may be and then even future thinking of if certain types of events were to happen modeling out and forecasting how does that look that can all be powered by postjis for us to help get those um figured out and then just your standard normal geospatial processing clips joins buffers um convex holes, whatever it may be, those are all available for our users to use as well. Within our database itself, we have um many different types of data. So, your standard administrative boundaries, states, counties, zip codes, um things like that. We also have more specific Pearl maps. So, things like um wildfire risk, hurricane risk, um risk, things like that. We also have specific data sets to state farms. So where are all of our agents, where are our facilities, our custom territories that we come up with, as well as where our policies enforce. And then with that, u we have weather data that's coming in. So think things like USGS. So um hurricanes, wildfires, all those types of different weathers events, we ingest those into our database as well. And then we have just our standard users coming in and bringing in data as well whether that is um geospatial in format or non- geospatial in format and we convert that into a geospatial table. So in order for all of this to work um we have a multitude of different APIs that connect to our Postgres database to make all this happen. So our first one is our collections API which for the most part um has a lot of API endpoints related to the OGC features and tiles standards. And then on top of that we have some more custom and specific ones to do things to help power our platform. So things like autocomplete doing statistical analysis that are outside of that standard OGC realm. Then we also have our analysis API where they can go and plug and play with that whole analytical suite that I talked about below. We also have an imports API to let people bring data in whether it's in a flat file format or something sitting out on a web page or something else within State Farm. They can point to that, press load and it will bring that data in for them. And then we have an items API to basically search through this entire catalog of all these data sets and then kind of jumping up to above all these different maps and apps that are available for them to build out as well. So once we have all these APIs built up and established, this is where most of our user bases sitting on using our platform every single day is the um single page app that allows them to do things like we have a map builder that connects to all these different APIs to um allow them to query the data, analyze the data, color code it, visualize it um and then share with others throughout the enterprise. We also have a multitude of different noode apps. Um it's all drag and drop interface where they can do things like build out graphs, um build out analytical widgets, things like that to help them query and analyze the data and share that with others in the enterprise who may not have any geospatial background but can go into a dashboard and zoom in and out and get information that they need. We also have a portal search just due to the mass amounts of data sets that we have within the tool. This allows them to search on key contextes or filter by date ranges or types of data or whatever that may be. And then we also have a Python SDK built around all of this as well for our data scientists um within State Farm so that they can do things um thinking within like Jupyter notebooks um so that they can query and analyze business data and then ingest that or analyze that from a geospatial perspective. So, how do we make this kind of easy from a user experience perspective and what does it look like on the back? So, on the left here, this is kind of a screenshot that we have from our map build calculating geometries, things like that. They go in and select one or two different um buttons, configure that analysis, and click run. that that then sends that request off to our analysis API which is running within our Fargate cluster and what we do then is actually validate hey whatever analysis that they're running do they actually have access to that data set not everybody has access to every data set so we basically want to perform that easy validation from there we can generate that SQL query pass that back onto the database um and then using whatever those specific functions are we can go ahead and generate that new table from that analysis this request. Once we have that table built, then we gather a bunch of metadata about that table. So, who created it? Who did they provide access to? How large is the table? And then continuing forward kind of have an audit trail so that we can understand what that data is, when it was used, and retain it um in case we need to as well. And then once that um kind of jumping back once that analysis is complete like everybody we want to be able to see that data. So that's where we backtrack to our collections API. Our collections API allows us to um visualize that data using our vector tile API endpoint. So this map is showing all of our State Farm agents throughout the entire US and that just comes back as um vector tiles. So, how that works is same thing kind of like before. Um, because of all the different types of data that we have within our portal and it's allowed to be used by anyone within our enterprise, we want to make sure does that specific user um should have access to that data set. If they do, then what we're going to do is actually check to see if that tile has been cached. Um, so for anyone who's ever generated vector tiles on the fly within Postgres, um, if you don't know, depending on how complex that geometry is or how much geometry there is, it can take a very long time to get that tile generated. So what we do is if it's something that has been generated before, we go ahead and store that off in Elastic. And then at that point when they actually make that API API request, we're just going to pull up the ZXY coordinate and that specific map, get the bytes for that and return it back to the user. If not, that's okay. Then we can go ahead and generate that SQL query, send it off back into PostGIS. And at that point, using the STSMPT function, generate the tile that's needed for that specific location and return that back to the user. And then in the background, we'll then actually take that bite array and push that back into Elasticash. So then either the next time that user goes to that specific viewpoint or if they share that data set with somebody else, they're going to get that better user experience because we're going from potentially something that takes a couple of seconds to 50 to 70 milliseconds to get that data um pulled back from the API and painted onto the map. So these are just a couple examples of from a technical perspective, how can we take all these different data sets from either different um external sources that we get or from internal data that we have at State Farm and make that available for consumption for users from within our platform. Um so now I want I want to switch into what types of questions are we answering with Post.js at State Farm. So, as you can imagine, insurance company, we have claims and a lot of them. Um, so our claims department is one of the largest users of our platform as a service. Um, and they use it on a daily basis to help our customers. So, a couple of different examples of how are they using our platform is doing things like analyzing catastrophes. Um, and that is for every type of catastrophe. So in this example image here, we have some satellite imagery with a wildfire perimeter. So if you could imagine at this point then we would be able to overlay our policies and force to see um are there any policies directly within the perimeter? Are there any that are encroaching on that perimeter of the fire? Um, this allows us to quickly look at that and then we can also do it from an analytical perspective to say model out if this fire continues to grow, what does that impact look like? And then as our claims start to come into our claim system, um, we're able to then kind of see in real time where those claims are. Are people reporting them? Are they not reporting them? To help us better understand what is the impact of that specific event. For larger events where um thinking things like hurricanes, we can get 30 40 50,000 claims coming in, we have the ability to auto to automatically um assign claims as well just due to how long it would take from a manual perspective to assign those claims. PostJS helps in that perspective because as you can imagine when you have that many claims, it's going to be a lot of people that are going out and helping um those people. So, we want to make sure that that claims adjuster isn't bouncing around all over the place. Um, so when we get those claims assigned, we can keep them locked in into a very small perspective of geospatial area so that they're not driving 10 miles to this house, 15 miles to this house. They're going to stay within a very small area. We also um use Post.js to help build our claims territories for non-catastrophe claims. um kind of from the same perspective, we want to make sure that we don't have too many claim handlers in one location where they're not getting um enough claims coming in or vice versa, one claim handlers trying to handle too many claims at once and um we want to make that a better experience. So it can go both ways there per se and that can be based off same thing is based off of their geography as well as metrics within State Farm that we use to determine that. And also we use PostJS to help route vehicles to the nearest repair shop. Um and that could be while they're still at the accident scene or already at home and they're going ahead and calling in and reporting that claim. we can help post.js to help decide um what repair shop are there and which ones the customer wants to work with. So kind of a cool example that we had working u kind of handinhand with claims was in response to wildfires. Um so as our user base continued to grow of using our platform um claims started to use this on a kind of hourly basis when large wildfires were burning where they were repeatedly doing the same analysis over and over again where they actually came up with a whole process to say these are the specific geospatial analyses that we want to do every time a wildfire was happening so that they have a consistent report to share. share out with others. The issue with this though is there was only um one to two maybe three people within our area that could go ahead and run those reports. So what we did was work with them to figure out what are all those analyses that you guys need to run for every event and we went ahead and um built that as a new endpoint within our analysis API at that point. Then within our platform, they just had a drop down where they could simply just say what was the name of the wildfire that you're interested in, click analyze, and then at that point, we always had a consistent report that would come out on the other side that would give them a bunch of gojson data back so that they could look at that and get metrics about what was the impacts of that specific wildfire. So, this was really cool to see because it was nice for those couple of people within claims. they kind of got that relief of I don't have to keep doing this over and over again. And then it also gave the ability for other people within the department to be able to go ahead and respond to um from a geospatial perspective to those events. Um we also use PostJS a lot from an underwriting perspective. So, PostJS gives us the ability to um look at every single uh property from a peril perspective. So, anytime a new property comes in, we want to know one, where is it located? And then from there, what are all the different associated risks for that property? So, what's the risk of a hurricane? What's the risk of storm surge? or whatever it may be. Um, POJS has the ability to take that take that point and give us that information which is great and we can do that at an individual property basis or we could do it at a entire book of business scale or narrow it down to specific areas. That can all be done within the platform to help um underwriters better understand that type of information. It also helps from a market penetration perspective kind of the same concept of we have x amount of policies within this location. Do we have room to grow within that area? Um and then as well it they can kind of focus and move around within the map to specific areas and get regenerated and recalculated information on the fly so that they can help make their decisions. So these are just a couple of different examples from a business perspective as well of what type of questions do we get from our different departments and how we kind of tailored and built our um platform around those. So that is all I have for today. Um thank you to Elizabeth and Paul for giving me the opportunity to talk today. If you guys do have any questions, looks like I have a couple of minutes to answer. If not, feel free to get in touch by going to the URL below and filling out the form. Thank you. >> Thank you, Michael. I really appreciate you uh coming to post your stay and and giving this presentation and letting us know what's going on at State Farm. Uh so I see one question hiding in the >> the chat. [clears throat] It's a question I had actually, which was when you'd decide to expire the elastic catch. Um >> good question. Um so it kind of it's a it depends. Um so we have data that um the location services team which is the team that manages this platform they have a bunch of data that they're responsible for that gets updated on different intervals. So in some cases for like live data we actually don't even cache it whatsoever because it's being updated so frequently. We want to make sure that nobody ever even hits potentially a cached tile. it's always fetching directly from postgrow so that they get the most up-to-date data. In other cases, if it's data so like states the boundaries really never change for those. Um so those we just simply leave because a lot of people like to use those from a thematic perspective and those can just directly get rerendered. So really it depends on what the data set is. Um and then if it's user data sets, those have specific policies that we have to follow within um our organization that have retention set on them. Those are just based off of those. Once it hits that retention policy, it just rolls off of Elasticash. >> Yeah. [clears throat] have the deer AI question because I mean whether you like find the prospect of AI exciting or terrible because I feel like like you've done the hard part like in assembling a whole bunch of consistent data and having metadata about it and standardizing your platform and APIs like it's not a moving target. Um and it would seem like a place where you could get AI to be fairly reliable in terms of the answers it gave back. Have you started playing with that? Do you are you afraid to play with that? what's what's your POS posture of on AI? >> Yeah, good question. um don't have anything that I can say within this talk specifically right now but um it's definitely something that we're looking at internally >> and uh okay I have another impolitic question which is um so you're in this this realm of providing end user maps and cardioraphic visualization and I found in organizations frequently the desktop wags the dog and determines the uh platform you're allowed to build like why hasn't shut you down or why didn't they shut you down when you started in terms of like building the platform. >> Well, I mean uh often times I found like the Ezra if if an organization has an existing Ezra user base that has its hands around the the function um that will determine a lot about how you end up architecting the platform or the central system. >> Oh, I see. Yeah. Um so in our case we kind of got um we previously had our own prior to this tool we had kind of our own internally built mapping product. Um but this is like in the earlyish 2000s. >> Um so it didn't have a lot of the functionality that you would see in any kind of platform as a service today that whether you build it internally or buy that from a vendor. Um, so luckily that's where kind of our core user base was at the time and we were kind of just able to lift and shift them onto this platform and then from that point we just continue to build and build upon it based off of whatever the requirements were >> your app states to start with. Um, what uh what's what's coming next? What's your next big uh change to the platform? >> Yeah. We're at a pretty good state where a lot of our business partners are kind of happy with what they're seeing today. Um, so once we hear more from them, that's how we continue to adapt based off of that. >> Awesome. Well, um, thank you very much for for joining us, Michael. "}
{"url": "https://www.youtube.com/watch?v=3YDSV2S-eAA", "title": "PostGIS: Spatial, Special, And Something Else", "transcript": "Our uh our next uh our next presenter needs no introduction but of course I will introduce her no nonetheless. Um Regina Obe is a member of the Postess project steering committee. Um she's the author of Postess in action and Postgress up and running and probably other books as well. Uh she's one of the earliest users of PostJS and is now a core contributor and the developer who keeps us all on track and keeps the trains running on time. Bringer of order from the chaos, Virginia obey. Welcome to Post Day. >> Hi y'all. Um I'm also the maintainer of the Windows Posters bundle. So if you want to complain about that, I'm the person to yell at >> the thing. Um, and I'm working on the fourth edition of Postgress up and running and also the second edition of PG routing. So hopefully get done this year or early next. So today I'm going to take you on a travel back in time to the year 1974 when I was a baby of three. And this is before Postgress even existed. Prior to Postgress, there was a database called ingress, a research project started in 1974 around the time of system R, which is when relational databases started. And it was funded to create a prototype for a geographical and graphical database system. But as all good funding initiatives go, why spend money on what you were supposed to be building when you could build something else that more people can use? And so much of their time was spent building a relational database system instead. And to say that it didn't have any geographic use would not be quite right. they did have um data types such as polygons, points and line six to satisfy at least some of the grant requirement. And ingress turned out to be a pretty impressive relational databases because it was open-source. A lot of the um proprietary systems spun off from that. It also had a language called Quill that was closer in syntax to the relational algebra that the um father Edgar Fcod of relational databases wanted. So it it catered more to mathematicians than it did to normal people and it also led to other initiatives such as introduction of parallel queries, distributed data, distributed execution. But as far as graphic and geographic, it fell a little short of that objective. And today, Ingress still exists except it's closed source. And it you can find um it was the seed for other closed source databases. So things like CIA, Microsoft SQL Server, non-stop SQL all those still exist today and those had their foundation in ingress. So after the Ingress project was done, another database project started started by the same people that founded Ingress and this database was called Postgress. It didn't use any of the code that Ingress had. It was a reimagining of what Ingress could be if instead of hardwired ex um types, the type system was more flexible and it provided flexibility for indexes and operators and all that good stuff. And for testing all that, we have our favorite friends polygon point and lines which showed up in the codebase. And it had a language that was like well called Postquel which could do something else that was interesting. And that interesting thing was time travel. And the foundation of time travel was how it stored data that it would never delete data. It would just append data. So what's a what's in a name? So we call Postgress by Postgress or PostgreSQL. And Postgress in the beginning didn't even uh support SQL. that came much later around 1995. um during that time when Postgress was getting its SQL footing, it crashed a lot and so the years after it the the research project ended and Postgress found its way into the open-source world, the objective was to prevent it from not crashing so much and to try to catch up by beefing up the SQL um support in it and extending that. And so today the biggest problem that Postgress has is a marketing one because PostgreSQL is hard to pronounce. And so when you go into when sales people try to sell it, they stumble on their words, which is really bad if you're trying to sell stuff. And so you call it Postgress, but then it doesn't really match with the website. So is it the same database? We don't know. And then if you're in the spatial world, you might call it post. And the spatial people haven't heard of postgraph, so it's just called post or it's post geese or it's postgis. So it goes by several different names. So forget about marketing because marketing is not important. Let's talk about the let's not crash so much years and how that led to where we are today. So the let's not crash so much years are the years where the code was um smoothed out a little bit. Some complexity was removed and the biggest pieces of complexity were the extended type support system and this time travel thing. And so there were discussions to remove both because who really needs extended types? Who really needs to time travel? But shortly after that while this debate was going on another project started called Postchess and this built on the extensibility that Postgress offered. But um but what was special about Postess is that it actually made good on the promise of a geographical database management system. And so that G that's been lingering around since ingress was finally deserving of a capital G instead of a small G. So since post the extensibility in Postgress has only increased. We now have a new SQL grammar called create extension which allows you to dynamically load extensions into your database. and how deep you can go to change Postgress has been continually changing. Even if you just consider the um the spatial extensions we have, that's a rich set. But if you go beyond that, we're talking about hundreds of extensions that you can install in Postgress. So this time travel thing that I talked about that was ripped out. This was ripped out before postress even came on the scene in 7.0. The last version of Postgress to support time travel was 6.5. And what exactly was this time travel? Well, it allowed you to do what's called um flashback reporting where you could say for any point in time, what did my database look like? If I were to generate a query that matches exactly that paper report I gave my CEO, would I be able to do it? And how would I do it? So, you would just simply state the date that you gave the report and you could generate the same query exactly as it was. And it's used for other things like it's a good way for recovering data. um if a user deletes data or updates data, it's a way to check if your data base has been tampered with. And today lots of databases support time travel. So Snowflake supports time travel. Oracle supports time travel, SQL Server supports time travel and they all support it much in the same way as what the ISOSQL standard says that it should be supported. So unlike the Postgress way um you have the choice of what tables you h um can support time travel whereas in the Postgress days you were stuck with time travel all your tables support time travel you had no choice about it and so even though time travel was removed from Postgress there's still remnants of it for one you just need to look at the mascot we've got an elephant An elephant's known for never forgetting just as Postgress is known for that. And if you look deeper in the core of Postgress, you'll find that every single table in Postgress has hidden fields. When you do select starum table, you wouldn't see these fields. But you if you explicitly select them, you will see them and they'll tell you an idea of when a record was created and if it's the latest version. And then there's a problem that haunts Postgress that has haunted it for quite some time, but um it's gotten much better these days, and that's the idea of vacuuming, which is to remove dead tpples. And the reason why this even existed is to archive dead tpples to tape so that you could still query them. You could still um do time travel. So where does that leave us today? So even though the core of Postgress no longer supports time travel, you'll find time travel living under the current of many extensions. If you were just to look at our own backyard, you'll find that even um [clears throat] Postgress the core has improved on support of temporal and a lot of this is used by users to build their own time travel systems. And then there's post. Does post support time travel? Well, um, many people believe that the ST in the SQL multimedia part 3 standard for spatial stands for spatial temporal. It's still unclear whether that is what it stands for because the the um specs never tell you what it stands for. They just state that all functions and types will start with ST. And then there's this mysterious M coordinate that post and many spatial databases have which many people use for either measuring things or for recording time for trajectories. And you'll find several functions and posters that support trajectory. And then there's an extension um which is relatively new which builds on top of postches except it introduces temporal types which are more explicit than the use of M and those are used for mostly managing spatial trajectories but they can be used for nonspatial trajectories as well. And there's a recent book that came out about a year ago that has uh several examples for using it written by the author of the mobility DB extension. So now I'm going to show you some examples of time travel. So first of all um in our work we do deal with a lot of time series like data where we need to do analysis such as determining length of stay in hospitals if a person is covered for a particular period by their insurance if people have worked enough to garner pension and how much pension they would get. And so in the past um Postgress only had range types. So range types aren't sufficient if you need to record gaps in time. So my partner Leo, what he would do is he would leverage the post functions to manage time. And so he basically modeled um time as a as a line. For the most part, all he really needed was the x coordinate, but on occasion, he would also need the y-coordinate to record certain things. So, if we were to say record salary, convert um a salary period to a geometry, he'd have a function something like this. And the way he recorded um time because you all he needed were dates. He didn't need to record at the at the second level. he would use Julian calendar days. And so he'd convert a date to a Julian calendar date. So here's an example of what that would look like if you're using it for managing salary data. You create an employee table and you'd have a salary history column that's a geometry. And then you would create you can have a function that converts a date to a vertical line. And then if you want to make your life easier, you can create a cast that converts a date to a geometry. And so then if you wanted a table, a view that kept track of what the latest salary is of anybody, you would create a view on top of your table that looked like this. It would basically translate the geometry back to a salary and it would convert the x coordinate back to a date. And then so you don't really have to think in terms of geometry. You could create a trigger an instead of trigger against the view. So here so views can't have real triggers but they can have instead of triggers which is instead of actually editing uh updating the data in the table do this instead. And so if I were to add data, so the the trigger that I created here, um I'm actually converting inserts to updates if the table if the record already exists. So that I always have at most only one record per employee in my employee table, even though it looks like I'm inserting tons of data. So if you look here supposedly I'm inserting two records and here supposedly I'm inserting six records and in this one I'm inserting four records but I'm not really inserting If you look at my original table, you will see there's only three records. So all that inserting converted to just expanding my line string. And if you were to look at the view, it would show um a pay period date and a salary, which is the latest So this has a nice feature that you can do you can determine what anybody's salary as any at any point in time using the built-in functions in post. So I can ask the question um for this date what was my salary simply by asking what um what point in time intersects what what salary intersects with this point in time that I I'm giving you and so it would show um only Sam because only Sam was working in this period so he's the only one that's going to show up since I'm filtering for only those that intersect And I could do that for another date. Similar thing. So I would see anybody that was working at that time. I can check the duration that anybody has worked. So that tells me the number of days people have worked. And you could do a similar thing with mobility DB. It's a little bit less cryptic with mobility DB because mobility DB understands time as time instead of uh as a number. So if I were to recreate the view, I mean the table, I would use this uh data type that's provided by Mobility DB, which is a is a time float, a sequential time float. And instead of using the post functions, I would use these um functions provided by mobility DB. And a time float would look something like this in its canonical form. So you could make out the dates is not as cryptic as a whole bunch of binary bits. And again, I would do a trigger so that I would end up appending to my my sequence and I could insert the same records using the same exactly the same syntax as I did before. And one thing that's nice about mobility DB is if you specify a type as being sequential, it won't allow you to break that rule. So if it's sequential and not overlapping, this will fail because it overlaps with um with data I already have. And so you get the same answer except the sale history is a little bit more legible. You can actually make out stuff. And you can ask answer the same questions using this value add timestamp. And one thing I noticed that's a little different is if you ask for the duration, it gives you 84 days and 1 hour because I I realized now that um the date I had specified here was during a period where there was um time um where there was daylight savings. So I actually gained an hour. So I worked an extra hour during that period. And mobility DB is primarily known for um managing spatial data. So data in motion like transportation or bird flight or things of that sort. So you generally have like a GPX file that you would load in. And so the way you could load in a GPX is convert uh so each point you can convert to a geographic a temporal geographic point and then you can aggregate them using this append instant. And so you have or it's a binary thing and like ge like posters data you can create a just index on it and the neat thing about it is well first of all your operations are a bit more intuitive. you don't have to do this um convert back and forth between geometry to do temporal checks. And it's also got other neat functions like being able to compute speed. I think I had another one. there's a average speed thing or you know figure out the average speed of um of a trajectory and there are a whole bunch of other functions that you could do with it. So the other thing that's useful is um let's see if I go back to this you see that it allows you to convert a temporal mobility DB back to uh geography and the way it does it it um converts it using to the using m for time. So you can go back and forth between the two. >> Okay, I think that's about it. Does anyone have questions? >> You've left everybody stunned. >> Really? >> Did I? >> Seems like um didn't there used to be a contrib extension table history? >> Oh yeah, there is. It's still there and I haven't looked at it. >> There is or was. I was just looking for it and I couldn't find it. >> Yeah, it's in extras, but it hasn't really We never turned it into an extension. >> Yeah. Oh, it's our extras. >> Yeah, our extras. >> Okay. >> But there's also another extension called period which tries to simulate the um Actually, let's see. Do I have it here? This one. I think I kind of glossed over it. Uh yeah. So there's this extension called periods which tries to simulate the ISOS SQL standard. But the problem here which is which I think would be neat a feature that would be nice to have in um Postgress is I don't think there's a way in for an extension to add grammar to SQL. >> No, not at this point. >> Yeah. So they're kind of forced, they do the best they can to simulate the ISO standard, but they can't get the syntax because an extension can't introduce new SQL syntax. >> But that would be neat because imagine all the things we could do. A >> I've always liked well I've liked history and time travel but I find whenever I actually start reading the standards for temporal data my eyes glaze over because of the whole system time versus world time disjunction the idea that every record has to have not one time but two time or sorry not one period but two periods. >> Yeah. Yeah. It's always been much. >> Yeah. So I think people re end up reinventing their own way of handling it because at least you understand what's going on. >> And the tempor temporal people will tell you that you've done it wrong. >> Yeah. >> Inevitably your in invention is is too weak and does not cover the various corner cases they can lay out. >> Yeah. Yeah. I think I always end up inventing my own cuz I tried to use the SQL Server one once and it was because I couldn't figure out how to rewrite history or even if you could rewrite history because you know if a load goes bad. >> Yeah. >> Or you or you record data out of um out of sync or something like that. >> So we do have questions on the channel uh like Phil asks how does this differ from NVCC? Don't most databases do that now including Postgress? >> How is it? Um it probably doesn't too much. Well, the way I think the way others implement MVCC, they have a separate file. It's not appended to like to the same table. >> Yeah. But Postgress's handling of MVCC is in fact like a an outgrowth of its old time travel facility. >> Yeah. >> Yeah. Um Showb has a mobility DB question. I've been dying to find good data sets for GTFS. Uh GTFS RT is there now a good source in your experience to try more use cases around road networks and traffic. >> Yeah, I was going to use I think like math GIS has GTFS and I was trying to understand it. I still don't have my head around GTFS. I I did have an example in here for GTFS but I took it out because I figured I wouldn't have time to cover it. And the final question is, have you seen some of this space-time data stuff that Josh Campbell is working on? Um, I think somewhat related asks Elizabeth. >> Uh, no. I don't even know who Josh Kimble is. Yeah, he's uh affiliated with the state department and uh his problem is the fact that uh international borders change all the time and you will definitely want to keep a historical space-time view of those changes. "}
{"url": "https://www.youtube.com/watch?v=UdQVRTUAhRc", "title": "Service-Oriented Isochrone Mapping With PostGIS And PGRouting", "transcript": "Samuel Matherther, >> a first-time Postess presenter. >> Yes, indeed. >> The first of many times, I assume. Um, >> that's the hope. Yes. >> He comes to the topic with fresh eyes, and we are looking forward to hearing what he has to tell us about Postess, uh, PG routing and isocrones, which have to be some of my favorite crrons. Um, Sammy Mather, I understand you're of the Cleveland Mathers. >> Yes, I am. Yeah, I know. I have uh uh some expertise in my little corner of Cleveland that I pulled on for this project uh which I'm excited to share. >> All right. Um so feel free to take over the screen share and uh >> let's see. >> I can waiting waiting waiting. I can see your screen. >> Excellent. Okay. Good. Not used to this little bar. This thing's new. Do I minimize? There we go. Okay. Yes. Hello. Uh, my name is Samuel Mather and I am here today to talk a little bit about my experience with serviceoriented isocchrome mapping with postris and pg routing. Um so I am a high school student at the Mastery School of Hawin which makes my use case for PostJS perhaps unique among those presented about today. Um it's a a smaller project and my first with uh PostJS uh uh PostgrSQL and any database manager. Um but uh I think it's a pretty interesting one. Um so uh by way of context for uh mastery uh mastery structure we're completely gradeless. Uh so we demonstrate our mastery on our transcript um through um instances of applying the skills that are uh deemed necessary to have before graduating high school. Uh usually through community partnerships. Uh those are kind of the catalyst of our learning structure. So uh in any given class we'll have three partnerships um and our partners will give us a challenge uh which we will work uh towards addressing in teams of four. uh and this term I took a class on epidemiology and we had um two partners um whose challenges to us uh prompted me to uh use uh PostgresSQL and the PostJS extension um to uh address their challenges. Um the first of those two uh partners was the Kyhoga County Board of Health. Um they have a points of distribution system uh pods um which they operate uh when there is some uh emergency in the community that necessitates the distribution of emergency resources. Uh so that could be um an like a a chemical spill where people need cleaning or a pandemic like we saw in 2020 where people needed vaccines uh or something that causes a food or water shortage. Um and basically they wanted to make these points of distribution more accessible um to a greater number of people with uh uh disabilities uh or uh people um above a certain age who might not be uh as able to transit long distances. Um and then we also worked with the Greater Cleveland Food Bank who were looking for local food pantries uh to whom they wanted to distribute um a greater a greater amount of produce using a specific protocol called the pharmacy program. Um and they were looking for pantries that were high capacity and in areas ideally uh with a high need for more accessible high-quality produce. Um, for the purposes of uh what I'm going to be talking about in this presentation, I'll mostly be focused on my time working with the Kyhoga County Board of Health. Uh, that was kind of where this process all got started for me. Their challenge language that they gave us, uh, they they asked us specifically to use data and community input to recommend how they could make pod locations more accessible for people with disabilities and transportation needs. Um, and uh, there are kind of two main pieces to that. the more data facing piece and the more communityf facing piece. Um, and I wound up very focused in my team on the data facing piece because I wasn't as able to get out in the community as some of my teammates. Um, and I really wanted to find a way to visualize um access to uh the pod locations um by roadway. Uh that was an idea that I had and I was asking my father about it who is a GIS expert himself and uh he told me about isocchromeone and isodistance maps and advised me uh to uh literally take a page out of his book uh and uh pointed me to pages 223 to 232 of the first edition of the postGIS cookbook which he co-authored uh which detail how to make um isocchronone and isodistance maps. um he schooled me on um managing a database in postgrsql. Uh and um uh I I found myself in a situation where I was working with code that is very old and not compatible uh or efficient with uh the latest version of postcarql and the latest version of postchess. Um so the task that I took on in that partner challenge was uh revising the code to work with uh the current version of Postgra and be as efficient as uh it it uh could be um and then um write it all up in a format that was accessible to the board of health so that they could use these tools uh in future for visualizing roadway access uh to their pod locations. Um, and I include this photo uh uh postress has been a uh in the background of my life for uh as as long as I can remember. Um but I have not until now had a chance to really explore it. So this was very exciting for me. Um so just to briefly define issues though I'm sure many people on this call are familiar um if we have a buffer map where um the we we have a point and the buffer uh has a radius of distance x away from that point. Um presumably if this exists on a map somewhere and there are roadways extending from that point past the borders of the buffer um they will not uh travel in the most direct path possible uh to leave the buffer. They will probably wind around a little bit and thus um distance x along the roads is probably not going to be uh uh is not going to go as far as a buffer with radius x. Um so when visualizing access uh we can basically shrink wrap um a buffer around the roadways I guess is one way to think about it. Um so that a buffer of a certain size uh is represented in terms of a distance along roadways instead of distance in the purest sense. Um and actually what I'm mostly working with here are is distance maps. Um but my spell check says that is distance isn't a word. So I've uh uh rather inaccurately used isocchromeone in most places because it is a much more convenient word to work with in Microsoft Office. Um so the deliverable structure that I created for my challenge partners was I gave them ISOs at 1500 m 1 mile and 5 miles from each of their pod locations uh using uh walking distance long roadways for 1500 m and 1 mile driving distance for 5 miles. Um, I created a map overlay with census data um, for visual comparisons between areas of accessibility and areas of high risk/ne. Um, and then I also gave them the actual code for their future use. Um, and uh, I'll I'll quickly give the uh, recipe I guess that I used to make this all happen. I started with um, a set of points. Um, uh, they had everything. They didn't actually have a map of all their pod locations. Like they didn't have any way to visualize access. Uh so my team had to do some geocoding to make this happen. Um now these are not the actual points um where their pod locations are in Kyogre County. They do not want those to be revealed to the public. So for the purposes of this presentation, I'm using a randomly generated set of points uh that will be our fake pods. Uh for these purposes I then imported um some OSM derived roadway data uh via geo fabric um which I used to do the actual dystra routing like those were the routes that I was running through uh and those had walking distance and driving distance um which I used respectively at the different distances. Um and then I also imported uh some census data that was relevant to the nature of the challenge. uh for the purposes of this example, households without a vehicle. Um and then the tools that I used postgra obviously postJS um and then I compiled everything in QJIS. Uh the two of note are I used PG routing to actually perform the dystra routing and OSM to PG routing to make PG routing be able to read the OSM based roadway data that I had. Um so this um uh the the code that I uh revised from my dad's old code to make this happen. um utilizes two functions. Um the first is a fairly simple function that uh takes a point on the map and locates the nearest piece of the roadway um and uh uh marks that and makes a list for all of the points um all of the pod locations where the nearest bit of roadway is. Um and then uh uh it gets fed through um those points get fed through this function. Um um the first half of which is shown here. Um and uh uh oops, sorry about that. Um this is the part that actually does the dystra routing. Um so this goes out along the roadways at the distance that is specified um and uh then creates a concave hole in this part of the function um around the end point of those distances uh to create the isodistance map uh which would then looks something like that. Um and then this is the piece that uh ties those functions together. It's pretty straightforward. The two bits that change frequently um when making um uh different iso distances is just the number of iso distances, the distance specified which is that number at the far end of the uh parenthesis uh in the indented section and then which data set it's pulling from. Um so these are some examples of the products that um I was able to generate. Obviously um the stuff that is tied to uh the pods is uh um random points. So this was not our actual deliverable. Um but this is representative of one pattern we were able to very easily identify using this visual medium uh which was that there were certain areas uh on the edges of the county um where there were uh town centers that did not have any pods in them or near them. Um, this shows uh ISA distances from pod locations at 5 miles. And we can see that uh reminderville does not have uh any within 5 miles along roadways. Um we can also use this method to very simply and easily visualize areas where uh there's limited access at scale. Um which was something we were able to do as well for uh the Kyoga County Board of Health uh in addition to giving them the code. And then we were able to recommend locations uh within those service gaps to uh institute new pods. Uh and then you can also easily compare um between uh different locations uh which are more accessible by foot. Um so obviously in this case the one on the right is far more accessible uh by foot path than the one on the left. Um, now I can show you the deliverables for the Greater Cleveland Food Bank since it's uh common knowledge which of the uh which of the the locations in the Cleveland area are food pantries. Um, this was one of our deliverables for them. We showed them um we we narrowed down their highest capacity pantries and then we created uh ISO distances around those high-capacity pantries and visualized it over top some census data showing food insecurity in the area so that we could identify which of the locations that could take on the produce that uh you want to distribute uh are in high need areas and we were able to identify a couple straight away. Um so there are some limitations next steps to this process. Obviously, it's not a true isocone is since cost was not a factor. We weren't considering um uh travel time or uh like terrain difficulty or anything like that. Um and there is some potential there to deepen the analysis. Um there's also potential here for a proportional disagregation of the census data relative to the concave hole created in the isocchronone making process which is something that I'd like to look into developing this further. uh and it's hard to know the exact impacts that these deliverables have had or will have because uh the one drawback to our community oriented system is because we move on so quickly to the next thing. Uh we seldom follow back up with uh community partners. Um so yeah, it's hard for me to know exactly how this got used or will get used. Uh but it is something that I tried to make accessible to the partners. Uh and I guess just generally this being my first forier into uh using Postris and PostgrSQL, a big takeaway for me was that uh database management is not nearly as scary as I thought it would be. And actually uh the way these things are set up and uh uh the the guides that are out there for getting started um actually felt very accessible to me. Um, and I I um I guess would like to express some gratitude to this community uh for setting things up in such a way that uh I could make an impact on my own local community here using these tools uh and get started so quickly and easily. A couple other thanks are in order. Thanks to Stephen Matherer's mentorship, provision of resources and consultation during this process and to my challenge teams for their support particularly uh from the team sweet peas which was working with the Kyogre County Board of Health, Katherine Bolan, who uh did the geocoding manually which was crazy and impressive and very useful and Jonas Baird from Epidemiology which was the team I was in that was uh working with um the Greater Cleveland Food Bank uh for finding uh the the census data uh and uh information on food insecurity which was more difficult than we expected. Uh yeah, that's what I've got and I will now address any questions. Well, if anyone has questions for Daniel online, put them in the Q&A or put them in the chat. Um I'm always amazed by the extent to which is actually, you know, an approximation of a circle in a dense urban grid. It's like the only time it tends to break down is if you have like a city that's built around a river >> or some some big natural barrier like oh yeah a grid and it it's mostly circular. >> Um >> well there definitely are some weird barriers like uh around highways that was one thing uh that I saw pretty >> freeway system acts as a big barrier system. Yeah >> which are often in areas where there is higher need for these resources. Uh so that does become an interesting problem space in and of itself. >> Yeah that's history on the map. Um, and then uh I don't know, did you play around any at all with costed isrons to see if that changed the shape of your shapes? >> Uh, I I did a little bit with some of the test stuff. Um, I haven't had a chance to really play around with it with the deliverables, but I would like to and I'm especially interested in the the data aggregation side of things. That's something I've been uh hoping to find some time to look into. And >> what do you mean by data aggregation? uh like taking the census data and um uh breaking it up into the uh the the polygon created by the ISO distance and uh doing some um analysis based on that like aggregating the data into those areas specifically and comparing them uh like with specific figures because what I like the deliverable that I presented to um the the Greater Cleveland Food Bank for example was just overlaying a census data a map with an ISO distance map and that is inherently lossy because you're uh gathering from what you can see in that overlap uh not what is uh true about that overlap inherent to the data. So >> great. >> Yeah. >> Awesome. That's it for my questions. U oh we do have a question on the guest. How did you come up with one mile for your isoperone? >> Uh it was a nice round number. Same with five. Um, and one mile was nice, too, because um, we were trying to find like what what the outer limit of what someone might reasonably walk to access some of these resources might be. Um, and that felt like like probably if it's more than one mile, it's um, something that would be more reasonable with transit or driving if those are accessible tools. Um, was the thought. Uh, >> right. >> Yeah. >> And a question from Benjamin on the chat. Uh, cool work. Did you try an AAR as an alternative to Dystra referring to routing? >> Uh, no I did not. >> No. >> All right. Thank you so much for your time, Samuel, and for coming and sharing that with us. That was a great uh great presentation. "}
{"url": "https://www.youtube.com/watch?v=oFg3nh3hZ7I", "title": "Getting OpenStreetMap Data In PostGIS With osm2pgsql", "transcript": "Welcome to my talk about OSM to BGSQL. My name is Yor to I'm a freelance software developer and consultant specializing in open street map data processing. was into PGSQL is a tool for importing open street map data into a postGIS database and if you want to keep it up to date there from changes in OSM it is um been around uh since 2006 and uh I've been working on it um since 2019 and I'm uh the person doing most of development at the moment. Uh the tool is written in C++ and is available for all major platforms. Um the main reason always SQL exists is to create maps from open street mapap data. We import the data into the database and then have lots of tools that uh and you you might know a lot of them that can be used to create um maps from data in in postjs. It's not the only reason but um this is how oto pgsql was started and um this is the maps that started it all. That's the map that you can see if you go to optimum.org. Um [snorts] [clears throat] soal upstream cart map. Um and um this is still run from open from OSMPGS SQL. Um and um but this is not the only um map in use today. Of course, there's lots of other maps that take open street map data and do some specialized things with it. And this is where um PostJS and OM2 PGSQL shine. For instance, this is a railway map that takes open street map data about railway infrastructure, about signals, about lines and how electrified they are or not and all these kind of information and uh loads it into a a database and then creates specialized maps from it. Another um example is the open camping map that takes information about campgrounds um and um turns that into a specialized map. Again, um here's another example that I created from um um data in uh postJS using uh OM2P SQL data in in postJS um uh and created vector tiles from it. Um but it's not only maps. You can use the data obviously for lots of other things. For instance, for search, this is the nominatum search that you find in openstream.org U where you can type in a place in this case a case and it it'll find the place for you and uh put a put a marker on the map. [snorts] Um nominatium also uses an OSM2pgsql generated database with lots of extra processing on top of it that are done in uh the postjs database. Um so why do we need a specialized tool for that? There are um other ETL tools that you can use to import Open Street Map data into a database. Um for instance, OGR um has functionality for reading OSM files. So you could uh could could use those also. But there's um several reasons which make OM2P SQL the right tool for these kind of jobs and I'll go through all of them. Um first it understands the OSM data formats in detail. OSM data doesn't uh come into the normal simple feature data formats. Uh it has its own sort of u idiosyncratic um format with nodes, ways and relations. Nodes are basically points. Um so they have a location and they have one or more tags or zero or more tags that tell tell us what this node is about. So here for instance maybe this red one red dot is a amen tagged as amenity restaurant which means it's a restaurant [snorts] um and um other uh nodes have tags or maybe they don't have tags. So in this case um the gray um nodes don't even have tags. They are only used as um points for what we call ways. Um so to make up a line string for instance you basically need a way. The way points to the nodes it goes over um and then has tags again here. In this case the blue way has the primary highway primary tag. Um so it's some kind of major road and and a name. And the other uh ways here um might have different uh different highway or other tanks. Um this whole thing builds a topological model um where you can actually route over the lines. You know when two lines cross if there's a node that connects them then that means there is a connection. If if there isn't then there is no connection at that point. It might be a bridge over another road. These kind of things or um you can for instance have um here this a barrier gate. So the node is tagged as barrier gate and it the node is part of the way. So we know that there's a gate inside um on on this road um that that can block the traffic there. Um and then we have something called relations. Um relations um connect other objects. They connect nodes um ways or other relations into a bigger object. In this case, we might have a hiking route and the hiking route goes through the blue Y and the green Y and the black Y um and connects them all to one longer route. So there could be a hiking route or a bus route or something like that. But relations are also used to create multipolygons for instance or um to give um uh give uh other information. Um so as you can see this is a very different model than the simple feature model that you know if with points line strings and polygons and multi-points multi-line strings and multiplygons and the normal um sort of postgress postgress would be tables with columns uh or or um layers and attributes um that that you have in a shape file or in other kind of um GIS formats. Um a second reason why always M2PGS SQL is a a good tool for um this kind of job is it scales from very small extracts to the full planet. So you can use um data just for a single city or something and um use OM2PGS SQL to import that data into a postquest database and it will just take a few seconds. Um or you can uh get all the data from open streamer for the whole planet at the moment. That's more than 1.4 four billion objects um and import them all into a database and um you don't even re need a huge machine for that. 128 GB of RAM uh you need uh for this task and one or two terabytes of um SSD depending on um configuration all that and it will take 6 to 12 hours something like that depending again on many configuration options and uh options and and and and your hardware. Um what is also um uh different uh is that open or PGCQL can do updates. So you can import the OSM data and then later on uh keep up with the changes that come into OSM and there's changes every second some something is changed somewhere on the planet and you can um push them all into the database piece by piece. So um this is how it uh works when when you're running into PGSQL you call it on the command line you give [snorts] it the database name um so it can connect to your database and some extra information if you have a password or these kind of things um you have to switch it to flex mode um um so the tool has been around a long time as I said um and we want to be backwards compatible so the same command line that you could run 10 years ago you can still run today but um nowadays you don't want to use um the old mode that was pretty limited in what it can do but uses a flex mode. So you give it a command line and option uh- o flex and then you give it a config file and the input file the data file from OSM that you want to import. Um if you want to have updates you have this extra option d-slim. Um again one of those old uh options that doesn't make much sense but it basically means this is an updatable database. Um and then it imports the data in a specific way and give um store some extra information in the database that um are needed to update um the database later on. And um then you can initialize the replication process. Um there's a specialized tool called OM2PGSQL-replication um that comes with OSMPGSQL and it will orchestrate the whole update process. So you initialize it once and then you run it every minute or every hour or how often you want to do the updates and it will automatically do the right thing. Download the right data from open street map and um push it into your database and always M2PGS SQL is highly configurable. It can do a lot more than other tools can um to bring the data uh into the database in the form that you need it. And uh the way it does that is by using the Lua configuration language. Lua is a a programming language often used in in games or for writing the game logic. Um and it's easy to integrate into C++ program. That's why we're using it. Um and this configuration u um file that you create using the Lua language has has basically two parts. you set up everything once and then um you describe how the data processing is supposed to work. So setup means you define um the tables with their column names and the types of the columns and these kind of things. Um and you define indexes and some other things. Uh, and this is sort of similar to what you would do with a create table command and all these things, but it gives some extra information to um, OSM to P PGC. And here's an example how this could work. Um, when I created this uh, talk um, it was Halloween, so I I created this uh, decided to create this zombie table that shows all the areas or will later show all the areas where there are zombies. Um, and um, I want to have two columns, a name column um, that's just text column and a geometry column called geome um, that's a geometry column with a with a polygon type. And I could add here more information like which projection to use. So I could use the SR ID um, or some other information uh, in here. And I define this as a table and as I want to store ways in that table that is important um so that it works with updates and uh keep a handle um to this newly created table called zombies and um this so that we can later refer to this table when we fill it. The second step then uh is to define how each and every node way and relation is to be handled. um and um OS into PGSQL will read read the OSM data file and um to um handle each object uh piece by piece go through all the objects look at each object and give it to you for data processing and you decide what to do with it. So in this case um we're looking how the processing for ways work. So this is a process wave function that you define here and we know that there's two different ways of tagging a cemetery in opt can either be tagged as land use cemetery or as amenity graveyard. Um so we check for these uh for one or the other of these tags. Um then we take the name uh tag and we exchange every letter O in that name by this jacko'lantern emoji. Um and then uh we can insert this into a database. So we're using again the zombies um v variable that we defined earlier that gives us information about the the table that we have created above and [snorts] um insert the data in this case the change name and we create a polygon from this way um uh and put that in the in the g column and this is the way to tell um M2PGSQL how to do the processing Um and uh so you always have basically these different steps. You select some kind of interesting data by uh looking at the tags. Um so if you want to create find find all the highways, you say let let's see everything that's tagged as um highway uh and um then you clean up the data. You um transform it in a suitable form. So for instance, you might say for the highway data, there's a max speed uh tag that gives you the information how fast you allowed to go on this stretch of road. Um and if there's no unit and it's just a number in open street map, then that means kilometers per hour. But if if the max speed is given in miles per hour, then it will have the information MP or the unit um mph in it. And you can figure that out and then transform it and say okay um um multiply it by 1.6 so they you get the kilometers or or whatever you want to get a form that makes the data easy to use then later on for whatever your use case is. Uh and then you create the proper geometry. Um in this case a polygon from this way. But if you have a uh with a with a um cemeteries we created a polygon but um if if it's a road map that you're interested in you would create a line string from uh from the way and so on and write this uh information then in the right database table. There's a lot more geometry processing that OMPGL can do. We can project into any projection that you want to. We can simplify the data or merge lines or calculate the length on an object. Calculate a centrid of a polygon. Um this is only sort of the most important geometry transformations that you often want to do when using open street map data. It doesn't it it can't do everything that postJS can do, but we're importing the data into into postgs. So there's a lot more that we can do afterwards just sort of the most important stuff that we often want to do um we can do in OMTP SQL. Um there's some more um stuff I want to talk about uh locators about tile expiry and about data generalization features that are in um OS enterpq locators is a new feature just um uh added in version 2.2.0 zero. Um, it allows you to find out where an object is. Basically, it feeds every all the data, all the objects from OSM um through a point in polygon check. Um, so you give it a list of polygons, for instance, a list of country polygons or states in the US or something like that and it figures out where exactly the data is. Um and then you can use this for instance for high if you want to have high veil shields on um your map you need to know in which state something is so you can use the proper highway shields with this with the proper symbols um but there's many other use cases and it's a lot faster to check with M data um against this list of polygons while doing the import instead of doing this afterwards in uh postgs which is of course also something you can do Um and then there is um the tile expire. In many cases when you want to create a map there will be a tiled map uh either a raster map or a vector tile map. Um so if something changes when you do updates you want to figure out which tiles do you have to rerender or reprocess in some way and um that is what um OSM2gsql also can figure out for you and um store this information in a database table or in a file. Um and then there's the issue with cgraphic generalizations. If you zoom out um uh you can't show all the information of course you have to do some kind of generalization. Um and OMTPGS SQL have has several functions for helping you with that. So for instance uh you have here on the left um some forests maybe and they are mapped in a lot of detail to BGSQL can take this information render it raster map simplify the raster map um and then create vectors again from this simplified raster map. It's a lot faster then to do the same thing with buffers um in in vector space. Um and you can uh do very fast generalization of this kind of land use data this kind of polygon data. Um but there are some other um algorithms built in. So for instance um when you create a map uh it you have to decide which labels to show on which zoom level and uh so for um for places like in this map it is sometimes difficult if you just have a absolute measure saying show all the places that where more than a million people live or whatever it is and then you get clusters of labels in one part of the map and no labels in other parts of the map. If you want to have a more even spread um [snorts] there's other algorithm algorithms to do that like in this map that I created here um that are built into PGSQL this categoric generalization there's basically two ways of doing this um you can either um trigger some SQL commands um and do the processing in the database um or as I mentioned uh with this other examples you take the data out of the database, process it externally, uh, and, um, then, um, write back the generalized data into the database. And this is tied in with the expiry, the tile expiry that I, I talked about before. Um, so it can figure out that something changed in this area and uh, recreate the generalization for this tiles [snorts] that will be affected. This generalization is still sort of better and is not integrated 100% with oentpgsql but there's a separate command oent pgsql uh-gen uh that does this functionality but it reads the same config file and um once we get some more experience and and some more people using it we'll we'll uh include that and and bring it into the main o2pg program. So all of this sounds like a lot of work. You have all these many different configuration things you have to write um for every layer that you want to create, for every database you want to create. You have to write all of this. Um but in many cases there's somebody else who already wrote part of the configuration. Um so somebody might have written a um um mapping from OSM data to create a highway network and somebody else uh created something uh to have land use information or something like this and um there's this added theme park framework that is basically some extra lua code that helps you mixing matching different kinds of configuration if you uh uh have your write your config files in in a certain way. Um you can bring in several configurations and create some larger configuration out of that. Um so have a look at uh the uh theme park framework also that might be interesting. Um if you're using OSM2PQL or using open street mapap data in general please consider supporting open street map uh the open street map foundation or maybe some local chapter um and uh consider supporting OSMPGSQL. It is open source um and open stream is all open data but it's uh it still costs money to create all of this. Thank you for your interest. "}
{"url": "https://www.youtube.com/watch?v=b66yr1CtB4c", "title": "The Power Of PostGIS And Apache Sedona", "transcript": "Hey everybody, uh Matt Forest here. I'm excited to join PostJS day again this year. This is one of my favorite events of the year leaves a ton of stuff to watch after the fact. So the fact that you're here watching it live, kudos to you. And I think really appreciate you being a part of this talk. And today I'm going to talk a little bit about Apache Sedona and PostGis, how they fit together, how they're similar in many ways. They use spatial SQL and they can look very similar if you just kind of expected them on the hood from that perspective. But I want to talk a little bit about how they fit together um and how they can kind of work in unison and what they're good for, how they can, you know, kind of build a really complete geospatial ecosystem and how to think about that and actually do that in a short demo right at the end of this. So that's what I'm going be talking about today. A little bit about me. I've been in geospatial my whole career. I've been using PostJS for many years. Also using a lot of other spatial SQL ecosystems uh and things like that. And then um really u you know dug deep into Apache Sedona. I work at where so that's kind of the basis of the platform that we do. but also want to show how these the pieces fit together. So that's what I want to talk to you about today. So when you look at Apache Sedona and PostGis, you see a lot of the same things. There's spatial SQL functions. They use ST functions. There's a lot of the same things and and they're two of the best in in the business when you think of geospatial tools that use spatial SQL. Poss has by and large the largest number of spatial SQL functions but Apache Sedona has you know completely you know and and fully committed to providing as many or as close to those as many as possible in similar spatial SQL functions as well. So that's kind of what they've been building. Now the difference is when you look at them is you know in really two areas I try to break it apart from this perspective when I when I think about PostGIS and I've used it for many different things to to be clear. I've used it for processing over time. I've used it for um you know as a true database. I've used it in many different ways. Um, so I I I'm really love all the things that PostGIS has to to offer. But when you think about it, it is um really looking at it as a uh database for transactional real time data retrieval things like that. When I think of a database, you know, in in the most classic sense when you boil it down to that perspective, that's what that's what you have there. So, it uses all, you know, as you are well aware and I'm probably preaching the choir here. You know, it's battle tested, it's stable, it's many production systems, many of which we're hearing about today. Um, and it's the go-to database and has been for many years for GIS, right? It it provides the largest number of functions, high stability, lots of different ways to use it. But as if you've tried to push it to further limits for large scale data processing, pushing more data in, as you noted, the database, the compute and the storage is coupled, right? So those two things go hand inand if you need more storage you have to add it to the database. If you need more compute you have to add it there as well which is why as data volume has increased you know over the years different systems have started to come out. So we've seen you know uh systems like you know data warehouses things like snowflake, big query, Amazon red shift those have come out and those all use you know the distribution or separation of compute and storage as well as other systems like single node services like duct DB and Sedona DB which is a new one from the Apache Sedona ecosystem which I'll talk a little bit about later but Sedona what it is it's a distributed spatial processing framework so the compute and the storage are separate you store your data in a separate location generally in parquet files or geoparet You can read in different spatial file types from giojson, shape files, things like that as data frames and the compute is brought down to the storage which means it's completely separate. You can store the data in just a cloud storage bucket and the compute you spin up and spin down. Generally that's done with a spark ecosystem. Although you can use Apache Flink and some other tools as well, but generally that's how this works is that when you spin up or have a processing task, you start your compute. It takes it and it splits it out across multiple distributed computing nodes if you think of it that way. So instead of it being coupled, it's split up. And that's probably the biggest architectural difference here. It's not for data retrieval. You don't ask Sedona for, you know, go find me information about this specific row or this data or do massive inserts, updates, and deletes or stuff like that. So it's not meant for that. But it does have table based features in the format Apache iceberg, which is a cataloging service we'll talk a little bit about later as well. So when you think of it that way, you can think of it as an analytical engine for these massive distributed workloads. when the data gets really big or you have a massive job you need to run. We'll actually look at one of those in examples later, but that's the best way to think about these as two different systems there. Now, when I think about transactional, like that's where you can do thousands of short running queries, there can be many of them running at once. So, if you have thousands or you know, you know, tens of thousands of people accessing the same database, they can ask those questions. Postgress is meant to handle that. You know, it's very fast. You need that you know sub you know, millisecond latency like you need it to be get the data right now. That's what you know, databases are really excellent at. You have all the things from CRUD and asset built into that. So it allows you to do a lot of different things from APIs to tile servers to routing engines and all these different pieces. Those are like when I think of a transactional workload, that's what I'm thinking of there. When I think of like a analytical workflow, that's a longunning job that is over millions or billions of records, right? You have just a large volume of data that you wouldn't necessarily want to move into a database. You wouldn't want those same things running on the same system anyways. if you have a longunning query that could you know delay the transactional processing on the other side. So you kind of want to architecturally think about these as two separate systems. But of course why I'm talking to you today is they all fit together really nicely as well. So that's what'll be going into as well. So when I think about this I think about spatial ETL pipelines. want to create a pipeline that you know pulls in data from maybe disparate sources, prepares it nicely, runs the analytical workload and it can certainly pull into a transactional database to inform records in that other data system as well. On the reverse side as well, if you have transactional records that you want to pull into an analytical work analytical workflow, you can do that as well. So it provides that birectional component to do that and Sedona you know allows for AT to connect in and kind of for these systems to really uh interoperably you know play well together which is going to be really important. Now when I say like complex joins of course we all know and have all probably used st intersects or contains or any of the other spatial predicate functions in postis before when we have a complex join or maybe it's a large number of features or maybe you need to do something at a large volume um or the geometries are highly complex right these are systems that you can run it very quickly on you can integrate that with raster to vector join doing like large distributed zonal stats which we'll show an example of later as well and then that batch compute that's distrib distributed across many different clusters, right? That's a big thing. Now, you know, one of the big like scary things in all this is the Spark backend. And if you have set that up before, how do you manage some of the different pieces there? How do you distribute or partition data? These are all things that I think can be a little bit daunting at times like effectively what we do at warbox is try to handle that backend. So, it's a you know service that we manage. It's serverless and then allows you to actually distribute the data spatially which none of the other like spark based systems do is that we actually have these spatial kind of predicates embedded into the partitions which allows us to do things a lot faster than just a vanilla spark spup. So that's kind of what we do. It allows you to do that scale out processing and then you know transactional scale up. So that's kind of another way to think about this on those vectors as well. So again what are like the things that postgis does amazing that Sedona doesn't do right and it's important to call these out. I um really believe that you choosing the right technology for the right role or the right you know uh job you need to be done is really important and these are just some there's a full list you know that you can kind of start to break apart but dynamic tiling is is one key thing that I wanted to call out you know doing that from you know STS MVT creating real-time vectors at multiple zoom levels any of the routing or network analysis doing things like PG routing you know AAR dystra all the different routing algorithms that bring in with PG routing again Sona doesn't do that out of the high performance retrieval, right? If you need to, you know, full pull in very specific, you know, components of data that you can go under very fast response times, Postgress and Post.js, you know, are tuned to do that that very fast data retrieval and and in addition inserts, updates, adding triggers onto the database, all these things as records move in and out are really important. And Postgress and PostJS are tuned to do this the best. And and I think it's like it's one it's the best. And I've loved using it for all those different things again like database capabilities, your asset transactions, you know, row level foreign data wrappers, you know, these rolebased permissions, all these built-in things. Sedona doesn't necessarily replicate that, right? Sedona is not a database. So you can't have multiple people calling it at once. It's usually kind of one, you know, system accessing that from that perspective, one person, one request. You know, that that Spark isn't like a data system where multiple people start to grab data from it. It's really meant for one batch job and one purpose. So that's a different thing. and like all the extensions right if you think about H3 PG routing mobility DB you know even like time scale DB all these different pieces that you can extend with Postgress and PostGIS as well is another key thing to think about and then of course there's like advanced you know vector functions there's the raster functionality all these different things that add in and make it like one of the most fully featured spatial databases that is there that's a great way to think about it and even into just you know your you know connecting postGIS to your QGIS system managing it as a you know basically it can be your your GIS server and have all these capabilities built into it. It's really incredible to do that. So whether you're, you know, doing editing workflows, you're doing APIs, your transactional systems, you know, you're doing production routing, tile serving, you know, real-time lookups, all these different things are what I I believe, you know, postGIS shines, you know, heads and tails above anything else that's out there. Um, so that's really like those are the things that Sedona is not going to do and doesn't do. So it's just important to keep that in mind. So what are some of the things that Sedona does that PostGIS doesn't and that again picking the right you know tool for the job here is important. So when I talk about distributed processing, it basically takes a job, a query, whatever that that might have and allows you to distribute that over multiple compute nodes. You know, postgis and postgress is is single, you know, single shredded. So it allows you to say if I have 10 workers, 20, 100, whatever, how many of that is in the back end allows you to process that data in parallel, parallelize the joins, dissolves, buffers, whatever you're doing there. Same thing with raster vector data, all that different stuff. It works with cloudnative data like really efficiently. So you can leave that that cloudnative data on the bucket where it sits be that geoparet cloud optimized geotiffs things like that allows you to leave that data where it is and read it in as a data frame and you can do that within that in-memory kind of dataf frame operation and then write out to those formats as well. In addition, you know, we've added support for iceberg for geospatial file types. So it allows you to create an iceberg catalog on top of that which allows you to have you know transactional like kind of capabilities but for an analytical workload again you wouldn't want to like serve transactional capabilities all the stuff I talked before from an iceberg catalog it's a catalog for analytics that allows you to reference that. So, it has a lot of, you know, built-in partitioning and a lot of things you want for snapshots of a hens, that type of kind of, you know, history and stuff built into it, but it's not transactional, right? That's not really what it's meant to be. It's meant to support and make these types of processing workloads a lot easier. So, what like I said, these they, you know, they look like similar things, you know, that's why I've been using these images of like a, you know, two animals and and, you know, a roadrunner and a elephant. So, they look very similar, but they're they're quite different when you think about it. And again, raster processing at scale. If you needed to do, you know, NDVI calculations, map algebra across large areas, you can distribute and split those raers up across space and do those calculations very efficiently there. Now, again, I also mentioned like zero ETL cloud reads. If your data is on a bucket, be it S3, Azure, Google Cloud Storage, even on HTTP or HTTPS, you can point Sedona at the bucket and read that data in remotely and then allows you to read that in over the wire very efficiently. With the cloud native formats, it also allows you to just pull in some of that data as needed. So if geoparet has builtin, you know, bounding boxes and stuff like that, if the query only requires a certain amount of data, you can do a spatial predicate push down to the file, move in what you need, and then you're processing significantly less data that you do that. And again, autoscaling sparks and serverless options that allows you to do that specifically. And then what we've done at where is really to try to add the spatial compute all the way down to the individual node level. So you're distributing spatially not just based off of you know however Spark decides to do that. So again these large scale ETL cloudnative pipelines big data processing big raster processing raster to vector processing spatial joins and then into like things like ML feature engineering you know geoparet production at scale data you know creating you know kind of this like you know geospatial data lakehouse that's really where Sedona focuses in on that's the thing that it really wants to do and focus as a toolkit as a standalone service. Now, Sudonb is a new tool that we've been working on and and it's basically single node analytical power. You can kind of think of it like very similar to duct DB. You install it with Python and effectively download it that way. But it there's no backend in Spark. So, you don't have to set that up. You would have to set up a Spark server to use Sedona out of the box. With this, it is a Rustbased execution engine, which is, you know, kind of, you know, a very specific terminology that does zero copy with aerodya access and stuff like that. It's based on So it's based on Apache data fusion which is kind of the Rust engine of it and allows you to do a lot of the basics that you know you can do with Apache Sedona. So it's geoparet in and out. It's great for laptops even instances in memory type of things stuff like that even airflow tasks and transformation. It's really focused in on spatial specifically. So doing fast things like you know KN&N joins if you're doing different pieces you know writing and reading geo parket files. It has interoperability with geopandas and it can connect into postgis and pull in data or write data back out. So if you need that extra boost of you know kind of analytical power when I say analytical that's you know large scale spatial joins that's doing you know area weighted interpolation queries across large you know swaps of areas things like that even into processing tasks and and those components that you don't necessarily want to bog down your transactional database with sudonb provides a really nice middle ground to do that without all the you know setup of a full spark system so that's another component of these so like local prototyping allows you small to medium data sets airflow egg tasks, you know, even into ML feature generation at a certain speed. It's like a high-speed analytical powerhouse to do those different things. So, we're going to do one demo with Sedona DB. The other one, Apache Sedona running on whereabouts, which gives you the full Spark Bass infrastructure. So, Spark SQL, there's a data frame API, SQL API, which is where you get all the SQL functions in integrating with the, you know, data lakeink, you know, you can kind of work with Delta Lake out of the box, iceberg or just parket files if you have a pure data lake itself. and then allows you that unified analytics across that full data ecosystem and allows you to do that in a distributed format that horizontal scalability which is what we want to build in there. Now one more I want to call out is Sedona snow which is where Sedona meets snowflake and that is basically allowing you know Sedona like functions inside of snowflake. Now a lot of these um you know the so of course snowflake has spatial functionality that's built into it but it extends some of that as well with you know 3D functions you know adding the the zcoordinate or 4D functions adding the coordinate allows some of these different pieces there allows some different integration and capabilities snowpark integration for Python and Java workflow so some extra pieces it comes with if you're already inside of the snowflake ecosystem so it gets you know kind of Sedona's engine with some of Snowflake's elasticity and some operational simplicity is what it allows you to do so if you're you know already using you know snowflake if you're already using postGIS inside of that ecosystem how all these pieces connected you can get some of these and that extra boost here as well if you're already inside the snowflake ecosystem as well so just other options about how to use Sedona within that same mix as well now one piece that I wanted to talk about is iceberg interoperability and we just um wrapped up a course over at where talking about geospatial data engineering and using kind of the the multihop architecture is could also be referred to as the medallion architecture and and I've heard it referred to many ways, but basically it allows you to have like three levels of tables using iceberg back storage um that allows you to kind of create readytouse analytical data. So the the whole concept is kind of using raw data inputs be that from you know government files your own data maybe it's data from a data provider maybe it's streaming data you know it could be as postgs could be a source wherever that data is coming from you pull that into iceberg as a raw copy that's your first layer here was that your raw data table the second is where you do your silver or your processing your heavy spatial tasks maybe you're doing spatial joins maybe you're doing conflation to things like over mapaps data maybe you're doing like I said area weight interpolation raster to vector joins whatever you're doing that's heavy spatial processing you're doing it there and with the Spark ecosystem allows you to scale the right compute for that job right so you can create different jobs that you're running and you can choose a small instance for the small jobs maybe you need a heavy one for the large ones whatever that is but it's not all jobs running on the same compute which is really important and then the final step you're creating these analytical ready tables that can go into different systems one table might look different for a business intelligence tool one table might look different for going and connecting into an OLAP system like a snowflake table might look different going in back into postGIS which is what we're going to talk about today or it might look different going into you know maybe something like a GIS tool or maybe data outputs for an all who knows right it's it's data going into the purpose fit in the right specific way now why iceberg it allows you to do this and have that complete history with the tables as well you can do appends it has built-in partitioning lots of bonuses there's time travel so you can go back and see the history of the tables over time so you basically store it once And you can, you know, basically read and write iceberg tables with Sedona to build that out. And then with the geometry that's, you know, accessing, uh, you know, making that even easier. That's in the V3 spec, which has been released since this summer, but it's starting to roll into some of these systems now. So basically, Sedona reads it in and writes these iceberg tables. Once it's in there, it gets all these benefits of those built-in partitions, the history, you know, looks like a table, things like that. So if you're writing SQL, becomes a lot easier than using just a dataf frame API out of the box. It also allows you to use like table features. There's incremental processing. Like I said, time travel, schema evolution, those asset guarantees, all that stuff that you want on those tables is built right in there. And it all works on object storage. It's just data in S3 or Google Cloud Storage or Azure blob, wherever that is, it's in cheap cloud storage. And then you get into this kind of like zero ETL type of process like I can query that bay it back in Sedona, be that in Trino or Spark, Dreo, DuctTB, you know, even into we're adding support for Sedona DB and things like Snowflake. Snowflake allows you to read iceberg tables. So you can immediately just say here's a cypress table. You can immediately say here's this iceberg table I've created. I've done it efficiently and processed it and it's ready to go. I just connect that system and it's reading directly from that table. You're not moving files or moving data back and forth. It's a very easy like zero ETL step to connect that. So you can store your geospatial data once and then query it from any engine of your choice including and going back into postgs which we'll cover today. All right. So let's get into the demo. I've set up a postgis cluster here on crunchy. that's just inside of my US West zone. So you can see some of the different data that I pulled in. We'll show you exactly how I'm doing that here. And effectively now that I have this, you know, easily spun up PostgJS database, I can start writing and reading data to it. So you can imagine if I have some data in here already, maybe there's, you know, an application, you're serving tiles from this, whatever might be in here. You can kind of see what that looks like. But let's do two different processes to actually that. The first we're going to do in Sedona DB. Now Sedona DB is that single node service that I mentioned before. It allows you to connect in and read in lots of different layers here. You effectively you just install it with Python pip install. You connect it in and you allow you to do those different things. As you can see, you can just install it here. You create this connection and then you can start writing SQL and even connect to remote data sets. So that's exactly what we're going to do. Now I've imported Sedona DB. I've established my connection. As you can see, it actually has this you know geopando's interop if it's your data is not in paret format. So I can read in this this is building footprints in New York City. just create a data frame here and then once I have that I can actually start to query that inside of Sedona DB. I can create that as a view here and I'm also going to create a table here from the over maps data set. This is their places data set. You can see I'm connecting it here to the entire data set and then what I'm going to do is create a query that pulls in three important restaurant choices inside of New York City. So pizzas, bagels, and coffee. So I'm going to pull in all three of those inside New York City and then I will create that as a memory table which you'll see here. And then once I establish that that's a view. So now I have my two data sets my buildings and my restaurants my pizzas coffees and bagels. And what I can do is create a you know a KN&N query. So the K nearest neighbors here. And basically what I want to do is find the closest, you know, or the six closest of those restaurants to every building in I believe this is just in Manhattan. So there's like 1.8 million buildings or something like that. So for every building, I'm going to find the closest one and I'm going to get the average distance of those six, right? So if you look at the query here, have my buildings geometry is the average distance from buildings. I use the stkn function which is, you know, a k nearest neighbor query kind of simplified here. and then wherever it intersects this you know geometry or sorry this WKT which is just in the east village. So I'm just going to pull those that subset of data. So just for the demo you can do that. You can see the values here. You can see for this building the average distance is 101 m or 80 m and so on and so forth. And then basically what I do here is I'm going to pull that back into a pandas data frame or geopandas in this case. And then I'm going to create my connection. Then here I create my connection back to my postjist database using SQL alchemy. and I can just write that table back up. So that's what I've done in in in that's what I was able to do over here. Now this wasn't like a small join. This is like 1.8 million buildings to all this different data. Again, what's really nice is I can leave all the overture maps data over there in parquet. Pull that in and temporary data structure here and then run that query. And that whole query here took about like point4 seconds to run to do all of that and it scales up really nicely even if you add larger amounts of data or things like that. So it's a really fast once you do all the data steps there. But again, no ETL for this overture data. I can leave it there. Um, very simple to do that from that perspective there. Now, as you can see here, I'm over in CUGIS. I have my NYC food layer here. And I can style it. And you can see these lighter areas are where they are closer to those three different restaurant types. And you can see there's a few key stretches here. I believe this is like, you know, first or second, this Second Avenue here. And you can see these pockets where it's closer to those different assets and darker where it's a little bit further away from them. So, pretty simple query and nothing too complicated there. But again, I let the analytical big join in Sudon DB wrote it right back into PostGis and now I can use that and access that as a transactional table in terms of a lookup or whatever I want to do or serve these as tiles, whatever you want to do, you can kind of, you know, do that back in Post.js. Now, in the case of a like very large join, that's what we'll do here. And I'm going to take a look at this one in where I'm connected to Sedona. Sedona, I've spun this up on a cluster here in Whereabots. And what I can do here is I've actually connected to this data set called hand which is height above nearest drainage. So this is data that is raster data on an S3 bucket. It's open data and it effectively allows you to see uh normalizes the terrain model that gives you height along a drainage network to drive like you know soil gravitational potentials. It's really interesting you know to understand things like flash flooding and features like that or any sort of flooding from that perspective. It's about a terabyte of raster data. So not something you're want to download or pull in and it's global coverage here as well. So what I can do here is I've processed that data. I can read it directly as a stack catalog. Read that into iceberg with whereabouts and arisadona. Pull that in. And now I have this table which represents my raster data here. And you can see that I've I basically pulled it in. It's about 1.1 entry or 1.1 million entries or I've split it into smaller tiles. So it's a little bit more efficient to process. And that allows me to do it from from that side which is great. And then I pulled in every building from the overture maps data set here. So we've pulled that in. You could read these as data frames yourselves, but we've done the work to kind of split those in tables and buildings. So in our open data catalog, we have that as buildings. And then what I'm doing is doing a zonal stats join. I'm pulling the average value for each building or every building in overture in a 30 m buffer. This is just using the distance spheroid. So that's what the argument there is. And then I'm pulling the average to get that in. and false just says if I'm wanted to see, you know, pixels that full are only pixels that are fully contained inside the buffer. So, I'm including all of them in this case, but you could turn that off. That's just a spatial join. Instead of ST intersects, I'm using RS intersects, and you can see what this looks like here. So, here's my building ID. Uh included the height, you know, feature, but some of those don't have them. Here's the geometry, and here's the height above nearest drainage. And then I ran that process across every single building, which is about 2.5 billion buildings across the entire globe. This is using I think this is like a large runtime on whereabouts. So it took about 1 hour to run to process for every single building that we have here. And then what I'm going to do is write a subset of that back into postJS. I don't want to write the whole thing. Again, this is an analytical data set. I can lead that result in iceberg and I can connect that into the systems that I want to strategically. I'm using a library called Wickls. A team member that I work with called named Prennob. He built this out. So it allows me to actually use basically well-known locations. Wickles for, you know, specific data using the WKT from Overture. And so what I'm passing in here is just this function that allows me to grab the well-known text for New Orleans. So I can just use this function. Now I have that WKT. And all I'm going to do is pass in get the height, the elevation. I have to still just turn this into E WKB. So that goes into post as well binary. And then what I can do is basically where that intersects that bounding box or that WKT polygon. So that's what I pulled in. I just have to build my JDBC connection. And then here I uh write it using the overwrite feature using my JDBC URL connection and the connection properties. And I'm going to pass it to a table called nola which if I go over here you can see now I have that table. I just have to translate that geometry from a w sorry from well-known binary back into a proper geometry which I have here. And then we can load that on the map. So here's that final map. Once again, this is processed for every building in the globe. So 2.5 billion, but all I care about is this subset here. And what we can see is that even within New Orleans, which is a relatively flat area, you have these different sort of areas. Some are a little bit higher and some are a little bit lower, but you see these different drainage areas. Even here, the central business district, but as we go along some of the different streets, see what this looks like. And across the city, there's these, you know, light differences across these different places. But again, I can do a heavy process over here. I can pull in some of that data. Maybe it's a system for an insurance company that wants to understand, you know, sort of elevation and risk across the city of profile or anything like that. You can do these back and forth processings connecting in postGIS and doing it really efficiently there. So that was what I wanted to talk about today. Hope this was helpful to see how these things fit together really seamlessly and really fast and easy. I built this whole thing from starting my first post instance all the way up to finishing the process in about an hour. So, it's very efficient to get started and do these different things. So, I hope this helped explain and show you how you can do that. And yeah, it's been, you know, always a privilege to join this audience. "}
{"url": "https://www.youtube.com/watch?v=kNFyLdNzvGg", "title": "Your Dorky Spatial Database is My Magic Answer Machine", "transcript": "Uh, any other items, Paul, we need to cover before we dive in? >> Well, you're this guy, uh, Brian Timny, currently a senior developer relations engineer at IQGO, but longtime Post Day attendees. We'll know him as the unapologetic spreadsheet evangelist and notable KML deadender. Brian has two great >> KML is an official OGC format, please. And in a world where shape files still exist, I'm not going to apologize for the mixture of data and styling in KML. Um, we've already gone off track, so I'm just going to jump in, Paul, and get started. >> And, um, users or viewers will note my, uh, rather aggressive title. And this stemmed from um telling a tech adjacent friend uh about my participation in postgis day and how you know it's fun you do live demos you risk catastrophic embarrassment if your demos don't work etc. And then I mentioned that, you know, the videos of past postGIS day presentations are are online and this tech adjacent friend said, \"Wait, there's videos of you on YouTube ranting about SQL for like 25 minutes straight.\" And and friends, I took umbrage. I took umbrage because first of all, I'm not ranting. um emoting enthusiasm. So that's clarification number one. Clarification number two is spatial SQL is awesome. So that got me thinking to a larger issue of language and how we fail to update our priors if you will as technology changes but our dusty musty concepts of words stay the same and I would say that is very much the case of database so for 99% of the world the database is an extremely tedious, boring concept. But for us, for us, we all know PostJS, Postgress is not just a passive storage mechanism for rows and columns. It is a dynamic platform, a dynamic platform that helps us glean knowledge and insight from said rows and columns. So, let's let's acknowledge that um less enlightened folks have an antiquated legacy definition of database and that ours ours is the evangelical mission to help correct that. So, I'm going to do two sets of demos today. Um, and what better way to demonstrate the dynamic nature of Postgress and PostGIS of being a dynamic platform for data data analysis and data insight than to hooking it up specifically to the context of it begins with an A, it ends with an I, and it's two letters. Can you play Wheel of Fortune? and we will be doing AI. Now, for those who are watching live, we just had a presentation with Claude. Claude will make a reappearance. My bold prediction for 2030 is that Claude will be the most popular desktop GIS software. That's my prediction anyway. But I have 25 more minutes of presentation. So we will do a claw demonstration and then we will take it up a notch. That is the plan of the day. So before I tell you about the great um great technology of MCP, let's just get Claude going on our local database. So I'm going to say please tell me about my Postgress database public schema and what tables I have. And so now all of these demonstrations you'll see today are on an extremely underpowered cheap Windows laptop with a street value of $71. And so there's no GPU. It's uh minimalistic uh CPU. So it'll add an extra layer of excitement. So what's going on? It's connecting to my local database. It says I have geographic data tables and that's awesome. Um, and if you guys remember last um last year, this is my postGIS day presentation from last year. All the tables, I still have them. Just go back to the same database. Why uh keep it complicated? All right. So, uh, look in the France table and give me the five most populous departments using 2008 data. All right. So, why don't we while Claude's pondering that, why don't we go ahead and um talk about MCP? And I assume other people have con uh talked about this. This is model context protocol. And you can see there's a bunch of lovely people on YouTube who are ready to help you out with that concept. Basically LLMs need context uh to understand specialized tools. So the Postgress MCP provides a context for which it can understand what's in your database and even uh do some queries. So oh look at this I have I have data straight out of my uh thing. Whoa. Whoa. Okay. you just can't give me answers. Uh like show me the sequel because like we're all SQL people here. Um and the idea of in our deterministic uh all right so it selected all the fields but it did choose popcore 2008 to give me 2008 data. All right, I'll let that stand. All right. Uh, next question. Uh, the abetantes and 2008. Now, for those of you uh paying close attention, uh that that's Spanish, Spanish with a Philadelphia accent, I'm sure it resonates um around the world in Spanish-sp speakaking countries. But the idea of MCP plus Postgress, we have foreign language support out of the box. And here is the interpretation. And there are our our five departments with the least population in France. So we have MCP, we have claude and now we can talk to post, we could talk to our Postgress database. We saw DB browsing and it told us our tables. We did a basic select. We did multilanguage. Now let Claude just grind on an infographic. Claude, we need an infographic [snorts] with the most interesting insights from the France population data. Now, doing a um an open-ended uh prompt like that to Claude, it gets at the uh what has already been covered in other presentations, the probabilistic nature of LLMs. Those of us who work in the data world, we like deterministic things where repeatable processes that are audible audited can be audited. Better yet, um, and where steps uh happen the same way each time. Now, for instance, when I was, let me expand this. When I was doing this particular demonstration, um, it used Python and Mattplot lib to come up with a extremely cool graphic and now it looks like it's just crunching on a bunch of HTML. So that's a polite way of saying I have no idea what this final graphic is going to be. And again, it gets to the heart of when we're using LLM's kind of solo without supplementary tooling. Um, it can it can do all sorts of unexpected magical things. Let me open this in Google Chrome and see what this infographic looks like. Wow, look at this friends. Fastest growing departments, declining departments, population extremes, key insights. Wow. All right. Well, if you like summary statistics, you'll love my infographic. So, that's kind of awesome. And again, this is Claude plus just the the generic Postgress MCP. Tell me something about my database. Answer some basic questions. Give me some SQL if I want to audit exactly what you think you're doing. And then generate a nice graphic uh with me expending exactly s zero energy. Um your sales and marketing team will love that especially with that level of effort. All right, let's move on. What's what's next on the slides? Uh slides. Slide slide. Ah, friends. So, you know, Claude, we saw the interface. We saw me typing in generic stuff into the interface, some questions. So, that's me, the interface interacting. We have a connection to my local host database. So, I understand how all that works. What if I told you that just like in any good horror movie, the call is coming from within the house. And what I'm speaking of specifically, friends, is a tool created by our intrepid host Paul Ramsey. And this tool pgsql- open aai lets us access a via HTTP APIs to our favorite language models directly in the database. And now you're saying no way that's impossible. Well, the idea of making HTTP API calls from within a database, that horror film trope, that's not a casual metaphor that I chose, friends, because for many people, the idea of just managing network traffic inside your database is anathema. But we're intrepid explorers. So, what's happening here? We're using Paul's HTTP tool to communicate with the outside web world using um using the libraries he created, which is a polite way of saying if anything goes wrong here, it's all Paul's fault from here on out. Claw did its job. We'll see if Paul can step up. So, I have installed locally on my severely underpowered machine. There's your Windows command prompt. That interface hasn't changed in 25 years minimum. So, you can see I have four models uh downloaded locally. These are all open source, free to use. And then I have Deep Seek Cloud. My laptop isn't big enough to host DeepSeek. So, I am connecting through it through OAMA's cloud infrastructure. We will see that shortly. So, um I got this uh small m prompt and I'm just going to run a select statement from my I database IDE within within my postgis database. This is crazy. Now, the first thing you know notice about my underpowered laptop is when I'm hitting the local model, the response time is uh how should we say leisurely. Um, and I will remind you once again, this demonstration went perfectly um before this presentation started. So, we're going to wait 10 more seconds before we get our time out. But every experience is a learning experience. And I'm just asking it. What are the three colors found in the US flag? Right. Oh, we got time out. So, our local models on our laptop that leaves much to be desired in the performance ain't going to work well. All right. So, let's go to the Oama cloud option. And again, I'm just setting my parameters. So, my um so we're just changing the model we're hitting. So, we're hitting deepseek friends. This is cutting edge. This is the open- source, one of the open- source Chinese models that could possibly undermine the multi-billion dollar US-based LLM industry. But, you know, let's live dangerously and let's change the prompt. I am a GIS analyst who is trying to disambiguate US cities with the same name. Where is this US city named Portland associated with the lobster industry? Respond only with a valid GOJSON point object representing the geographic location. Can Chinese open-source technology give us an answer to this? Friends, that response came back from Lama Cloud in two seconds. Let's just copy it and then let's let's go and verify this fragment of of Giojson that was sent back. Okay. So you see the format of how it got sent back. And friends, let's delete that. Oh, not Portland, Oregon. Friends, Portland, Maine. An opensource Chinese LLM model could disambiguate Portland, Oregon from Portland, Maine. This is amazing. We live in a days of miracle and wonder. Let's go back to our database IDE because I want more of this. Let's up the Annie. I'm a GIS analyst. Blah blah blah. Where is the US city named Frisco most associated with the skiing industry? Only give me a gojson object. Friends, this is a what we call a hometown question because we have Frisco, Texas. We have Frisco, Colorado. And of course, at the risk of intentionally in offending our friends on the West Coast, we have San Francisco that is often called Frisco. Friends, it's zoomed right into Colorado. Frisco, Colorado. So, the idea is twofold. LLMs are great at parsing language. So if you need them to disambiguate and give some uh reasoning about hey what do you really mean what location it's a great option and then secondly secondly LLMs are really good at formatting text and giving you responses and this gojson approach uh I stole from Regina O at Fos 4G few weeks ago when she presented it and I'm like great let's let's just get gojson back and the great thing about it is since we're making these requests from inside our Postgress IDE if we were so inclined we could just take that data we could do a simple reax query to make sure we're only getting the JSON fragment and we could um use it as a spatial object, convert it to a native uh Postgress WKB geometry, etc., etc. But friends, let's up the Annie like giojson of point locations kind of cool maybe. I don't know. But you know what's cool? What's cool is if we have images that we want to analyze. So, we're going to go we're going to change the model that we're hitting through Paul's wonderful tool that has performed quite well. Let's see what open AI what model we're hitting. Open AI GPT4- Turbo, which sounds awesome. Okay, what what do we what's the task at hand here? Well, uh, let me find the correct window, which is not clear to me on my layout. Uh, oh, there we go. So, we have this image and it's on the internet on one of my websites. So, it's just a JPEG sitting out there. Can we send a request from inside Postgress to tell us what is inside that image? And you're like, all right, let's let's give it a try. GPT4 Turbo and we are watching the response. We are feeding it the prompt. I need you to analyze the image at the provided URL. We're giving it the URL and then, oh, the image features a standard red red stop sign prominently in the background situated against a clear blue sky. Blah blah blah. Friends, that's pretty cool. My image lives elsewhere on the internet. GPT4- Turbo can go out and then it brings me back information about it. Okay, whatever. That's a picture of a flag. Who cares? All right, friends. This is the final the final trick. All right, I have this 177 kilobyte JPEG of the uh urban rural interface in Arapjo County, Colorado. Uh this is the end of the subdivision. I have a red polygon around a suburban house and then I have a blue polygon around a field. So, is the current state of AI technology able to tell us something intelligent of this not very high resolution JPEG 177 kilobytes? Can it give us back useful information? Let us go to the prompt because we're going to have to be a little more verbose this one. All right. Tell me the man-made structures within the red polygon, then the structures within the blue polygon, and give me the result as a JSON object. For the red polygon, add an ID property to the JSON response with a value of 33203. For the blue polygon, add an ID of 9903. Here's where the image lives. And use that same model. Friends, what is going to come back when we do something like this? Let let it uh I'm waiting with baited breath. That's b a t e d. Okay, I'm going to copy this and then put it in a uh wonderful word interface. Okay, here is the response. Now notice like I just wanted JSON. LLMs can't help themselves. They give you a bunch of text before they give you the JSON. But here's the deal, friends. Red polygon, it tucked in that ID for me of 33203. Gave me the structures, gave me the blue polygon, and said there's no structures and area appears to be a baron field. So imagine you're in an assessment office and every year you have you have a certain number of parcels like did anyone build anything on it? You might have some permit data and you could generate these aerial views with the parcel boundary and send it to an AI to get to do the analysis and give you structured data back. structured data that comes with ID tags that you can then link back to your source parcel data. Friends, friends, here is what I'm going to leave you with. We already did that. We used uh Alama models, open source, both local and cloud. local didn't work because I have a bad laptop underpowered. Open AI did the image analysis. Here is my go-to visual metaphor of where we are at currently with AI. Um, most of us are in the bottom leftand corner uh watching some with cameras all of us expecting something mostly catastrophic to happen. Uh we have a single person actually on the wave of AI surfing. It appears this surfer, he or she is going to do well. And then of course in the background you see a couple people on wave runners. Those would be the upper management of your organizations that you work for cheering you on and urging you to catch that wave of AI. And with that, I thank you for your attention. That's all I got. Paul, >> that's it, [laughter] >> right? No Excel. No Excel. >> Oh, that's true. No Excel. But prediction that claude is going to be the GIS of the future. I was could it finally become true that metadata is actually going to be important >> that so this is this is my dearest hope. So as you know I work I evangelize for the APIs in the network utility and network telecom space. So, um, and I don't think I'm breaking news by saying, uh, telecom and utilities have some of the most problematic data quality anywhere in the geospatial landscape, right? But again, cleaning up your data, that sounds like Saturday chores on steroids. Will AI be enough of a payoff for organizations for them to invest significantly in data quality? That's that's issue number one. And will AI help the way data is collected in the first place to ensure data quality standards as it works its way into our postGIS data stores? That is the hope and dream that sustains us all in these otherwise dark times. Mr. "}
{"url": "https://www.youtube.com/watch?v=jv7O0a5YKgg", "title": "PostGIS In the Age Of AI: Teaching Claude To Think Spatially", "transcript": "Thank you for joining everyone. This is Hi, Miss Sanchez from Felt. Let's just give it a couple of seconds while we get everyone here. All right, let's start. Um well, welcome to to felt session for postgis day um 2025. Today we'll be talking about uh how we thought or how we use clot uh to do geospatial queries in posters and uh with me I have Mamata. Uh so let's just first uh mam you want to go and introduce yourself and then I I'll go. >> Yeah. Hi uh thanks for having us. My name is Mamita Kella and I'm head of cgraphy here at Felt. >> And my name is Haime Sanchez and I'm the director of partnerships uh at Felt. Um and me and Mamata go way back. Uh we've been doing just facial for a while. So it's really nice to be back together uh for this webinar. Um all right, let's get started. So for the first uh uh bullet point here, the first thing we'll do is do a felt overview uh what the company is and how what problems do we solve. Then we'll talk about how um what are the common pitfalls in in for LLMs when when it comes to spatial. uh then we we'll do the most exciting part which is a demo of felt and our SQL AI and how we can query posts and create really complex queries with a couple of of uh just natural language lines. Uh and then I'll explain what we've learned on on how we should you know best practices of on on how you should think about um when you think about um squaring posts with an LLM. Um let's first start with um the problem that felt solves which is building spatial apps um and um doing that fast. Um and in the in this image you basically see the process that takes uh that takes most people when they do spatial apps um to to actually get a product out of the door. uh it starts on the left with you know the raw imagery the raw vector data that you're starting with all the way to the right with the production geospatial app and what we see is that there's different uh personas or different roles uh working together on the left you you have data scientists GIS folks data engineers that know geospatial uh but maybe they lack the web development and the UX uh on the middle uh you have developers and designers that have never done any uh geospatial SQL. Maybe they've done SQL but they haven't interacted with geatial SQL like postGIS. Uh and then you have business users that are just stuck waiting for for weeks or months on on you know getting their answers uh to their to their questions. uh and you can see the different processes of how you know this is an iterative process uh from creating a a table to creating a query to a tile set down to a web server and you know just having an application um and again this also requires a lot of collaboration between different teams which is which can be hard it can be something that is stuck in GitHub or stuck in presentations uh and you know there's just needed to be a better alternative for that uh and We we've been promised a lot by Gen AI and and AI. And I like this meme because it represents a lot of the pains that we've seen in the in the out there in the in in in some of our customers is yeah, my AI just wrote 10,000 lines of code in 2 minutes feeling great. But uh now I'm going to spend two days debugging it. So, you know, it's uh it's it's a lot of it is great, but a lot of it is also uh you know um not as much. And so why is this? Um MIT calls this a geni divide. Uh and basically what we've learned from that from this report that they put out is only 5% of companies actually are seeing a return on on their geni projects. Um and the one of the biggest gaps really is is tools are not integrated very well. So um chat GBT or cloud or your LLM of choice is just really just not integrated well with your database and this is what we're going to talk about today. Um, and to wrap it all up, um, we have the felt platform, and this is why, um, we've created Felt to enable collaboration to build applications and dashboards fast and in seconds, um, to connect to pretty much any database u, and allow for collaboration. And so we're a platform that's cloud native, AI native that integrates natively with the Postgress SQL ecosystem. And also uh since we're here today at the post shoot state by snowflake also uh reminding folks that we uh interact natively to snowflake as well. Um also um wanted to make a big a big note a big um mention of how we see the the modern GIS stack and and for us this is something that it it couldn't be we couldn't be here if it wasn't for the giants that postress are as well as others like TPU and QGIS and G valve. Uh so we ourselves use a lot of these uh most of these or all of these actually in our own stack. Uh and again we couldn't be here without without them. Uh and a special mention to post just because really it's it's a big enabler for us in how we do geospatial. Um so one of the things that uh felt takes is a very like invisible AI approach to how we do um our design uh in our product. So rather than being very buzzworthy about it, we like to introduce um you know cloud and because we use cloud in in in just ways where um our our users really need it, not just throw throw it at you uh just for the sake of throwing it at you. And so there's three areas where we do this. First one is when you want to just extend the map and and write some JavaScript. Uh the second one is the one we're going to talk about today is SQL AI when you want to write a query against postes. And the third one is just styling when you want to style a map and add a popup for example. Um and then something that we we found um as common pitfalls when we were starting to do the journey of SQL and AI uh was that in spatial there's like three big areas that we saw a lot of um the LMS failing. Uh the first one is just um understanding the right geometry column to use. Um so boundaries versus centroidids or maybe the wrong table like admin areas versus grid cells. Uh so schema understanding is a big one. And uh the second one is just uh natural language can be very ambiguous when you think about spatial um near overlap inside along right all these um code some space into them but maybe the LLM doesn't know what you mean when you say inside or overlap and so it doesn't know which if it's std within or st intersect so that that can get messy. Um or also uh just an example of that is is a Mississippi estate or is it a river? Um and then the other one and this is again not an exhaustive list. This is just the top top three that we found uh that we're sharing today is just um when you when you're talking about a spatial problem uh like for example count the customers within a 10-minute drive of any store inside this metro area. This this requires um uh a multi-step logic like you can't just do that in one query or if you can uh it's a really really hard thing to oneshot. So uh complex multi spatial logic is again a place where where LMS can fail. Uh and um with that I will say one thing before we we go to the demo and we I share the screen to to to Mamata is that our philosophy at felt is is one that we don't want to overengineer the prompts we do to LLM because um they change every months right every three months there's a new anthropic model a new open AI model so baked into our philosophy is that you know these models will improve So don't overengineer the problem because we're actually going to get a lot of benefits by by waiting uh for some of these. So um we'll go through the demo and then we'll explain how we think about uh teaching claude and and how we actually did it. Um so over to you. I'm going to stop sharing my screen and you can share yours. >> Okay. Thank you Himeay. Um, so yeah, we're gonna go through the process of using uh felt AI and specifically SQL AI to query a bunch of layers to help us understand um where we could potentially place EV charging stations in the city of Seattle. So for this analysis, we're going to use a few different um layers that we have. uh one is demographics at the block group level to understand um some commute patterns a land use layer from the city of Seattle that is uh just land use classification because we want to locate commercial areas to place our charging stations. Um we're going to look at uh how areas that have a lot of traffic flow. So people that are commuting see these charging stations. Um and then we also don't want to place our charging stations near um other EV infrastructure. So we're going to use uh this level three chargers um layer. Okay. Hi May. And before I uh start with our queries, I'm just going to share the map with you so we can both be working on here together. So I'll just invite you as an editor. Great. >> And there you go. And we should see Himemeay pop up on this map um as well. Hi. >> Okay. So, we're going to go uh to the felt uh library and this is where you can do your uh source connections. Um here we're connected to this uh Seattle demo that we're looking at. Um, and what you can see is I have uh all of the tables that we just looked at. So, um, we have census block groups that are, um, polygons. You can see here all of the attributes including, um, the GM and a bunch of demographic variables. We have the EV charging stations um, to understand our competitor location. Um, the Seattle land use um, again with a lot of just zoning information. Um and then finally we have the the flow counts um the lines that we saw styled on the map based on commuter flow. Okay. So to get started what uh many of us do I'm sure is when we're thinking about a problem the first thing we want to do is kind of understand our data. So, you know, we can look at things like uh metadata to understand what all of the attributes mean, but sometimes uh attributes have cryptic names and it's kind of hard to understand what we have. So, this is uh one way we can just start to um explore what we want to do. Um so, I'm just going to click here on the new SQL query. Um and this brings up our uh AI assistant. Um, and here I have the prompt window and here's where the code will be um, generated. So the first thing I'm going to ask is uh what attributes um are in the demographics table to help understand commuting patterns commute patterns in Seattle and click enter. And what you'll see is it's thinking and then it'll give us an answer very similar to how you uh interact with other LLMs. Okay. So that's great. We have a little bit more information about um the attributes. And now I want to uh do a more a little bit more of an advanced analysis between two tables. Um, so I'm going to ask it to find all commercially zoned areas that don't have EV charging within uh one mile. So in the previous um query, it or in the previous question, it didn't write a SQL query um because we were just asking it about our data. But now it's going to um actually start writing a query. >> Yeah. >> And we're >> Yeah, I'm just going to fix this error here. Sorry, Himeme. >> No, that's fine. And actually just to comment on this, this is exactly what we mean when we're teaching cloud how to think spatially because what we're doing is when we're fixing the errors, we're like putting in the error message plus sending what we call a schema tip. So it's not just saying, \"Hey, fix this error.\" It's saying, \"Fix this error plus you have all these columns available to you. You have all these functions available to you.\" So it's actually by design. uh and it it fixes it really quickly as you saw from from clicking twice there. And another thing that's quite interesting to note here is that not only it used um std STD within but also it's doing ST transform because it knows Seattle's latitude is actually quite high and it prefers to to use um uh wet mercader or EPSG 3857 as opposed to the the the the bridge the prediction that's currently stored in which is WGS. as uh 84. Uh and this is one of the things that's coming through when we when we uh were prepping plot is you know uh if you use degree of longitude you know this varies dramatically with latitude as we all know. So and Seattle's latitude which is around like 47 north one degree of longitude is only about 69% I think of the distance of one degree at the equator. So, this is why it's important to do these transformations and why we won't really have to think about them because cloud thinks about them for us. >> Okay. And so, here we're um loading in our uh result and what we should see are all of the commercially zoned areas um that are not that don't have EV charging within one mile. So, we'll just uh >> Nice. I see a couple of interesting things already like this one in the the north of downtown where I'm drawing a circle. Um or also in the south there's a space here clearly for for EV. But we're not done yet. So what else can we do to enhance this al this yeah this algorithm or this analysis? >> Yeah. Okay. So, I think what I want to uh have next is just regardless of our final analysis or our site selection, like we just still do want to know a little bit more about um demographics in the area. Um and so we have this demographic table that we saw earlier, but all of the um all of the attributes in there are totals. And what we really want to know is uh what percent of the population uh commutes by block group. So we can make a cororoplath map of that. Um and so I'll just ask it to do that. Uh and so it's going through looking at all the columns we have and writing the case statements that we need to um calculate the percentage. And so again, it understands like the uh function specifically, so it knows exactly what's wrong and how to fix it. Um, okay. And so now we have a um new field percent commuters. Uh, and we can go ahead and uh symbolize that. So I'll >> click create the layer. And while while it loads, we can do the next one. and I'll I'll wait for it and I'll style it uh just to show how collaborative felt can be uh in in this whole process. >> Okay. So, we've done a couple of things so far. We've uh found some um commercially zoned areas that are not that don't have EV charging infrastructure and also created a demographic layer that we'll use um in our final map. And now we want to do a more uh multi-step uh advanced analysis. But before we run the analysis itself, another great way to use uh felt AI is to um make a plan. So I'm going to ask it using the tables I have, how can I determine ideal locations for EV charging stations? And here what it's going to do is uh kind of work through a plan with me and give me some um ideas again looking at the tables and uh coming up with some ideas. So this is just like >> and this just sorry mama to to add some context here like we found that asking and I think this is a general approach that people have found is just asking it to create a plan like this works wonders when then you ask it to write the query because uh in the previous steps it was just like oneshotting a a a multi-step query and it wasn't getting it. Uh most of them don't get that. So planning is key. And this two two-step approach can improve success uh on this type of logic. [snorts] >> Yeah, that's a really great point. It's like we're building off a conversation now. Um okay, so I'm going to go ahead and now ask it to uh write the query that's going to do this. I'm going to still give it um some information. uh and you can be as detailed as you want but I'm going to say okay so now write the query for the uh site selection analysis um to identify the best commercial areas in Seattle to place EV charging stations. So, just giving it the goal again here. And it did go through and see all of the layers, but I'm going to remind it um I want locations uh that are near areas with a high number of commuters, have high traffic volume, and uh won't have competition with the competition. with existing infrastructure. Okay. And here it's again just going to tell us what it will do and um it'll start writing the query for us. >> So I can see it's doing many sub queries which is uh is great. So you have multi multiple tails there and then it's going to do a group by at the end. >> Okay. And then it's going to tell us that >> again. >> And then uh once this uh finishes, I'm just going to create a layer and we can look at it on the map. Great. All right. So, we have um our layer there. >> Yes. Okay. Uh so, let's go ahead and um style this. But before we do that, we can see uh if we had a live connection, we could actually uh set a refresh cadence. Um, in this case, you know, we don't need it to refresh. Um, and also what's nice is you can see the uh SQL query back here. And if you want to make any updates, you can uh do it here and uh save it again to refresh it. Uh, okay. Hi, Mesa. Let's see what we got. I'm going to uh do this by uh categories >> and um let's see what I think it said. It gave us something market opportunity. Um, and let me just symbolize this so we could see it a little bit better. And there we go. So, we have now uh areas um in the in the bright pink um that are the most promising areas for shopping centers for uh EV charging. um we have good opportunities um with some competition around us in the orange and then the purple um are more high competition or moderate competition it says but I'd classify as high competition. >> All right, that was a great demo. Um that was great that we were able to find with a couple of queries in natural language where to put EVs in Seattle. That's nice. Um, and I personally really like just the the way we're able to do that collaboratively and uh, you know, in one map be be able to both do do our own quering, do our own styling and come together into one uh, one solution there. Um, then we often get asked how does all of this work? Uh, well, at the core of it, there's a system prompt that we that we build into this so that we're able to get the the most out of post. Um and so um when we think about this, we go back to the um the pitfalls um that we saw a lot when we were quering LMS for spatial queries. Uh and and we'll share today not all of them, not all of the solutions, but the three that really uh improved our success of quering the most. Uh and when we say success, how we measure success is um is is not really on matching exactly the the the example query that we have like the example truth query. It's just uh it's matching the data results because you can imagine if you've used posters before, you've done any SQL quering before that you can arrive to the same conclusion in different ways. So really what we're we're how we compare it is with the results uh of the query. Um and the first thing is is feeding in spatial knowledge. Um LMS's love tools or love functions. So the whole repertoire or of post functions that you have available. This is something that they already know because they've been trained uh with SQL and just with a corpus that includes poses. But we reiterate uh you know our our system with them. Um we also do clear guidance on geometry and and geography. Um we do a lot of like finding what is the right um like geometry column to use uh based on the rest of the context of the of the table. Uh then uh have we also note on the CRS expectations of like EPSG versus projected. Um the other one that we do um and this is quite interesting and actually drove a lot of the success up is to describe tables in geographical terms. So whenever you connect a table or like sorry a connect a post a postrest um schema uh when we scan all the tables and the variables for each table or the columns for each table what we do is we ask the LLM to also describe that table in uh geographical terms. So the roads table is not just a road tables with its variable. It's a Beijing road network with a live stream live stream GM uh describing that road network. Uh the county counties table is not just county tables is a is a Arizona administrative boundaries with a polygon gome and the name gome. Uh and and and and you get the idea. So we we do a lot of that and that helps us a lot uh down down the line. Um we also mentioned the geometry type and the region covered that that's also something that we said in the example and highlight the primary geometry uh if multiple exist. Uh and the third one that you you likely saw in the demo is basically use the execution feedback and mainly this is when we find an error uh we basically capture the the DB error and convert it into into what we call a schema tip. So let's say there's a an error like hey the column G on poly does not exist uh table uses geometry or something like that instead. So we we pack that into a tip back so that when we so that we are able to reprompt the LLM very easily and we find that whenever we get into this the second time around it works all works like a charm. So those are three big ones that we use uh in the system prompt. And that's been uh you know our session on how to help claude think spatially. Thank you so much. "}
{"url": "https://www.youtube.com/watch?v=P58b50_MQNI", "title": "Optimizing Sales Distribution and Routing with PostGIS: A Case Study from Kenyas GSM Network", "transcript": "Good morning, good afternoon, good evening from wherever part of the world you're watching this uh presentation. My name is Adam Kipk. I'm from Kenya. I work for a telecommunication company called Telecom Kenya. One of the leading uh telecommunication as a geo special manager. uh I deal with mostly mapping and and analytics data analytics on the organization. So today I'm going to take you through how we harness the power of post to optimize sales distribution and to reduce GSM churn here in Kenya. Main po point of focus will be optimizing SIM SIM cards where we call starter pack also we also also refer to as starter pack starter packs in other part of the worlds an airtime vulture distribution due distribution through your partial intelligence as a sales team I work for in the commercial which is sales marketing and also help in roll out of the network. But in the sales in in our the sales teams in the organization normally faces various distribution challenges which results in sales distribution inefficiencies and so we the same our our current JSM sales teams faces significant challenges and they've been outlined as you can see majorly there's a lot of uh there's is an issue of stock unavailability where we look in our in this study we're going to look at SIM card and vulture outages in given sales clusters and when we look at a sales cluster that is the most granular form region or area of administration in our sales company and is manned by a sales agent called sales commander So these commandos normally have manual routings just wake up today I'm going to do this route and there is a tendency for them to be stuck to a given profitable route leaving out other areas that they deem unprofitable or not lucrative and served. So if there is inefficient routing, inefficient routing planning leads to wasted time and resources by even sometimes serving unassigned routes. All this is due to lack of special intelligence. So there's been for a long time most organization have not been embracing the power of spial data to make informed decision and therefore uh the sales teams we've ensured that they there is at least a facet or a a segment of our sales commandos or managers who embrace and use this special intelligence also in the markets. They face a lot of poor visibility because of uh areas that are not well branded or you can say that our distribution is not evenly distributed in terms of uh branding and visibility. There's also issues of uh coverage. We have issues of GSM coverage. there's a bit of inconsistency or incomplete coverage across the sales clusters. All these inefficiencies results in draw lost opportunities, loss revenues, lost sales opportunities and also if the it increases churn because if the areas are not well served they result in poor service delivery and therefore resulting in CH. So this project aims to transform sales and distribution through harnessing the power of posts and some of the facets that we're going to pick in the field as the commandos go to visit the outlets in as much as they pick some very important information like for instance they pick information on attributes on whether the the outlet had been served whether there was an outage, whether they and they also look at how the the visibility is, is it well branded and they not it down. In as much as they pick all this data, there's also the component of H coordinates that are picked. They're picked automatically from the tool. And these coordinates we look at the field accuracy and we have set some threshold to ensure that whatever data is mapped is mapped at the with the right location. Therefore this is a clear indication that the people or the commando served a certain route because we can real time or later look at their at the way they served the the customer or the client. So also this project also look at how we're going to optimize these clusters to ensure that there's a balanced and efficient sales territory because these clusters as I'm going to show are supposed to serve certain sites and when we call here BTA sites or we call masts masts are telecommunication equipment that propagate signal and each is created is is treated as a as a profitable entity and therefore there should be products and services to ensure that it improves its usage and all these we're going to try to address in this tool and all these at the end of the day is to ensure that decision that are made are datadriven leveraging the special analytics to improve sales strategies In a nutshell, you can see three maps on my far on the first farthest uh right. This is the sales region which is further subdivided into sales zones and then further subdivided into clusters. Clusters are the most granular and these are the ones that we which are served by the sales commanders or the field agents. So you can see we have the field agents. This is the hierarchical structure. We have the sales commanders who are the field agents responsible for direct sales at the cluster level. And these are the face the customerf facing personnel and they report to the zonal manager who oversees a a number of commandos within a given sales zone. Then we also have the regional managers who over who oversee the zonal managers and the commanders and the regional managers report to the sales director. The map here shows the top topmost map is what I shown in the previous slide and the below map is the one on our current coverage. I just picked one signal uh and this is the the GSM coverage of uh showing the 3G coverage and therefore we are looking at these polygons how we draw them we ensure that they contain they are multiple uh they defined polygons and each polygon contains outlets or shops that these clusters are supposed to be visited that have been visited that keep upon being visited uh by the sales agent to ensure that there is proper distribution of our services that is the SIM card and the and the and the air time. So posts will ensure that there's no overlap of the clusters. Uh that means that uh in this sense we're talking about when we allocate a commando a given cluster they're not supposed to go to another person's cluster just to cannibalize or make more sales just in case they feel lazy to or they find it to be more more lucrative to sell there or easier to sell there. We just want them to be on their assigned clusters and that is that has been a major problem where you have a commando they just start selling where they live where they reside instead of going to market. So this is to ensure that the clusters are well designed to optimize the workload and minimize travels. We look at how we do the routing routing. So as I've seen we ensure that uh we also will be giving some information some intel based on this project. Uh we can tell you how far you you're from your your base station or the mast and these are some of the functions in postures that we use like uh the distance uh from the BTS and whether you're within a given cluster. So ST within and ST distance. What you're looking at here is uh a small report of the data that has been picked from from the field. I just decided to just do some dummy data because um we have data protection and we not uh the Kenyan laws do not allow you to dulge private information uh without their authorization. So I just this is confidential company data. So I just did some dummy just to for visualization to show how we are able to pick the accuracy in the field and the dates that were done the time stamp. So we ensure that uh when we go to the field the data that is picked in terms of GPS is accurate by enforcing uh uh GPS accuracy by minimizing it or putting it not less than not more than 30 m greater than 30 m equal or greater than 30 m and this data is sent to the CRM which contains information about our network performance. It contains information about our site performance and it contains information about the customer activity. So these comprehensive data ensure once we pick data in the field we should we ensure that we have reliable special analysis when we are the back end or in the office. It is uh it also informs uh realtime sales commanders that they where they are serving the right clusters because it has alerts and prompts. So it will it will alert you that you're with it will tell you that this is the cluster that you're serving at at a given time and uh upon pinning the location upon pinning the location you can be able to to know that uh that that you're serving maybe you've encroached to someone else's either whether you've encroached to someone else's cluster but it will show you that this is the name of the cluster and you're in the right cluster based on your credential that you've used to login. Uh then next uh so this is how the tool is. The tool basically picks the accuracy and it tells you that if you overshoot the accuracy like for this in this instance we're just using maybe Wi-Fi uh translation or BTS translation is not very accurate and it might not give you the right accurate. So it will throw an error until the until the accuracy is out of the 30 m treasure. So you can reload the GPS there's a button you can do that. So and that is how to achieve the accuracy. So I'm going to to take you through the the the architecture. uh we are working on a we're using jungle as a back end and we have a a database as poss and these are some of the way we query use SQL or inbuilt ORM in jungle basically we have geometries like the polygons which are the the clusters and the regions and the the zones the poly line is the sales route that we've given you that you're supposed to use to go to the field and then the points are some of the outlets that you go visit and then they're going to be picked and pushed back to the back end. Uh we we have uh some special constraints to ensure that you within and valid. These are some of the constraints and we have also some of the functions and triggers where it alerts you on on compliance just to enforce the special integrity. you're you're within the right polygon cluster for the right assignment as I've mentioned before. So optimizing sales agents route also we have the PG routing to improve route efficiency. So this is to minimize redundant travels for sales commandos so that they when they go to market they just don't go far they just know that if they they use this this route and this route this route they will efficiently cover the area without wastage of time and optimize the light the outlet coverage is within of course I've mentioned that is to ensure that you whatever you're serving the cluster is within whatever you've been assigned The algorithm considers network traffic and outlets. So linking a network usage with sales data. Uh special analysis receive re reveals key insights. We try to cor correlate sales data with network performance data which is based on our on our CRM and and some analytic tools. So we identify which base station or the masks are underserved and impacting negatively on sales. We also pinpoint chan hotspots for targeted interventions. This is when we we just correlate the loyalty and ret working with the loyalty and retention team and the revenue planning team to ensure that every BTS is profitable. All right. So this is a visualization of uh the weekly visit of a given commando. You can see this commando h they tend to encroach and leave out other places. As you can see this commando has gone. You look at the green indicates the extent of a given zone and then the blue is a clusters that each commando is given. Find that some of these commandos they leave their clusters and jump into someone else's clusters. And this normally happens during the end of the month when they are running to hit their targets and they tend to have sales cannibolism and inefficiency just to to close a sale and hit the target. So these are some of the things that we're trying to discourage you want and you can see that they went to look for other to other areas other clusters and zones and left out like you can see where here the underserved arrow is indicating. So these are some of the things that we trying to discourage and the managers normally have some uh some measures. For instance, they can reduce your commission for these these these starter packs and air time that was sold and they look at the serial numbers and the uh they will not pay commission. Sometimes they don't pay commission just to discourage encroachment and moving out of your zone. So all these uh smart monitoring and alerts are powered by posties because the poses uh triggers enablers for out enablers for out of zone agent activities. So you can tell that if someone is operating out of your zone uh because of uh is all within ST is all within the defined uh cluster. So that's the we harness the power of postures in that that level. So we as I'd mentioned before we also look at missing data. We've introd some uh integrity checks, consistency checks and when there's low GPS also for accuracy this is how we just do this to to improve our data quality and compliance. Some of this notification are pushed through SMSS or even to the email of the agent. But since most agents are in the field, it is easier for them to get notifications on SMS. But also the app itself, it has its own notification that okay, today you had these number of sites that were not in within your this number of visit that were not within your area. Sorry for that. the these number of visits that were not within your area. So and also we look at the dashboards uh so the dashboard for the supervisors for real time compliance. This is to ensure that they know who is in the field and who is somewhere else. And there some people who are fond of maybe they say they're in the field but maybe that day they just went a half day and then they went and did their activ and they did their own activity. So this shows how the visits were, how long a a certain visit was before the next visit and and the the most important thing is to pick the location. All this resulted in a special organization. There was a big impact that we've been noticing impact as we continue to harness the power of special uh data. So we've noticed that there's increased uh coverage in the sales zones uh and they can be since they're monitored in real time so they can be able to know that this sales commando is serving a given uh a given cluster efficiently and effectively and these are rating improved coverage uh outlet coverage and visibility. So that one you also there's a lot of improvement visibility because they always indicate whether they there was branding and whether they branded whatever was removed they replaced posters and everything. This has improved efficiency in sales operation by reducing the sales overlap whereby an outlet is been is not being served by two or more commandos within a given day or given week. So there's a lot of ensuring that there's efficiency in that field. So we've also looked at the targeted sales effort has improved the performance per BTS which is the the GSM equipment for propagating the signal and also due to improved services. There's been reduced chunk uh by by by by giving customers good services and closing the sales staff. So all these are ens is ensuring that our levels are well branded, more competitive and and these are also we have enhanced our decision making uh process and uh for the managers and the executive committee and the and also reporting is that we ensure that the B people have access to reports including the people on the ground. The next steps we're looking at integrating with AI algorithms uh to look at the AIdriven demand mapping where we we are able to get an AI visit plan uh based on that is that has minimum human intervention by by interconnecting all the the data points to make informed decision. So we're also looking at real real time dashboards for all staff working for various KPIs. Currently we are limited to mostly the sales teams. We also want to uh wrap up loop loop the the network maintenance team and the finance finance team. We're looking at also ch prediction which is already happening but maybe integrating it with our sales and distribution. In conclusion, mapping for market intelligence we had key takeaways. Post has transforms our sales operations into a just partially intelligent system. enable datadriven decisions for improved sales performance and reduced churn. Special data special data is our sales power. Post has ensured fairness, no market can carib. So thank you so much to the post community for granting me this opportunity to showcase what I do in this other part of the world. Thank you to my team also the field team and also the the backend team the CRM and the data teams for the collaboration in data integration and analysis. So thank you so much if you have any question uh an answer session. "}
{"url": "https://www.youtube.com/watch?v=RR-Kbj_eK2U", "title": "Hexagonal Thinking: Managing Climate Datasets In PostGIS", "transcript": "Hello everyone and welcome to postJS day 2025 hexagonal thinking talk. It's great to be here. Today I want to talk you about hexagonal thinking specifically how we can manage massive complex climate data sets using the power of postJS combined with the H3 grid system. We are going to look at this through six specific angles or sites that we discovered while building our platform throughout.uk. We will explore how we move from traditional spatial management to a hexagonal approach to unlock the speeds and scalability. Okay, first a little bit about my background. Uh so you know where I am coming from. I'm a geatial developer and entrepreneur with about 22 years of expertise in the fields. For the last decade I have pivoted my focus almost exclusively to the climate change related projects. You might have seen my work on global droughtmap.com and currently I'm deeply in involved in the development of drought.uk. In building these systems, I realized that standards JS approach weren't enough for the sheer volume of the environmental data we are seeing. And what about the challenge? The core issue we face today is that I call to tsunami climate data to tsunami. Why this tsunami is worse than a normal big data? It is not just big. It's big and spatial. Spatial temporal temporal multi-resolution motor resource is equals to exponential complexity. For example, a single sentinel two tile is about 800 megabytes for uh 13 bands. Area 5 climate analysis is 35 terabytes for the full data set if you store row hourly fields. And for NVI and AF parttime series thousands of scenes per region per year. Most systems can handle big tables but climate data is big tables plus expensive operations reprojection resampling interpolation raster clipping spatial joins on large extents temporal ex aggregation monthly yearly and etc. Traditional database collapse under this load unless you build a very smart structure. Why classical JS fails here? ClassicalJS texts like RJS, QJS, Judal based tools can handle raers but not easily at national or continental scale. Not optimized for repeated queries over millions of pixels. often batch oriented not query oriented. PostJS tries but working directly with row pixels realers is too heavy and station data. The third one station data adds a totally different dimension. Climate stations are unevenly distributed, biased towards populated zones, often sparse or missing in rural areas, come with a metadata, elevation, sensor type that must be harmonized. You need spatial interpolation, IDV, clicking, spines to create a grided products. Mixing raers plus points and polygons is very hard problem to solve. And the real question in the slice raises how do we unify? This is where the entire talk is going. You pose the question how do we unify query or analyze the disperate heavy data at scale without the database grinding to a halt. The audience will immediately think how you tile it or do you tile it? Do you downscale it? Do you store row rausters? the prep process into cubes. This sets the perfect foundation for introducing H3 later. The unifying units, the scalable spatial index and postJS as the engine. Before starting, I just want to mention about the tech stack. To solve this, we settled on a specific high performance text stack. At foundation, as you expected, we have postgrl and postjs. This is the bedrock of our spatial data storage. However, the secret weapon or ingredient is the H3 Ubers hexagonal hierarchical spatial index. This allows us to discretize the word into manageable cells. We utilize duct DB for high performance OLAP quering when we need to crunch the numbers and we wrap the whole architecture in the NodeJS to serve the API. Okay, let's walk through the six sides as I promised to Paul just six no less or more of how this stack works. Okay, the side one is the unified management for complex data. Angle one or side one is the unified management. One of the biggest challenge with climate data is that every data set speaks in different language. You have a raster for temperature, a CSV for soil moisture, polygons for catchments and point-based station networks for rainfall. Each comes with its own projection, spatial resolution and temporal structure. What we do with H3 inside PostJS is translate all of these formats into single consistent spatial index. Rusters are sampled or aggregated into H3 cells. Vector points are snapped to their nearest H3 hexagon and polygons are dissolved into hexagons. They intersect. Once that happens, everything becomes simple. We have a table structure like H3 index, date, value and data type. It no longer matters whether the original data set was a geotive, a shape file or CSV. This normalization means postJS is no longer dealing with five incompatible spatial models. It's managing one unified grid which allows joins, overlays and comparison to become extremely fast and clean all based on a single column of H3 indices. And next site is the powerful data aggregation. Climate data by its nature is noisy. uh we rarely know or need to know the NDVI or the SPI value for every single pixel. What we actually care about is the trends over a region, a catchment or an administrative boundary. This is where the hierarchical structure of H3 becomes incredibly valuable because every hexagon has a parent at a coarser resolution. We can take high resolution drought indicators SPI, SPI, NDVI, soil moisture anomalies, etc. and roll them up the hierarchy with a simple SQL group by on the parent H3 cell. Suddenly regional drought severity, monthly summaries or long-term trends can be computed in milliseconds directly inside postJS instead of running expensive raster based zonal statistics, clipping, reproing, resampling. We are aggregating clean pre-indexed values. This turns what used to be a heavy JS workflow into a lightweight SQL query. Now let's come to site three scalable spatial queries. The third site is as postJS users. We know all geometric intersections SD intersects SD within SD union are among the most computationally expensive operations you can run especially when you are working with large detailed polygons or high resolution raers. When we shift to hexagonal thinking, we stop doing geometric maths at runtime and start doing integer math. Every H3 cell is represented by a unique 64-bit integer or text in identifier. So instead of intersecting two gamma, we are simply comparing two integers and that means our queries can write entirely on B3 index. If you want to know the drought status of a specific region, you don't need to clip a raster, dissolve a polygon, or run zonal statistics. I I just fetch the list of H3 indices that fall inside the region and perform a straightforward lookup. This transformation, what used to be a heavy spatial operation into a lightweight database query. And because integers scale linearly, these lookups remain fast when you are working with billions of rows. No geometry processing, no complex spatial joints, just pure scalable indexing. And site for this is hierarchical or multi-resolution analysis. Drought impacts occur at very different scales. A local farmer needs to understand soil moisture conditions within a few kilometers. A regional water authority cares about river basin trends. The national government wants to aggregate its picture of countrywide drought exposure. With postJS and H3, we can store climate indicators at finest practical resolution. For example, resolution 67 where each X might represent a few hundreds m to a few kilometers. What makes H3 so powerful is that parent child relationships between resolutions are deterministic and reversible. Every resolution seven hacks has a clear parents at resolution six, five, and all the way up to resolution one. This means we can query single data set at multiple case simply applying H3 cell to parent in SQL. Need a basin level roll up roll up to resolution five. Need a resolution snap snapshot roll up to resolution tree. This creates what I call a zoom stack for climate analytics. A database automatically provides an appropriate level of detailing depending on the user's viewing context all without duplicating or precomputing multiple data sets. In other words, we get multiscale analysis with one unified H3 index table and the transition between scales are seamless. Sorry, forget about switching the slides. Okay, the next one is site five improved visualization. Most of us are used to viewing climate data as block raster pixels. The problem is that pixels especially in web market distort size dramatically as you move towards the pose. This makes spatial comparison misleading. Hexagon solves this elegantly. Each H3 cell has a consistent shape and consistent distance to its neighbors, which makes them statistically superior for modeling continuous natural phenomena like rainfall or drought intensity. When we render climate indicators on the front end using an H3 grid, the patterns appear appear more natural, more uniform, and much easier to interpret. Instead of rigid square blocks, the hexagonal layout reels smoother transition and clearer flows across the landscape. In short, hexagons are aren't just visually appealing. They communicate climate patterns more truthfully. And we came to last sides bringing data to actionable insights. This is the really heart of the entire architecture. We are not building a spatial database because it's elegant. We are not aggregating SPI or NDI into hexagons because it looks good on the map. We are doing this because stakeholders from farmers to water regulators to natural governments need to answer the so what questions. By combining H3 indexed climate data sets with H3 indexed demographic or land use data sets in postJS, we turn scientific observations into real world meaning. For example, we can instantly intersect drought hexagons with population hexagons to compute drought exposure. How many people live inside the area currently experiencing uh SPI index below minus 1.5? How many crops or hectars of agriculture fall into extreme dryness? How many critical facilities, hospitals, schools, reservoirs are in high-risisk zones? These questions used to require complex JS workflows, clipping raers, running zono statistic, reproing layers and then trying to combine incompatible data sets. Now it's simple simple a join on H3 index because everything is aligned to the same hexagon algorithm. the analysis becomes transparent, repeatable and fast. This is what makes the gap between raw scientific data which often just looks a binary drought mask and the actionable insights that the policy makers, councils and planners need to make decisions. In short, H3 gives us a shared spatial language and PostJS turns that language into intelligence. And what about the road map of or life cycle of a data? Uh the first we ingest we load the satellite data and DVI station data calculate then we process it clean validate store in postJS table then we aggregate them snap all points to H3 resolution 7 cells and then we use it in our analysis and visual visualize it eventually. So, H3based map. Now, uh I just want to make a short demo on the drought.uk. Okay, this is the main map. By the way, there isn't any JS servers behind just NodeJS, no T server, map server, arch server is just the data and hexagons are created on the fly. You can change to risk index, exposure index, rainfall, or you can just move between the time series. It's just super fast. Okay, what about the feature? To wrap up, this combination represents the future of environmental data management. HostJS provides us with the robust industrial strength spatial foundation with trust. H3 provides the universal high performance indexing key. Together they create a scalable system capable of turning massive chaotic climate data sets into actionable intelligence we urgently need to combat combat climate change. Thank you very much for your time. I invite you to visit rout.uk to see some of these concepts in production. If you have any questions about implementation or the stack, please reach out to me. at the email listed here. I'm happy to take any questions. Now, back to you, Paul. "}
{"url": "https://www.youtube.com/watch?v=F1XLowasV_A", "title": "Innovate With PostGIS: If You Do What Other People Do, You Get What Other People Get", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=zTGaWmkXyu4", "title": "Snowflake And PostGIS Together", "transcript": "Good day everyone. It's a real honor to be here at PostJS and I'm here to today to talk about a shared challenge the divide between our operational systems and our analytical systems. My name is Favad Kureshi and I'm a global field CTO at Snowflake. This talk isn't about or. The key word is and. It's about how the amazing open-source world of PostGIS and the scalable world of the AI data cloud can come together to break down barriers. Today, as we know PostJS powers application and data warehouses power analytics and for decades, they have lived in separate worlds. We all know this divide. On one side we have our systems of engagement and for the geospatial the undisputed king is postgis. It is fast. It has the richest functional library on the planet and it's the open-source heart of countless applications. On the other side we have our systems of insight. This is where we try to analyze massive scale data and historically this is where things get difficult. The traditional data warehouses that are companies use often have weak or no GIS support, can't scale and force us to create another data silo. And in the world of analytics, this silo creates a lot of challenges. GIS or the GPS technology more specifically was created back in the 60s as as a classified technology and that legacy of being being a classified technology for a long time has has remained in our data ecosystem. We every customer every industry that I talk to has an enterprise data team data analytics team and every customer every industry has an enterprise GIS team and they hate each other. They don't talk to each other. They don't interface with each other because they all have different types of systems that that have their own strengths and weaknesses. And I'll talk about some of the examples and stories that I have worked with. So at the beginning of co I was working with a telco to build a covid contact tracing system. We built this on three three months of data, lots of several terabytes of data combining with the healthcare data and we built a model and it was fairly accurate. When we tried to move that model to production, the DB of the data warehouse said, \"Fawad, if your query takes more than 3% CPU, I'll kill the query because I cannot afford for your complex geospatial query to create resource contention issues with the rest of the workloads.\" So we had to take that workload out of that central warehouse and run it into a separate system. Whereas on the other hand side in the world of GIS I was using a very popular desktop JS tool trying to connect it to Snowflake and it gave me error. I'll not connect to a table with more than 250,000 rows because most of the desktop JS tools they try to bring in all the data into the laptop memory and they try to process it them there. So scale becomes a challenge. And then when you want to do complex advanced geoprocessing, okay, in working with a retailer here in the UK, um they asked me to help them with identifying their loyal customer. Upon inquiring the definition of loyalty, they said they live close to the number one retailer in the UK and they drive past that retailer to shop at their store. Um they are a loyal customer. So literally going the extra mile and you know not shopping just because of convenience. So we had to do drive time calculations but most of the databases only support crow flies distances and crow flies distances are useless when it comes to urban cities. So we had to export the data out of that data warehouse into a separate routing engine and it took several weeks for us to complete the job for like 10 million customers. And then if you want to proc uh combine various type of capabilities in a traditional GIS tool, it becomes a very cumbersome task. So how can we build a world where we are able to combine the capabilities of all of these operational and analytical enterprise data systems and enterprise GIS all in a single platform in a single solution where we don't have to make a decision we don't have to compromise we don't have to say oh this or that how many times can we add an and to the conversation that's the vision vision that we are trying to work towards. So why is a company like Snowflake here at PostJS? It's simple. We are heavily invested in the open source world and we believe in using the right tool for the right job. PostJS is the right tool for transactional geospatial and our goal isn't to change that. Our goal is to solve the other side of the problem, the analytics silo. In in fact, our commitment to this ecosystem is growing. We're investing heavily in Postgress, including our new fully managed Snowflake Postgress offering. We are part of this community. So for those of you who aren't familiar, here's a quick intro to Snowflake. It's not a traditional database like Postgress. It's its core architecture is different. It's a cloudnative solution. It separates storage from compute. allows you to run massively parallel compute on on a very large amount of data. This means you can have one copy of your data and spin up or down any number of compute engines to work on it all at the same time without them ever competing for resources. You can run various type of workloads on the same type of data. Data engineering, data leaks, uh application, geospatial, advanced warehouse, all of that on the single copy of the data. This is why it is so powerful for these advanced types of analytics. You can run a massive GIS query, a BI dashboard, and an AI model at the same time, and they won't slow each other down. It is built to solve the OLAP side of that great divide. SQL is powerful, but most of our stakeholders don't read SQL or Giojson. They want to see a map or a dashboard. This is where another open source tool, Streamlit, comes in. Streamlit lets you build a web app purely in Python in just a few lines of code. And Streamlit is available natively in Snowflake. This is a gamecher for JS analysts. You can write a Python script that queries your PostGIS and Snowflake databases, does some analysis, and then instead of just printing a table, you can display an interactive map that a manager can actually use. It's another way to bridge that gap between the technical expert and a business user building on the open-source tools we all know. And the same application can be shared with desktop tools, with mobile devices, tablets and more without the need for installing any client side utilities. The there are no copies of the data being made. The the application runs directly on the data following the same underlying data governance principles that were set up when a data exchange was set up. And that means there um you can trust the uh the process. You can create lineage traceability across the entire process. And it is it is very simple, easy to use and uh extremely simple to deploy. So this brings us to the state-of-the-art today. This isn't hypert 2026. This is what you can do right now. The best part is you can use the open source tools you already love like QGIS or modern web platforms like Felt and Cartto. These tools can connect to Postgis for your live hot data and to Snowflake for your massive code analytical data at the same time. Imagine on one map seeing your live field crew locations from Postgis layered on top of a 10 billion row heat map of customer activity from Snowflake. That's the power of this better together approach. Now for a little fun. Um we all know PostJS SQL is incredibly powerful but how many people can remember STD within or SD intersect which isn't something your average business user knows. I know we we have a lot of geeks and a lot of technical uh people in in the postJS day today. But if we look at the the global statistics, I was looking at some stats from Stratesta across the whole world there are only about 4 million GIS developers. We have 18 million Python developers. We have about 85 million SQL developers. But do you know how many English speakers are there in the world including the people who have who are native speakers as well as second language speakers? It's nearly 1.46 billion. And the world population is 8 8.5 billion. So why I'm talking about these billions of numbers? Because we now have an ability to build natural language interfaces on top of this data. What if we could use AI to build a bridge for them? We can experiment with using Snowflake's AI service cortex as a natural language interface to PostGIS. We can build a translator, a middleware that can take those natural language questions, convert them to code. And the the beauty of this is Cortex allows you to bring any type of AI model uh any type of LLM, open source, commercial, private. You can host your own models and we ensure that the the data remains private, your metadata remains private, your model remains private and your prompts remain private. will will guarantee that your data will not be used to train somebody else's model. The user can then ask a question in plain English. Cortex generates the complex PostJS compatible SQL and you could execute this query against your PostJS database or even against the new Snowflake Post service and get back the gojson to instantly build a map. This is about accessibility and reaching out to to domain users that were not able to uh to access the data before. So you can have defining conversations, you can you can have birectional conversations. So people can ask question in English to map and you can ask the um build describing interfaces. Oh, I have this map. I don't understand what it is showing me. So you can take a screenshot of the map or you can ask for more metadata to describe what's happening inside the map. So instead of u people raising a ticket to IT teams the end users can interact with the data using self- service ask the data hey give me more more context give me what's happening so that you can build more powerful more empowered users inside and out uh and outside your organization. So one of the things that we are seeing in the world is this move towards open data format. I was working with a major public sector agency and they are what they told me that they have like 2.5 exabytes of data across various of their data states but the unique data is only about 300 pabytes copied eight times over into 10 different systems because each system requires different types of data formatted file structures and that's not a very scalable and repeatable approach. So what we are seeing increasingly across the entire data world is people are demanding the organization demanding more and more open table formats and we are seeing the same trend coming towards the world of spatial as well. The future of spatial data is undeniably open and that's is something that we need to celebrate. Open table formats like Apache iceberg give organization a stable interoperable and futurep proof foundation for for the geospatial data. Think about all of the geospatial data sets that still sit in isolated systems in a shape files and JSONs and this format and tiger and all um inaccessible and underutilized. By adopting open formats, we unlock these assets so they can move freely across tools, industries and clouds. This eliminates the heavy friction of conversions, handbuilt connectors, bpoke pipelines. More importantly, it accelerates innovation because when data becomes interoperable, new value chains start to form. Open formats only create value when the ecosystem around them is equally open. This is where PG lake becomes a powerful bridge. PG lake is a Postgres extension which provides native support for iceberg alongside widely used open format which means your data lake can speak the same language across the tools and platforms you rely on. And because Snowflake also supports a iceberg as a first class citizen, we now get something we never had before. A seamlessly connected data layer between Snowflake and Postgress through PG Lake. And this unlocks three major advantages. Interoperability without duplication. Snowflake heavily encourages zero copy sharing between various application and this is an extension of this the snowflake vision. Data stored in iceberg can be accessed and processed by snowflake and progress without heavy ETL or format conversion. You can have an an operational application uh doing front-end transactions processing and the data could be saved at the back end in iceberg and the same data can then be queried by by a snowflake query for analytical purposes doing some seasonality analysis different kind of mapping and strategic analysis that can be applied. bringing that strategic and tactical queries all in the same environment. Number two is the freedom of movement. Workloads can run where they make the most sense. Analytics in Snowflake, operational workloads in in Postgress, all pointing to the same iceberg back data sets without creating additional copies, no latency. The moment the transaction happens on the front end, immediately the data is available for analytics at the back end. So again that the combination of tactical and strategic running in the same environment seamlessly without creating additional pipelines is extremely powerful. And number three it opens up a unified open architecture instead of scattering data across proprietary systems. Organization get an open interoperable foundation that supports multiple engines while maintaining strong governance. PGL doesn't just support open formats. It connects them enabling Snowflake and Postgress to work together on iceberg in a way that feels natural, unified and futurep proof and that all unlocks so many possibilities for organizations all over the world across all different types of industries. So what next we are going to do is we are going to have um a quick demo of how some of these capabilities come together uh to to provide um unique insights in for different users. So let's talk about this. So what I am here is I'm inside the snowflake uh UI in snowite and I'm going to go into uh into the the compute environment and I'm going to spin up a postgress instance select the memory type version I can select high availability and I can then create the environment. it will generate a connection string and the environment variables. I can copy those those connection strings and environment variables and bring them those environment variables into uh into psql. I I can connect to psql and then I can look at the the various objects that are in in the system the the tables and sequences that are available. I can then u run a simple query. I have I'm looking at a simple sales table. um today's sale and I want I see what type of data exists in there and I've got some timestamps and different kind of other metadata about the the transaction that appears in there. Going back I've got another table in in snowflake which has sales information and it has some location information in the form of latl longl long as well. So snowite is giving you different um information about the the data. It has some histograms automatically generated as well. So now what we are going to do is we are going to expose these two tables through streamllet and not just we are going to generate some charts. uh we can we can visualize of course through a map and we can see what is the distribution of the data on the map and how where are the stores located different kind of insights this is fairly standard run-of-the-mill stuff that you can do with geo data but what we really want to talk about is how can we create net new um things through through this uh talking to your geo data in in this case what we are going to do is Okay, I'm going to ask it give me all the stores which are the west of Mississippi. So what it is going to do is take my my uh my national language command and try to convert that at the back end into into an appropriate structure. You can see the SQL code that is generates and then it will create a map on top of the data and uh and um so now this gives end users the pos possibility of interacting with the data in a unique ways. I can al also check for for the top 10 store locations different information. uh similarly uh to different tables I can combine them bring them into the environment and runs uh it will run fairly smoothly. It will give you that insights and you can combine of all of this data into a single uh single dashboard and unlock really strong powerful capabilities for for different users in the ecosystem. So what really does is it it allows the end users to interact with the data in a way that was not possible before. They're able to build net new capabilities, interact with the data, ask further defining questions. Instead of asking u um an intermediate team in the middle to ask to build custom reports for them, you now have the end users who are really the business owners who could work with the data directly without the need for uh for complex back and forth between different different teams and departments. And you can also ask questions. Hey, describe what's happening inside the map. And you can you can uh build a complete context around this conversation. And these are really powerful capabilities for for the user. So this is something that unlocks different types of possibilities. So what we can do so let's to wrap up let's look at the future today. We can build a powerful unified view using tools like QGIS and felt to bridge postJIS and Snowflake. We can connect to streamlate and we are already taking a huge step forward. You know things that were not possible before. The future we are working towards is one where this connection is even more seamless where the boundary between transaction analytical fades completely. The the future vision is that it becomes truly seamless. A data pipeline where maps, models and machine learning work in unison. Data flows continuously from the field to the cloud. um which is postg to um the snowflake AI data cloud with insights flowing back which is in the form of AI models and AI insights and these are this is the future we are building towards in partnership with the open source community. This is our geo without boundaries vision. It's not about postjs or snowflake. It's about postjis and snowflake working as one and we are excited to be part of the JS uh postjis community and building this future together. So I invite you I encourage you to to come and explore the the snowflake post postgress offering and leverage various type of postjs applications. Connect to streamlate use native tools such as felt and cart which are which are cloudnative. Connect to open source desktop tools such as QGIS which have native connectors for snowflake. So work with them, see the intelligence action and leverage the tools with the freedom of choice. You can use whatever tools you like connecting to the um the platforms which are based on open standards and open formats and you'll be able to build capabilities that were not possible before. My name is Favad Kureshi and it has been a pleasure talking to you today. Good luck uh in trying out all of these capabilities and I'm sure you will enjoy them as much as I did while exploring them today. Thank you so much. "}
{"url": "https://www.youtube.com/watch?v=ITPwFyk94Gk", "title": "PostGIS Topology: Strengths And Weaknesses", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=-Pd_yV738mg", "title": "Working With Geospatial EmbeddingsWwith PostGIS And PGVector", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=YrrGNWo-G8o", "title": "PostGIS: The Spatial Engine Of IGN For Two Decades", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=ie-1nImu_70", "title": "Processing The World With H3 And PostGIS", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=NHJb44RjCWs", "title": "Exposing PostGIS Data With OGC Standards", "transcript": ""}
{"url": "https://www.youtube.com/watch?v=KDlB-PZqnlU", "title": "What's \"New\" in PostGIS", "transcript": ""}
